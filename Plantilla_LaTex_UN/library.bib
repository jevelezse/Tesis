@article{Kawashima2017,
author = {Kawashima, Koya},
file = {:home/jennifer/Descargas/kawashima2017.pdf:pdf},
isbn = {9781509055043},
keywords = {are applied for finding,association words that indicate,biomedical literatures,breast,cancer,clustering,k-means clustering,relation extraction,text mining,the candidate,the relationship between breast},
pages = {1--5},
title = {{Text Mining and Pattern Clustering for Relation Extraction of Breast Cancer and Related Genes}},
year = {2017}
}
@article{Buckley1988,
author = {Buckley, Gerard Salton and Christopher},
file = {:home/jennifer/Descargas/salton1988.pdf:pdf},
journal = {Information Processing and Management},
number = {5},
pages = {513--523},
title = {{Term-weighting approaches in automatic text retrieval. Information Processing and Managemen}},
volume = {24},
year = {1988}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Jain, Anil K.},
doi = {10.1016/j.patrec.2009.09.011},
eprint = {0402594v3},
file = {:home/jennifer/Descargas/jain2010.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
pmid = {21856},
primaryClass = {arXiv:cond-mat},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Thiebaut2017,
abstract = {The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.},
archivePrefix = {arXiv},
arxivId = {1712.02259},
author = {Thiebaut, Nicolas and Simoulin, Antoine and Neuberger, Karl and Ibnoushein, Issam and Bousquet, Nicolas and Reix, Nathalie and Moli{\`{e}}re, S{\'{e}}bastien and Mathelin, Carole},
eprint = {1712.02259},
file = {:home/jennifer/Descargas/1712.02259.pdf:pdf},
journal = {arXiv},
title = {{An innovative solution for breast cancer textual big data analysis}},
url = {http://arxiv.org/abs/1712.02259%0Ahttps://arxiv.org/abs/1712.02259},
year = {2017}
}
@article{Weegar2015,
abstract = {Detection of early symptoms in cervical cancer is crucial for early treatment and survival. To find symptoms of cervical cancer in clinical text, Named Entity Recognition is needed. In this paper the Clinical Entity Finder, a machine-learning tool trained on annotated clinical text from a Swedish internal medicine emergency unit, is evaluated on cervical cancer records. The Clinical Entity Finder identifies entities of the types body part, finding and disorder and is extended with negation detection using the rule-based tool NegEx, to distinguish between negated and non-negated entities. To measure the performance of the tools on this new domain, two physicians annotated a set of clinical notes from the health records of cervical cancer patients. The inter-annotator agreement for finding, disorder and body part obtained an average F-score of 0.677 and the Clinical Entity Finder extended with NegEx had an average F-score of 0.667.},
author = {Weegar, R. and Kvist, M. and Sundstr{\"{o}}m, K. and Brunak, S. and Dalianis, H.},
file = {:home/jennifer/Descargas/2245432.pdf:pdf},
issn = {1942597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
pages = {1296--1305},
title = {{Finding Cervical Cancer Symptoms in Swedish Clinical Text using a Machine Learning Approach and NegEx}},
volume = {2015},
year = {2015}
}
@article{Hussain2016,
abstract = {Medications are an important element of medical records but they usually contain significant data errors. This situation may result from haphazardness or possibly careless storage of valuable information. In either case, this misspelled data can cause serious health problems for the patients and can put their life at a major risk. Thus, the correctness of medication data is an important aspect so that potential harms can be identified and steps can be taken to prevent or mitigate them. In this paper, a novel and practical method is proposed for automated detection and correction of spelling errors in electronic medical record (EMR). To realize this technique, major relevant aspects is taken into consideration with the help of PartsofSpeech tagging and Regular Expressions. The paper concludes with recommendations and future work for giving a new direction to the emendation of drug nomenclature.},
author = {Hussain, Faiza and Qamar, Usman},
doi = {10.5220/0005911503330338},
file = {:home/jennifer/Descargas/ICEIS_2016_240.pdf:pdf},
isbn = {978-989-758-187-8},
journal = {Proceedings of the 18th International Conference on Enterprise Information Systems},
keywords = {but they usually contain,electronic medical record,element of medical records,emr,information retrieval,medications are an important,natural language processing,parts-of-speech tagging,post,regular expressions and medical,significant data errors,spelling correction,text mining,text processing},
number = {Iceis},
pages = {333--338},
title = {{Identification and Correction of Misspelled Drugs' Names in Electronic Medical Records (EMR)}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005911503330338},
volume = {2},
year = {2016}
}
@article{Dewey2016,
abstract = {The DiscovEHR collaboration between the Regeneron Genetics Center and Geisinger Health System couples high-throughput sequencing to an integrated health care system using longitudinal electronic health records (EHRs).We sequenced the exomes of 50,726 adult participants in the DiscovEHR study to identify $\sim$4.2 million rare single-nucleotide variants and insertion/deletion events, of which $\sim$176,000 are predicted to result in a loss of gene function. Linking these data to EHR-derived clinical phenotypes, we find clinical associations supporting therapeutic targets, including genes encoding drug targets for lipid lowering, and identify previously unidentified rare alleles associated with lipid levels and other blood level traits. About 3.5% of individuals harbor deleterious variants in 76 clinically actionable genes. The DiscovEHR data set provides a blueprint for large-scale precision},
author = {Dewey, Frederick E. and Murray, Michael F. and Overton, John D. and Habegger, Lukas and Leader, Joseph B. and Fetterolf, Samantha N. and O'Dushlaine, Colm and {Van Hout}, Cristopher V. and Staples, Jeffrey and Gonzaga-Jauregui, Claudia and Metpally, Raghu and Pendergrass, Sarah A. and Giovanni, Monica A. and Kirchner, H. Lester and Balasubramanian, Suganthi and Abul-Husn, Noura S. and Hartzel, Dustin N. and Lavage, Daniel R. and Kost, Korey A. and Packer, Jonathan S. and Lopez, Alexander E. and Penn, John and Mukherjee, Semanti and Gosalia, Nehal and Kanagaraj, Manoj and Li, Alexander H. and Mitnaul, Lyndon J. and Adams, Lance J. and Person, Thomas N. and Praveen, Kavita and Marcketta, Anthony and Lebo, Matthew S. and Austin-Tse, Christina A. and Mason-Suares, Heather M. and Bruse, Shannon and Mellis, Scott and Phillips, Robert and Stahl, Neil and Murphy, Andrew and Economides, Aris and Skelding, Kimberly A. and Still, Christopher D. and Elmore, James R. and Borecki, Ingrid B. and Yancopoulos, George D. and Davis, F. Daniel and Faucett, William A. and Gottesman, Omri and Ritchie, Marylyn D. and Shuldiner, Alan R. and Reid, Jeffrey G. and Ledbetter, David H. and Baras, Aris and Carey, David J.},
doi = {10.1126/science.aaf6814},
file = {:home/jennifer/Descargas/dewey2016.pdf:pdf},
isbn = {10.1126/science.aaf6814},
issn = {10959203},
journal = {Science},
number = {6319},
pmid = {28008009},
title = {{Distribution and clinical impact of functional variants in 50,726 whole-exome sequences from the DiscovEHR study}},
volume = {354},
year = {2016}
}
@article{McCarthy2014,
abstract = {BACKGROUND Variant annotation is a crucial step in the analysis of genome sequencing data. Functional annotation results can have a strong influence on the ultimate conclusions of disease studies. Incorrect or incomplete annotations can cause researchers both to overlook potentially disease-relevant DNA variants and to dilute interesting variants in a pool of false positives. Researchers are aware of these issues in general, but the extent of the dependency of final results on the choice of transcripts and software used for annotation has not been quantified in detail. METHODS This paper quantifies the extent of differences in annotation of 80 million variants from a whole-genome sequencing study. We compare results using the RefSeq and Ensembl transcript sets as the basis for variant annotation with the software Annovar, and also compare the results from two annotation software packages, Annovar and VEP (Ensembl's Variant Effect Predictor), when using Ensembl transcripts. RESULTS We found only 44% agreement in annotations for putative loss-of-function variants when using the RefSeq and Ensembl transcript sets as the basis for annotation with Annovar. The rate of matching annotations for loss-of-function and nonsynonymous variants combined was 79% and for all exonic variants it was 83%. When comparing results from Annovar and VEP using Ensembl transcripts, matching annotations were seen for only 65% of loss-of-function variants and 87% of all exonic variants, with splicing variants revealed as the category with the greatest discrepancy. Using these comparisons, we characterised the types of apparent errors made by Annovar and VEP and discuss their impact on the analysis of DNA variants in genome sequencing studies. CONCLUSIONS Variant annotation is not yet a solved problem. Choice of transcript set can have a large effect on the ultimate variant annotations obtained in a whole-genome sequencing study. Choice of annotation software can also have a substantial effect. The annotation step in the analysis of a genome sequencing study must therefore be considered carefully, and a conscious choice made as to which transcript set and software are used for annotation.},
author = {McCarthy, Davis J. and Humburg, Peter and Kanapin, Alexander and Rivas, Manuel A. and Gaulton, Kyle and Cazier, Jean Baptiste and Donnelly, Peter},
doi = {10.1186/gm543},
file = {:home/jennifer/Descargas/gm543:},
isbn = {1756994X (Linking)},
issn = {1756994X},
journal = {Genome Medicine},
number = {3},
pmid = {24944579},
title = {{Choice of transcripts and software has a large effect on variant annotation}},
volume = {6},
year = {2014}
}
@article{Cassa2017,
abstract = {Shamil Sunyaev, David Beier and colleagues report an analysis of the fitness effects of heterozygous protein-truncating variants from the Exome Aggregation Consortium. They find that high heterozygous selection coefficients are enriched in Mendelian disease-associated genes and essential mouse genes, suggesting that this coefficient can be used to prioritize candidate disease-associated genes from clinical exome-sequencing data.},
author = {Cassa, Christopher A. and Weghorn, Donate and Balick, Daniel J. and Jordan, Daniel M. and Nusinow, David and Samocha, Kaitlin E. and O'Donnell-Luria, Anne and MacArthur, Daniel G. and Daly, Mark J. and Beier, David R. and Sunyaev, Shamil R.},
doi = {10.1038/ng.3831},
file = {:home/jennifer/Descargas/cassa2017.pdf:pdf},
issn = {15461718},
journal = {Nature Genetics},
number = {5},
pages = {806--810},
pmid = {28369035},
publisher = {Nature Publishing Group},
title = {{Estimating the selective effects of heterozygous protein-truncating variants from human exome data}},
url = {http://dx.doi.org/10.1038/ng.3831},
volume = {49},
year = {2017}
}
@article{Fu2013,
abstract = {Establishing the age of each mutation segregating in contemporary human populations is important to fully understand our evolutionary history and will help to facilitate the development of new approaches for disease-gene discovery. Large-scale surveys of human genetic variation have reported signatures of recent explosive population growth, notable for an excess of rare genetic variants, suggesting that many mutations arose recently. To more quantitatively assess the distribution of mutation ages, we resequenced 15,336 genes in 6,515 individuals of European American and African American ancestry and inferred the age of 1,146,401 autosomal single nucleotide variants (SNVs). We estimate that approximately 73% of all protein-coding SNVs and approximately 86% of SNVs predicted to be deleterious arose in the past 5,000-10,000 years. The average age of deleterious SNVs varied significantly across molecular pathways, and disease genes contained a significantly higher proportion of recently arisen deleterious SNVs than other genes. Furthermore, European Americans had an excess of deleterious variants in essential and Mendelian disease genes compared to African Americans, consistent with weaker purifying selection due to the Out-of-Africa dispersal. Our results better delimit the historical details of human protein-coding variation, show the profound effect of recent human history on the burden of deleterious SNVs segregating in contemporary populations, and provide important practical information that can be used to prioritize variants in disease-gene discovery.},
author = {Fu, Wenqing and O'Connor, Timothy D. and Jun, Goo and Kang, Hyun Min and Abecasis, Goncalo and Leal, Suzanne M. and Gabriel, Stacey and Altshuler, David and Shendure, Jay and Nickerson, Deborah A. and Bamshad, Michael J. and Akey, Joshua M.},
doi = {10.1038/nature11690},
file = {:home/jennifer/Descargas/fu2012.pdf:pdf},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {7431},
pages = {216--220},
pmid = {23201682},
title = {{Analysis of 6,515 exomes reveals the recent origin of most human protein-coding variants}},
volume = {493},
year = {2013}
}
@article{Holst-Hansen2017,
abstract = {Allele number, or zygosity, is a clear determinant of gene expression in diploid cells. However, the relationship between the number of copies of a gene and its expression can be hard to anticipate, especially when the gene in question is embedded in a regulatory circuit that contains feedback. Here, we study this question making use of the natural genetic variability of human populations, which allows us to compare the expression profiles of a receptor protein in natural killer cells among donors infected with human cytomegalovirus with one or two copies of the allele. Crucially, the distribution of gene expression in many of the donors is bimodal, which indicates the presence of a positive feedback loop somewhere in the regulatory environment of the gene. Three separate gene-circuit models differing in the location of the positive feedback loop with respect to the gene can all reproduce the homozygous data. However, when the resulting fitted models are applied to the hemizygous donors, one model (the one with the positive feedback located at the level of gene transcription) is superior in describing the experimentally observed gene-expression profile. In that way, our work shows that zygosity can help us relate the structure and function of gene regulatory networks.},
author = {Holst-Hansen, Thomas and Abad, Elena and Muntasell, Aura and L{\'{o}}pez-Botet, Miguel and Jensen, Mogens H. and Trusina, Ala and Garcia-Ojalvo, Jordi},
doi = {10.1016/j.bpj.2017.05.010},
file = {:home/jennifer/Descargas/holsthansen2017.pdf:pdf},
issn = {15420086},
journal = {Biophysical Journal},
number = {1},
pages = {148--156},
pmid = {28700913},
title = {{Impact of Zygosity on Bimodal Phenotype Distributions}},
volume = {113},
year = {2017}
}
@article{Balasubramanian2017,
abstract = {Variants predicted to result in the loss of function of human genes have attracted interest because of their clinical impact and surprising prevalence in healthy individuals. Here, we present ALoFT (annotation of loss-of-function transcripts), a method to annotate and predict the disease-causing potential of loss-of-function variants. Using data from Mendelian disease-gene discovery projects, we show that ALoFT can distinguish between loss-of-function variants that are deleterious as heterozygotes and those causing disease only in the homozygous state. Investigation of variants discovered in healthy populations suggests that each individual carries at least two heterozygous premature stop alleles that could potentially lead to disease if present as homozygotes. When applied to de novo putative loss-of-function variants in autism-affected families, ALoFT distinguishes between deleterious variants in patients and benign variants in unaffected siblings. Finally, analysis of somatic variants in >6500 cancer exomes shows that putative loss-of-function variants predicted to be deleterious by ALoFT are enriched in known driver genes.},
author = {Balasubramanian, Suganthi and Fu, Yao and Pawashe, Mayur and McGillivray, Patrick and Jin, Mike and Liu, Jeremy and Karczewski, Konrad J. and MacArthur, Daniel G. and Gerstein, Mark},
doi = {10.1038/s41467-017-00443-5},
file = {:home/jennifer/Descargas/s41467-017-00443-5.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pmid = {28851873},
publisher = {Springer US},
title = {{Using ALoFT to determine the impact of putative loss-of-function variants in protein-coding genes}},
url = {http://dx.doi.org/10.1038/s41467-017-00443-5},
volume = {8},
year = {2017}
}
@article{Li2017a,
author = {Li, Marilyn M and Datto, Michael and Duncavage, Eric J and Kulkarni, Shashikant and Lindeman, Neal I and Roy, Somak and Tsimberidou, Apostolia M and Vnencak-jones, Cindy L and Wolff, Daynna J and Younes, Anas and Nikiforova, Marina N},
doi = {10.1016/j.jmoldx.2016.10.002},
file = {:home/jennifer/Descargas/main.pdf:pdf},
issn = {1525-1578},
journal = {The Journal of Molecular Diagnostics},
number = {1},
pages = {4--23},
publisher = {American Society for Investigative Pathology and the Association for Molecular Pathology},
title = {{Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.10.002},
volume = {19},
year = {2017}
}
@article{Laboratories2015,
author = {Laboratories, Knight Diagnostic and Genetics, Medical and Health, Oregon and Road, Plank and Molecular, Clinical and Children, Nationwide and State, Ohio},
doi = {10.1038/gim.2015.30.Standards},
file = {:home/jennifer/Descargas/nihms697486.pdf:pdf},
journal = {Medical Genetics},
number = {5},
pages = {405--424},
title = {{Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology}},
volume = {17},
year = {2015}
}
@article{Peng2018,
abstract = {OBJECTIVE
Data quality assessment is a challenging facet for research using coded administrative health data. Current assessment approaches are time and resource intensive. We explored whether association rule mining (ARM) can be used to develop rules for assessing data quality. 

MATERIALS AND METHODS
We extracted 2013 and 2014 records from the hospital discharge abstract database (DAD) for patients between the ages of 55 and 65 from five acute care hospitals in Alberta, Canada. The ARM was conducted using the 2013 DAD to extract rules with support ≥ 0.0019 and confidence ≥ 0.5 using the bootstrap technique, and tested in the 2014 DAD. The rules were compared against the method of coding frequency and assessed for their ability to detect error introduced by two kinds of data manipulation: random permutation and random deletion. 

RESULTS
The association rules generally had clear clinical meanings. Comparing 2014 data to 2013 data (both original), there were 3 rules with a confidence difference > 0.1, while coding frequency difference of codes in the right hand of rules was less than 0.004. After random permutation of 50% of codes in the 2014 data, average rule confidence dropped from 0.72 to 0.27 while coding frequency remained unchanged. Rule confidence decreased with the increase of coding deletion, as expected. Rule confidence was more sensitive to code deletion compared to coding frequency, with slope of change ranging from 1.7 to 184.9 with a median of 9.1. 

CONCLUSION
The ARM is a promising technique to assess data quality. It offers a systematic way to derive coding association rules hidden in data, and potentially provides a sensitive and efficient method of assessing data quality compared to standard methods.},
author = {Peng, Mingkai and Sundararajan, Vijaya and Williamson, Tyler and Minty, Evan P. and Smith, Tony C. and Doktorchik, Chelsea T.A. and Quan, Hude},
doi = {10.1016/j.jbi.2018.02.001},
file = {:home/jennifer/Descargas/10.1016@j.jbi.2018.02.001.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
pmid = {29425732},
title = {{Exploration of association rule mining for coding consistency and completeness assessment in inpatient administrative health data}},
url = {https://doi.org/10.1016/j.jbi.2018.02.001},
year = {2018}
}
@article{Wu2017a,
abstract = {Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/Descargas/wu2016.pdf:pdf},
isbn = {0018-9294 VO  - 64},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {263--273},
pmid = {28113246},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
url = {http://ieeexplore.ieee.org/document/7587347/},
volume = {64},
year = {2017}
}
@article{Dave2017,
abstract = {The era of huge data is snowballing at frequent swiftness in size (volume) and in different formats (variety). This data which comes from various sources e.g. media, communication devices, internet, business etc. and there are many difficulties and challenges that one faces while handling it. Data mining is a process intended to reconnoiter analytical data (typically business or market associated data - also acknowledged as "Big data"). There are several data mining techniques such as outlier analysis, organization, clustering, prediction and association rule mining. In this paper we have discussed several applications and the importance of clustering. To examine the huge volume of data, clustering algorithms aid in providing a powerful meta-Iearning tool. Numerous clustering techniques (including traditional and the recently developed) in reference to large data sets with their pros & cons are being discussed in this paper.},
author = {Dave, Meenu and Gianey, Hemant},
doi = {10.1109/SYSMART.2016.7894544},
file = {:home/jennifer/Descargas/dave2016.pdf:pdf},
isbn = {9781509035434},
journal = {Proceedings of the 5th International Conference on System Modeling and Advancement in Research Trends, SMART 2016},
keywords = {Clustering,Data Mining (DM),Density Based Methods (DBM),Grid Based Methods (GBM),Hierarchical Methods (HM),Partition Methods (PM)},
pages = {328--333},
title = {{Different clustering algorithms for Big Data analytics: A review}},
year = {2017}
}
@article{Ramasamy2017,
author = {Ramasamy, S. and Nirmala, K.},
doi = {10.1080/1206212X.2017.1396415},
file = {:home/jennifer/Descargas/dies.pdf:pdf},
issn = {1206-212X},
journal = {International Journal of Computers and Applications},
keywords = {Data mining,association,association rule,classification,data mining,keyword-based clustering},
number = {December},
pages = {1--8},
publisher = {Taylor & Francis},
title = {{Disease prediction in data mining using association rule mining and keyword based clustering algorithms}},
url = {https://www.tandfonline.com/doi/full/10.1080/1206212X.2017.1396415},
volume = {7074},
year = {2017}
}
@article{Wu2017,
abstract = {—Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant in-formation for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Re-sults: To demonstrate how big data analytics enables preci-sion medicine, we provide two case studies, including iden-tifying disease biomarkers from multi-omic data and incor-porating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal im-pact. Index Terms—Big data analytics, bioinformatics, elec-tronic health records (EHRs), health informatics, –omic data, precision medicine.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D and Venugopalan, Janani and Hoffman, Ryan and Wang, May D},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
number = {2},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
volume = {64},
year = {2017}
}
@article{Wu2017b,
abstract = {—Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant in-formation for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Re-sults: To demonstrate how big data analytics enables preci-sion medicine, we provide two case studies, including iden-tifying disease biomarkers from multi-omic data and incor-porating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal im-pact. Index Terms—Big data analytics, bioinformatics, elec-tronic health records (EHRs), health informatics, –omic data, precision medicine.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D and Venugopalan, Janani and Hoffman, Ryan and Wang, May D},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
number = {2},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
volume = {64},
year = {2017}
}
@misc{He,
abstract = {Genomic medicine attempts to build individualized strategies for diagnostic or therapeutic decision-making by utilizing patients' genomic information. Big Data analytics uncovers hidden patterns, unknown correlations, and other insights through examining large-scale various data sets. While integration and manipulation of diverse genomic data and comprehensive electronic health records (EHRs) on a Big Data infrastructure exhibit challenges, they also provide a feasible opportunity to develop an efficient and effective approach to identify clinically actionable genetic variants for individualized diagnosis and therapy. In this paper, we review the challenges of manipulating large-scale next-generation sequencing (NGS) data and diverse clinical data derived from the EHRs for genomic medicine. We introduce possible solutions for different challenges in manipulating, managing, and analyzing genomic and clinical data to implement genomic medicine. Additionally, we also present a practical Big Data toolset for identifying clinically actionable genetic variants using high-throughput NGS data and EHRs.},
author = {He, Karen Y. and Ge, Dongliang and He, Max M},
booktitle = {International Journal of Molecular Sciences},
doi = {10.3390/ijms18020412},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Ge, He - Unknown - Big Data Analytics for Genomic Medicine.pdf:pdf},
isbn = {14220067 (Electronic)},
issn = {14220067},
keywords = {Big data analytics,Clinically actionable genetic variants,Electronic health records,Healthcare,Next-generation sequencing},
number = {2},
pmid = {28212287},
title = {{Big data analytics for genomic medicine}},
volume = {18},
year = {2017}
}
@article{Roy2018,
abstract = {Bioinformatics pipelines are an integral component of next-generation sequencing (NGS). Processing raw sequence data to detect genomic alterations has significant impact on disease management and patient care. Because of the lack of published guidance, there is currently a high degree of variability in how members of the global molecular genetics and pathology community establish and validate bioinformatics pipelines. Improperly developed, validated, and/or monitored pipelines may generate inaccurate results that may have negative consequences for patient care. To address this unmet need, the Association of Molecular Pathology, with organizational representation from the College of American Pathologists and the American Medical Informatics Association, has developed a set of 17 best practice consensus recommendations for the validation of clinical NGS bioinformatics pipelines. Recommendations include practical guidance for laboratories regarding NGS bioinformatics pipeline design, development, and operation, with additional emphasis on the role of a properly trained and qualified molecular professional to achieve optimal NGS testing quality.},
author = {Roy, Somak and Coldren, Christopher and Karunamurthy, Arivarasan and Kip, Nefize S. and Klee, Eric W. and Lincoln, Stephen E. and Leon, Annette and Pullambhatla, Mrudula and Temple-Smolkin, Robyn L. and Voelkerding, Karl V. and Wang, Chen and Carter, Alexis B.},
doi = {10.1016/j.jmoldx.2017.11.003},
file = {:home/jennifer/Descargas/PIIS1525157817303732.pdf:pdf},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {1},
pages = {4--27},
pmid = {29154853},
title = {{Standards and Guidelines for Validating Next-Generation Sequencing Bioinformatics Pipelines: A Joint Recommendation of the Association for Molecular Pathology and the College of American Pathologists}},
volume = {20},
year = {2018}
}
@article{Holzinger2013,
author = {Holzinger, Andreas and Zupan, Mario},
doi = {10.1186/1471-2105-14-191},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holzinger, Zupan - 2013 - KNODWAT A scientific framework application for testing knowledge discovery methods for the biomedical domain.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {191},
publisher = {BioMed Central},
title = {{KNODWAT: A scientific framework application for testing knowledge discovery methods for the biomedical domain}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-191},
volume = {14},
year = {2013}
}
@article{Robinson2017,
abstract = {Manual review of aligned reads for confirmation and interpretation of variant calls is an important step in many variant calling pipelines for next-generation sequencing (NGS) data. Visual inspection can greatly increase the confidence in calls, reduce the risk of false positives, and help characterize complex events. The Integrative Genomics Viewer (IGV) was one of the first tools to provide NGS data visualization, and it currently provides a rich set of tools for inspection, validation, and interpretation of NGS datasets, as well as other types of genomic data. Here, we present a short overview of IGV's variant review features for both single-nucleotide variants and structural variants, with examples from both cancer and germline datasets. IGV is freely available at https://www.igv.org Cancer Res; 77(21); e31-34. {\textcopyright}2017 AACR.},
author = {Robinson, James T. and Thorvaldsd{\'{o}}ttir, Helga and Wenger, Aaron M. and Zehir, Ahmet and Mesirov, Jill P.},
doi = {10.1158/0008-5472.CAN-17-0337},
issn = {0008-5472},
journal = {Cancer Research},
month = {nov},
number = {21},
pages = {e31--e34},
pmid = {29092934},
title = {{Variant Review with the Integrative Genomics Viewer}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29092934 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5678989 http://cancerres.aacrjournals.org/lookup/doi/10.1158/0008-5472.CAN-17-0337},
volume = {77},
year = {2017}
}
@article{Lubin2017,
abstract = {A national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications. This flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants. To provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settings, use of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard to the existing variant file specifications presently used in the clinical setting. Adoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities.},
author = {Lubin, Ira M. and Aziz, Nazneen and Babb, Lawrence J. and Ballinger, Dennis and Bisht, Himani and Church, Deanna M. and Cordes, Shaun and Eilbeck, Karen and Hyland, Fiona and Kalman, Lisa and Landrum, Melissa and Lockhart, Edward R. and Maglott, Donna and Marth, Gabor and Pfeifer, John D. and Rehm, Heidi L. and Roy, Somak and Tezak, Zivana and Truty, Rebecca and Ullman-Cullere, Mollie and Voelkerding, Karl V. and Worthey, Elizabeth A. and Zaranek, Alexander W. and Zook, Justin M.},
doi = {10.1016/j.jmoldx.2016.12.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lubin et al. - 2017 - Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinica.pdf:pdf},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {3},
pages = {417--426},
pmid = {28315672},
publisher = {Elsevier Inc},
title = {{Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinical Settings}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.12.001},
volume = {19},
year = {2017}
}
@article{,
doi = {10.1016/J.JMOLDX.2016.12.001},
issn = {1525-1578},
journal = {The Journal of Molecular Diagnostics},
month = {may},
number = {3},
pages = {417--426},
publisher = {Elsevier},
title = {{Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinical Settings}},
url = {http://www.sciencedirect.com/science/article/pii/S152515781730106X},
volume = {19},
year = {2017}
}
@article{Terlizzi2017,
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and Miriam, Antonella and Lullo, Di and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Terlizzi et al. - 2017 - Genotype – phenotype correlation and functional studies in patients with cystic fi brosis bearing CFTR comple.pdf:pdf},
journal = {Journal of Medical Genetics},
pages = {224--235},
title = {{Genotype – phenotype correlation and functional studies in patients with cystic fi brosis bearing CFTR complex alleles}},
volume = {54},
year = {2017}
}
@article{Wenger2017,
abstract = {Manual review of aligned reads for confirmation an; reduce the risk of false positives; and help characterize complex events. The Integrat; and it currently provides a rich set of tools for; validation; and interpretation of NGS datasets; as well as other types of genomic data. Here; we present a short overview of IGV's variant revie; with examples from both cancer and germline datase},
author = {Wenger, Aaron M and Robinson, James T and Zehir, Ahmet and Mesirov, Jill P},
doi = {10.1158/0008-5472.CAN-17-0337},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenger et al. - 2017 - Variant Review with the Integrative Genomics Viewer.pdf:pdf},
journal = {Cancer research},
number = {21},
pages = {31--35},
title = {{Variant Review with the Integrative Genomics Viewer}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {77},
year = {2017}
}
@article{Jurca2016,
abstract = {BACKGROUND Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. RESULTS We utilized PubMed for the testing. We investigated gene-gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. CONCLUSIONS Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene-gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
isbn = {1756-0500},
issn = {1756-0500},
journal = {BMC Research Notes},
keywords = {Breast cancer,Data mining,Network analysis,Text mining,breast cancer,data mining,network analysis,text mining},
number = {1},
pages = {236},
pmid = {27112211},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@article{Terlizzi2017a,
abstract = {BACKGROUND The effect of complex alleles in cystic fibrosis (CF) is poorly defined for the lack of functional studies. OBJECTIVES To describe the genotype-phenotype correlation and the results of either in vitro and ex vivo studies performed on nasal epithelial cells (NEC) in a cohort of patients with CF carrying cystic fibrosis transmembrane conductance regulator (CFTR) complex alleles. METHODS We studied 70 homozygous, compound heterozygous or heterozygous for CFTR mutations: p.[Arg74Trp;Val201Met;Asp1270Asn], n=8; p.[Ile148Thr;Ile1023_Val1024del], n=5; p.[Arg117Leu;Leu997Phe], n=6; c.[1210-34TG[12];1210-12T[5];2930C>T], n=3; p.[Arg74Trp;Asp1270Asn], n=4; p.Asp1270Asn, n=2; p.Ile148Thr, n=6; p.Leu997Phe, n=36. In 39 patients, we analysed the CFTR gating activity on NEC in comparison with patients with CF (n=8) and carriers (n=4). Finally, we analysed in vitro the p.[Arg74Trp;Val201Met;Asp1270Asn] complex allele. RESULTS The p.[Ile148Thr;Ile1023_Val1024del] caused severe CF in five compound heterozygous with a class I-II mutation. Their CFTR activity on NEC was comparable with patients with two class I-II mutations (mean 7.3% vs 6.9%). The p.[Arg74Trp;Asp1270Asn] and the p.Asp1270Asn have scarce functional effects, while p.[Arg74Trp;Val201Met;Asp1270Asn] caused mild CF in four of five subjects carrying a class I-II mutation in trans, or CFTR-related disorders (CFTR-RD) in three having in trans a class IV-V mutation. The p.[Arg74Trp;Val201Met;Asp1270Asn] causes significantly (p<0.001) higher CFTR activity compared with compound heterozygous for class I-II mutations. Furthermore, five of six compounds heterozygous with the p.[Arg117Leu;Leu997Phe] had mild CF, whereas the p.Leu997Phe, in trans with a class I-II CFTR mutation, caused CFTR-RD or a healthy status (CFTR activity: 21.3-36.9%). Finally, compounds heterozygous for the c.[1210-34TG[12];1210-12T[5];2930C>T] and a class I-II mutation had mild CF or CFTR-RD (gating activity: 18.5-19.0%). CONCLUSIONS The effect of complex alleles partially depends on the mutation in trans. Although larger studies are necessary, the CFTR activity on NEC is a rapid contributory tool to classify patients with CFTR dysfunction.},
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and {Di Lullo}, Antonella Miriam and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Sofia, Valentina Maria and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
issn = {0022-2593},
journal = {Journal of Medical Genetics},
keywords = {[I148T;3199del6bp],[L997F;R117L],[R74W;V201M;D1270N],gating activity,nasal brushing},
month = {apr},
number = {4},
pages = {224--235},
pmid = {27738188},
title = {{Genotype–phenotype correlation and functional studies in patients with cystic fibrosis bearing CFTR complex alleles}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27738188 http://jmg.bmj.com/lookup/doi/10.1136/jmedgenet-2016-103985},
volume = {54},
year = {2017}
}
@article{Jurca2016a,
abstract = {Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. We utilized PubMed for the testing. We investigated gene–gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene–gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
issn = {1756-0500},
journal = {BMC Research Notes},
keywords = {Biomedicine general,Life Sciences,Medicine/Public Health,general},
month = {dec},
number = {1},
pages = {236},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@article{Uppu2014,
abstract = {There have been many studies that depict genotype- phenotype relationships by identifying genetic variants associated with a specific disease. Researchers focus more attention on interactions between SNPs that are strongly associated with disease in the absence of main effect. In this context, a number of machine learning and data mining tools are applied to identify the combinations of multi-locus SNPs in higher order data. However, none of the current models can identify useful SNP- SNP interactions for high dimensional genome data. Detecting these interactions is challenging due to bio-molecular complexities and computational limitations. The goal of this research was to implement associative classification and study its effectiveness for detecting the epistasis in balanced and imbalanced datasets. The proposed approach was evaluated for two locus epistasis interactions using simulated data. The datasets were generated for 5 different penetrance functions by varying heritability, minor allele frequency and sample size. In total, 23,400 datasets were generated and several experiments are conducted to identify the disease causal SNP interactions. The accuracy of classification by the proposed approach was compared with the previous approaches. Though associative classification showed only relatively small improvement in accuracy for balanced datasets, it outperformed existing approaches in higher order multi-locus interactions in imbalanced datasets. Keywords—},
author = {Uppu, Suneetha and Krishna, Aneesh and Gopalan, Raj P.},
doi = {10.1109/BIBE.2014.29},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uppu, Krishna, Gopalan - 2014 - An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Geno.pdf:pdf},
isbn = {978-1-4799-7502-0},
journal = {2014 IEEE International Conference on Bioinformatics and Bioengineering},
keywords = {3,associative classification,billion-base human genome,epistasis,estimated that about 12,million snps occur along,multi-locus,snp-snp interactions,the 3-,the consequences of snps},
pages = {329--333},
title = {{An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Genome}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033602},
year = {2014}
}
@inproceedings{Hu2011,
author = {Hu, Xiaohua},
booktitle = {2011 IEEE International Conference on Granular Computing},
doi = {10.1109/GRC.2011.6122559},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu - 2011 - Data mining and its applications in bioinformatics Techniques and methods.pdf:pdf},
isbn = {978-1-4577-0371-3},
month = {nov},
pages = {3--3},
publisher = {IEEE},
title = {{Data mining and its applications in bioinformatics: Techniques and methods}},
url = {http://ieeexplore.ieee.org/document/6122559/},
year = {2011}
}
@article{,
file = {:home/jennifer/Descargas/116624.full.pdf:pdf},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
year = {2017}
}
@misc{Littlefield,
author = {Littlefield, Rayan},
title = {{An introduction into Data Mining in Bioinformatics.}},
url = {https://littlefield.co/an-introduction-into-data-mining-in-bioinformatics-964511e9ea21},
urldate = {2017-11-19}
}
@misc{,
title = {{An introduction into Data Mining in Bioinformatics.}},
url = {https://littlefield.co/an-introduction-into-data-mining-in-bioinformatics-964511e9ea21},
urldate = {2017-11-19}
}
@article{Breuer2017,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breuer et al. - 2017 - Genotype-phenotype association mining in bipolar disorder market research meets complex genetics.pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {116624},
publisher = {Cold Spring Harbor Laboratory},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@incollection{Agrawal2016,
address = {Boston, MA},
author = {Agrawal, Ankit and Choudhary, Alok},
booktitle = {Data and Measures in Health Services Research},
doi = {10.1007/978-1-4899-7673-4_2-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Choudhary - 2016 - Health Services Data Big Data Analytics for Deriving Predictive Healthcare Insights.pdf:pdf},
pages = {1--17},
publisher = {Springer US},
title = {{Health Services Data: Big Data Analytics for Deriving Predictive Healthcare Insights}},
url = {http://link.springer.com/10.1007/978-1-4899-7673-4_2-1},
year = {2016}
}
@article{,
doi = {10.1016/J.INS.2016.01.094},
issn = {0020-0255},
journal = {Information Sciences},
month = {jun},
pages = {146--162},
publisher = {Elsevier},
title = {{Computing exact permutation p-values for association rules}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300366},
volume = {346-347},
year = {2016}
}
@article{Sherry2001,
author = {Sherry, S. T. and Ward, M.-H. and Kholodov, M. and Baker, J. and Phan, L. and Smigielski, E. M. and Sirotkin, K.},
doi = {10.1093/nar/29.1.308},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sherry et al. - 2001 - dbSNP the NCBI database of genetic variation.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
keywords = {biotechnology,chromosome mapping,genbank,genetics,genome,genome, human,national library of medicine (u.s.),patients' rooms,single nucleotide polymorphism,united states national institutes of health},
month = {jan},
number = {1},
pages = {308--311},
publisher = {Oxford University Press},
title = {{dbSNP: the NCBI database of genetic variation}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/29.1.308},
volume = {29},
year = {2001}
}
@article{Yates2016,
abstract = {The Ensembl project (http://www.ensembl.org) is a system for genome annotation, analysis, storage and dissemination designed to facilitate the access of genomic annotation from chordates and key model organisms. It provides access to data from 87 species across our main and early access Pre! websites. This year we introduced three newly annotated species and released numerous updates across our supported species with a concentration on data for the latest genome assemblies of human, mouse, zebrafish and rat. We also provided two data updates for the previous human assembly, GRCh37, through a dedicated website (http://grch37.ensembl.org). Our tools, in particular the VEP, have been improved significantly through integration of additional third party data. REST is now capable of larger-scale analysis and our regulatory data BioMart can deliver faster results. The website is now capable of displaying long-range interactions such as those found in cis-regulated datasets. Finally we have launched a website optimized for mobile devices providing views of genes, variants and phenotypes. Our data is made available without restriction and all code is available from our GitHub organization site (http://github.com/Ensembl) under an Apache 2.0 license.},
author = {Yates, Andrew and Akanni, Wasiu and Amode, M Ridwan and Barrell, Daniel and Billis, Konstantinos and Carvalho-Silva, Denise and Cummins, Carla and Clapham, Peter and Fitzgerald, Stephen and Gil, Laurent and Gir{\'{o}}n, Carlos Garc{\'{i}}a and Gordon, Leo and Hourlier, Thibaut and Hunt, Sarah E and Janacek, Sophie H and Johnson, Nathan and Juettemann, Thomas and Keenan, Stephen and Lavidas, Ilias and Martin, Fergal J and Maurel, Thomas and McLaren, William and Murphy, Daniel N and Nag, Rishi and Nuhn, Michael and Parker, Anne and Patricio, Mateus and Pignatelli, Miguel and Rahtz, Matthew and Riat, Harpreet Singh and Sheppard, Daniel and Taylor, Kieron and Thormann, Anja and Vullo, Alessandro and Wilder, Steven P and Zadissa, Amonida and Birney, Ewan and Harrow, Jennifer and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and Spudich, Giulietta and Trevanion, Stephen J and Cunningham, Fiona and Aken, Bronwen L and Zerbino, Daniel R and Flicek, Paul},
doi = {10.1093/nar/gkv1157},
issn = {1362-4962},
journal = {Nucleic acids research},
month = {jan},
number = {D1},
pages = {D710--6},
pmid = {26687719},
publisher = {Oxford University Press},
title = {{Ensembl 2016.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26687719 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4702834},
volume = {44},
year = {2016}
}
@article{Hubbard2002,
abstract = {The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.},
author = {Hubbard, T and Barker, D and Birney, E and Cameron, G and Chen, Y and Clark, L and Cox, T and Cuff, J and Curwen, V and Down, T and Durbin, R and Eyras, E and Gilbert, J and Hammond, M and Huminiecki, L and Kasprzyk, A and Lehvaslaiho, H and Lijnzaad, P and Melsopp, C and Mongin, E and Pettett, R and Pocock, M and Potter, S and Rust, A and Schmidt, E and Searle, S and Slater, G and Smith, J and Spooner, W and Stabenau, A and Stalker, J and Stupka, E and Ureta-Vidal, A and Vastrik, I and Clamp, M},
issn = {1362-4962},
journal = {Nucleic acids research},
month = {jan},
number = {1},
pages = {38--41},
pmid = {11752248},
publisher = {Oxford University Press},
title = {{The Ensembl genome database project.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11752248 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC99161},
volume = {30},
year = {2002}
}
@misc{variome2017,
title = {{About the Human Variome Project: what we do and why we do it - Human Variome Project}},
url = {http://www.humanvariomeproject.org/about/about-the-human-variome-project.html},
urldate = {2017-11-19}
}
@article{Higasa2016,
abstract = {Whole-genome and -exome resequencing using next-generation sequencers is a powerful approach for identifying genomic variations that are associated with diseases. However, systematic strategies for prioritizing causative variants from many candidates to explain the disease phenotype are still far from being established, because the population-specific frequency spectrum of genetic variation has not been characterized. Here, we have collected exomic genetic variation from 1208 Japanese individuals through a collaborative effort, and aggregated the data into a prevailing catalog. In total, we identified 156 622 previously unreported variants. The allele frequencies for the majority (88.8%) were lower than 0.5% in allele frequency and predicted to be functionally deleterious. In addition, we have constructed a Japanese-specific major allele reference genome by which the number of unique mapping of the short reads in our data has increased 0.045% on average. Our results illustrate the importance of constructing an ethnicity-specific reference genome for identifying rare variants. All the collected data were centralized to a newly developed database to serve as useful resources for exploring pathogenic variations. Public access to the database is available at INTRODUCTION Next-generation sequencing technologies are revolutionizing the approach in identifying genetic variants that are associated with diseases. A current promising strategy focuses on rare variants that},
author = {Higasa, Koichiro and Miyake, Noriko and Yoshimura, Jun and Okamura, Kohji and Niihori, Tetsuya and Saitsu, Hirotomo and Doi, Koichiro and Shimizu, Masakazu and Nakabayashi, Kazuhiko and Aoki, Yoko and Tsurusaki, Yoshinori and Morishita, Shinichi and Kawaguchi, Takahisa and Migita, Osuke and Nakayama, Keiko and Nakashima, Mitsuko and Mitsui, Jun and Narahara, Maiko and Hayashi, Keiko and Funayama, Ryo and Yamaguchi, Daisuke and Ishiura, Hiroyuki and Ko, Wen-Ya and Hata, Kenichiro and Nagashima, Takeshi and Yamada, Ryo and Matsubara, Yoichi and Umezawa, Akihiro and Tsuji, Shoji and Matsumoto, Naomichi and Matsuda, Fumihiko},
doi = {10.1038/jhg.2016.12},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higasa et al. - 2016 - Human genetic variation database, a reference database of genetic variations in the Japanese population.pdf:pdf},
issn = {1434-5161},
journal = {Journal of Human Genetics},
number = {6},
pages = {547--553},
pmid = {26911352},
publisher = {Nature Publishing Group},
title = {{Human genetic variation database, a reference database of genetic variations in the Japanese population}},
url = {http://www.nature.com/doifinder/10.1038/jhg.2016.12},
volume = {61},
year = {2016}
}
@article{Kutzera2017,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-642-15120-0},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Data Integration in the Life Sciences.pdf:pdf},
isbn = {978-3-642-15119-4},
pages = {22--28},
title = {{Data Integration in the Life Sciences}},
url = {http://link.springer.com/10.1007/978-3-642-15120-0},
volume = {6254},
year = {2017}
}
@article{Davidson2017,
abstract = {Motivation: RNA-Seq analyses can benefit from performing a genome-guided and de novo assem- bly, in particular for species where the reference genome is incomplete. However, tools to integrate assembled transcriptome with reference annotation are lacking. Results: Necklace is a software pipeline to run genome-guided and de novo assembly and combine the resulting transcriptomes with reference genome annotations. Necklace constructs a compact but comprehensive superTranscriptome out of the assembled and reference data. Reads are subse- quently aligned and counted in preparation for differential expression testing. Availability: Necklace is available from https://github.com/Oshlack/necklace/wiki under GPL 3.0. Contact: nadia.davidson@mcri.edu.au or alicia.oshlack@mcri.edu.au},
archivePrefix = {arXiv},
arxivId = {103549},
author = {Davidson, Nadia M and Oshlack, Alicia},
doi = {10.1093/bioinformatics/xxxxx},
eprint = {103549},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, Oshlack - 2017 - Necklace combining reference and assembled transcriptomes for RNA-Seq analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
pages = {0--0},
title = {{Necklace: combining reference and assembled transcriptomes for RNA-Seq analysis}},
url = {https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/PAP/10.1093_bioinformatics_btx400/1/btx400.pdf?Expires=1498600002&Signature=NUtYYcdBpBQNhzgudJGKJAFp6hRQqryxL76tZJuI1v1V7Eh15Dwr9tfCwfud5GIZHEv4qb2hVBrPwLzSGZ8zh2jNkPnFA$\sim$Yd},
year = {2017}
}
@incollection{Kutzera2017a,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
year = {2017}
}
@incollection{Kutzera2017b,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
year = {2017}
}
@article{Luo2017,
author = {Luo, Ping and Ruan, Jishou and Member, Senior},
doi = {10.1109/TCBB.2017.2770120},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Ruan, Member - 2017 - Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data.pdf:pdf},
journal = {IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS 1 Disease},
number = {306},
pages = {1--11},
title = {{Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data}},
volume = {1},
year = {2017}
}
@article{Wang2017,
author = {Wang, Fei and Li, Xiao-li and Wang, Jason T L and Ng, See-kiong},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Guest Editorial Special Section on Biological Data Mining and Its Applications in Healthcare.pdf:pdf},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
volume = {14},
year = {2017}
}
@article{Pietrelli2017,
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management(2).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
publisher = {Oxford University Press},
title = {{myVCF: a desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@article{Wang2017a,
author = {Wang, Fei and Li, Xiao-Li and Wang, Jason T. L. and Ng, See-Kiong},
doi = {10.1109/TCBB.2016.2612558},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {may},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
url = {http://ieeexplore.ieee.org/document/7938559/},
volume = {14},
year = {2017}
}
@article{Oellrich2014,
author = {Oellrich, Anika and Jacobsen, Julius and Papatheodorou, Irene and Smedley, Damian},
doi = {10.1093/bioinformatics/btu260},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oellrich et al. - 2014 - Using association rule mining to determine promising secondary phenotyping hypotheses.pdf:pdf},
issn = {1460-2059},
journal = {Bioinformatics},
month = {jun},
number = {12},
pages = {i52--i59},
publisher = {Oxford University Press},
title = {{Using association rule mining to determine promising secondary phenotyping hypotheses}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu260},
volume = {30},
year = {2014}
}
@article{Shendure2016,
author = {Shendure, Jay},
doi = {10.1038/536277a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shendure - 2016 - Human genomics A deep dive into genetic variation.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Genetics,Genomics},
month = {aug},
number = {7616},
pages = {277--278},
publisher = {Nature Publishing Group},
title = {{Human genomics: A deep dive into genetic variation}},
url = {http://www.nature.com/doifinder/10.1038/536277a},
volume = {536},
year = {2016}
}
@article{Feero2017,
author = {Feero, W. Gregory},
doi = {10.1001/jama.2016.20625},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feero - 2017 - Introducing “Genomics and Precision Health”(2).pdf:pdf},
issn = {0098-7484},
journal = {JAMA},
keywords = {genomics,precision medicine},
month = {may},
number = {18},
pages = {1842},
publisher = {American Medical Association},
title = {{Introducing “Genomics and Precision Health”}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.20625},
volume = {317},
year = {2017}
}
@article{Poliakov2015,
author = {Poliakov, Eugenia and Cooper, David N and Stepchenkova, Elena I and Rogozin, Igor B},
doi = {10.1155/2015/364960},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poliakov et al. - 2015 - Genetics in genomic era.pdf:pdf},
issn = {2090-3154},
journal = {Genetics research international},
pages = {364960},
pmid = {25883807},
publisher = {Hindawi},
title = {{Genetics in genomic era.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25883807 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4390167},
volume = {2015},
year = {2015}
}
@article{Searls2010,
author = {Searls, David B.},
doi = {10.1371/journal.pcbi.1000809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Searls - 2010 - The Roots of Bioinformatics.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {jun},
number = {6},
pages = {e1000809},
publisher = {Public Library of Science},
title = {{The Roots of Bioinformatics}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1000809},
volume = {6},
year = {2010}
}
@article{Salter2017,
abstract = {The rise of bioinformatics is a direct response to the political difficulties faced by genomics in its quest to be a new biomedical innovation, and the value of bioinformatics lies in its role as the bridge between the promise of genomics and its realization in the form of health benefits. Western scientific elites are able to use their close relationship with the state to control and facilitate the emergence of new domains compatible with the existing distribution of epistemic power – all within the embrace of public trust. The incorporation of bioinformatics as the saviour of genomics had to be integrated with the operation of two key aspects of governance in this field: the definition and ownership of the new knowledge. This was achieved mainly by the development of common standards and by the promotion of the values of communality, open access and the public ownership of data to legitimize and maintain the governance power of publicly funded genomic science. Opposition from industry advocating the pri...},
author = {Salter, Brian and Salter, Charlotte},
doi = {10.1177/0306312716681210},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salter, Salter - 2017 - Controlling new knowledge Genomic science, governance and the politics of bioinformatics.pdf:pdf},
issn = {0306-3127},
journal = {Social Studies of Science},
keywords = {bioinformatics,genomics,governance,ideology,politics},
month = {apr},
number = {2},
pages = {263--287},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Controlling new knowledge: Genomic science, governance and the politics of bioinformatics}},
url = {http://journals.sagepub.com/doi/10.1177/0306312716681210},
volume = {47},
year = {2017}
}
@article{Mayer2017,
author = {Mayer, Gerhard and Quast, Christian and Felden, Janine and Lange, Matthias and Prinz, Manuel and P{\"{u}}hler, Alfred and Lawerenz, Chris and Scholz, Uwe and Gl{\"{o}}ckner, Frank Oliver and M{\"{u}}ller, Wolfgang and Marcus, Katrin and Eisenacher, Martin},
doi = {10.1093/bib/bbx140},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer et al. - 2017 - A generally applicable lightweight method for calculating a value structure for tools and services in bioinformati.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {oct},
title = {{A generally applicable lightweight method for calculating a value structure for tools and services in bioinformatics infrastructure projects}},
url = {http://academic.oup.com/bib/article/doi/10.1093/bib/bbx140/4582343},
year = {2017}
}
@article{Fong2017,
author = {Fong, Kenneth and Bailey, Celeste V. and Tuttle, Peggy and Cunningham, Bari and McGrath, John A. and Cho, Raymond J.},
doi = {10.1111/pde.13029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong et al. - 2017 - Questioning the Clinical Utility of Exome Sequencing in Developing Countries.pdf:pdf},
issn = {07368046},
journal = {Pediatric Dermatology},
month = {jan},
number = {1},
pages = {e32--e34},
title = {{Questioning the Clinical Utility of Exome Sequencing in Developing Countries}},
url = {http://doi.wiley.com/10.1111/pde.13029},
volume = {34},
year = {2017}
}
@article{Pietrelli2017a,
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
publisher = {Oxford University Press},
title = {{myVCF: a desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@article{,
doi = {10.1016/J.GHEART.2017.01.005},
issn = {2211-8160},
journal = {Global Heart},
month = {jun},
number = {2},
pages = {91--98},
publisher = {Elsevier},
title = {{Development of Bioinformatics Infrastructure for Genomics Research}},
url = {http://www.sciencedirect.com/science/article/pii/S2211816017300054},
volume = {12},
year = {2017}
}
@article{Merelli2014,
abstract = {The explosion of the data both in the biomedical research and in the healthcare systems demands urgent solutions. In particular, the research in omics sciences is moving from a hypothesis-driven to a data-driven approach. Healthcare is additionally always asking for a tighter integration with biomedical data in order to promote personalized medicine and to provide better treatments. Efficient analysis and interpretation of Big Data opens new avenues to explore molecular biology, new questions to ask about physiological and pathological states, and new ways to answer these open issues. Such analyses lead to better understanding of diseases and development of better and personalized diagnostics and therapeutics. However, such progresses are directly related to the availability of new solutions to deal with this huge amount of information. New paradigms are needed to store and access data, for its annotation and integration and finally for inferring knowledge and making it available to researchers. Bioinformatics can be viewed as the "glue" for all these processes. A clear awareness of present high performance computing (HPC) solutions in bioinformatics, Big Data analysis paradigms for computational biology, and the issues that are still open in the biomedical and healthcare fields represent the starting point to win this challenge.},
author = {Merelli, Ivan and P{\'{e}}rez-S{\'{a}}nchez, Horacio and Gesing, Sandra and D'Agostino, Daniele},
doi = {10.1155/2014/134023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Merelli et al. - 2014 - Managing, analysing, and integrating big data in medical bioinformatics open problems and future perspectives.pdf:pdf},
issn = {2314-6141},
journal = {BioMed research international},
month = {sep},
pages = {134023},
pmid = {25254202},
publisher = {Hindawi},
title = {{Managing, analysing, and integrating big data in medical bioinformatics: open problems and future perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25254202 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4165507},
volume = {2014},
year = {2014}
}
@article{Zaki2007,
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
issn = {1748-7188},
journal = {Algorithms for molecular biology : AMB},
month = {apr},
pages = {4},
pmid = {17428327},
publisher = {BioMed Central},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17428327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1852315},
volume = {2},
year = {2007}
}
@article{Lonardi2010,
author = {Lonardi, Stefano and Chen, Jake},
doi = {10.1109/TCBB.2010.28},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {apr},
number = {2},
pages = {195--196},
title = {{Data Mining in Bioinformatics: Selected Papers from BIOKDD}},
url = {http://ieeexplore.ieee.org/document/5460415/},
volume = {7},
year = {2010}
}
@article{Bustos2007,
abstract = {El presente art{\'{i}}culo muestra los resultados de la investigaci{\'{o}}n en la cual se aplic{\'{o}} la metodolog{\'{i}}a Shainin del dise{\~{n}}o experimental en la planta de producci{\'{o}}n de un ingenio azucarero del Valle del Cauca. Este trabajo destaca la importancia que tiene el Dise{\~{n}}o Experimental como herramienta estad{\'{i}}stica para el mejoramiento de procesos productivos, que va m{\'{a}}s all{\'{a}} del simple monitoreo impuesto por las t{\'{e}}cnicas de control estad{\'{i}}stico de procesos, sin desmeritarlas como herramientas {\'{u}}tiles para controlar su rendimiento},
author = {Bustos, Ligia and Moreno, Ricardo and Duque, Nestor},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/SPARC (Organization), Universidad Tecnológica de Pereira., Duque - 1995 - Scientia et technica.pdf:pdf},
issn = {0122-1701},
journal = {Scientia},
keywords = {catequina,condensados,fenoles,florotaninos,s polifenoles,taninos,taninos hidrolizables,{\'{a}}cido el{\'{a}}gico,{\'{a}}cido g{\'{a}}lico},
number = {037},
pages = {13--18},
publisher = {Universidad Tecnológica de Pereira},
title = {{Modelo de una bodega de datos para el soporte a la investigaci{\'{o}}n bioinform{\'{a}}tica}},
url = {http://www.redalyc.org/resumen.oa?id=84922625025 http://revistas.utp.edu.co/index.php/revistaciencia/article/view/4125/2181 http://redalyc.uaemex.mx/src/inicio/ArtPdfRed.jsp?iCve=84903790},
volume = {XIII},
year = {2007}
}
@article{BernalAcevedo2011,
abstract = {86 osCar bernaL-aCevedo • juan CaMiLo forero-CaMaCHo Rev. Gerenc. Polit. Salud, Bogot{\'{a}} (Colombia), 10 (21): 85-100, julio-diciembre de 2011 Resumen Objetivo: caracterizar y evaluar los sistemas de informaci{\'{o}}n del sector salud en Colombia. Metodolog{\'{i}}a: Se desarroll{\'{o}} un marco conceptual que incluy{\'{o}} contexto legal del pa{\'{i}}s y confor-maci{\'{o}}n de sistemas de informaci{\'{o}}n en otros pa{\'{i}}ses. Posteriormente se caracteriz{\'{o}} el sistema de informaci{\'{o}}n de salud colombiano, a partir de entrevistas con actores relevantes y literatura pertinente. Finalmente, se analiz{\'{o}} la conformaci{\'{o}}n del sistema, el flujo de informaci{\'{o}}n y las fortalezas y debilidades de {\'{e}}ste, para la posterior formulaci{\'{o}}n de recomendaciones. Resultados y conclusiones: el sistema de informaci{\'{o}}n en salud colombiano se encuentra fragmentado y presenta problemas de calidad, situaci{\'{o}}n similar a la de otros pa{\'{i}}ses. Es esencial el desarrollo de una cultura de producci{\'{o}}n, difusi{\'{o}}n y utilizaci{\'{o}}n de la informaci{\'{o}}n. Se debe aprovechar el momento de cambio que sufre el sistema de salud para buscar la mejor{\'{i}}a de la informaci{\'{o}}n. Los mecanismos de captura de la informaci{\'{o}}n requieren una simplificaci{\'{o}}n y estandarizaci{\'{o}}n. Palabras clave autor: servicios de informaci{\'{o}}n, Colombia, comunicaci{\'{o}}n, salud p{\'{u}}blica, informaci{\'{o}}n, administraci{\'{o}}n en salud p{\'{u}}blica.},
author = {{Bernal Acevedo}, Oscar and {Forero Camacho}, Juan Camilo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernal Acevedo, Forero Camacho - 2011 - Sistemas de informaci{\'{o}}n en el sector salud en Colombia.pdf:pdf},
journal = {Revista Gerencia y Politicas de Salud},
keywords = {de informaci{\'{o}}n en el,sector salud},
number = {21},
pages = {85--100},
title = {{Sistemas de informaci{\'{o}}n en el sector salud en Colombia}},
url = {http://www.scielo.org.co/pdf/rgps/v10n21/v10n21a06.pdf},
volume = {10},
year = {2011}
}
@article{Palmisano2016,
abstract = {Trials involving genomic-driven treatment selection require the
coordination of many teams interacting with a great variety of
information. The need of better informatics support to manage this
complex set of operations motivated the creation of OpenGeneMed.
OpenGeneMed is a stand-alone and customizable version of GeneMed (Zhao
et al. GeneMed: an informatics hub for the coordination of
next-generation sequencing studies that support precision oncology
clinical trials. Cancer Inform 2015; 14(Suppl 2): 45), a web-based
interface developed for the National Cancer Institute Molecular
Profiling-based Assignment of Cancer Therapy (NCI-MPACT) clinical trial
coordinated by the NIH. OpenGeneMed streamlines clinical trial
management and it can be used by clinicians, lab personnel,
statisticians and researchers as a communication hub. It automates the
annotation of genomic variants identified by sequencing tumor DNA,
classifies the actionable mutations according to customizable rules and
facilitates quality control in reviewing variants. The system generates
summarized reports with detected genomic alterations that a treatment
review team can use for treatment assignment. OpenGeneMed allows
collaboration to happen seamlessly along the clinical pipeline; it helps
reduce errors made transferring data between groups and facilitates
clear documentation along the pipeline. OpenGeneMed is distributed as a
stand-alone virtual machine, ready for deployment and use from a web
browser; its code is customizable to address specific needs of different
clinical trials and research teams. Examples on how to change the code
are provided in the technical documentation distributed with the virtual
machine. In summary, OpenGeneMed offers an initial set of features
inspired by our experience with GeneMed, a system that has been proven
to be efficient and successful for coordinating the application of
next-generation sequencing in the NCI-MPACT trial.},
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming-Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/Descargas/palmisano2016.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {alida palmisano,and applications to support,and diagnosis,at the biometric research,clinical trial management,clinical trial management system,division of cancer treatment,genomics,her research,his research,interests include development of,is a computational biologist,is a postdoctoral fellow,nci,next-generation sequencing,nih,open source software,phd,precision medicine,program,software for genomic analysis,yingdong zhao},
number = {March},
pages = {bbw059},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
year = {2016}
}
@misc{,
title = {{Galaxy Community Hub}},
url = {https://galaxyproject.org/},
urldate = {2017-11-16}
}
@article{Canuel2015,
abstract = {The rise of personalized medicine and the availability of high-throughput molecular analyses in the context of clinical care have increased the need for adequate tools for translational researchers to manage and explore these data. We reviewed the biomedical literature for translational platforms allowing the management and exploration of clinical and omics data, and identified seven platforms: BRISK, caTRIP, cBio Cancer Portal, G-DOC, iCOD, iDASH and tranSMART. We analyzed these platforms along seven major axes. (1) The community axis regrouped information regarding initiators and funders of the project, as well as availability status and references. (2) We regrouped under the information content axis the nature of the clinical and omics data handled by each system. (3) The privacy management environment axis encompassed functionalities allowing control over data privacy. (4) In the analysis support axis, we detailed the analytical and statistical tools provided by the platforms. We also explored (5) interoperability support and (6) system requirements. The final axis (7) platform support listed the availability of documentation and installation procedures. A large heterogeneity was observed in regard to the capability to manage phenotype information in addition to omics data, their security and interoperability features. The analytical and visualization features strongly depend on the considered platform. Similarly, the availability of the systems is variable. This review aims at providing the reader with the background to choose the platform best suited to their needs. To conclude, we discuss the desiderata for optimal translational research platforms, in terms of privacy, interoperability and technical features.},
author = {Canuel, Vincent and Rance, Bastien and Avillach, Paul and Degoulet, Patrice and Burgun, Anita},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data A review of publicly available solutions.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical research,Clinical data,High-throughput technologies,Information storage and retrieval,Translationalmedical research},
number = {2},
pages = {280--290},
pmid = {24608524},
title = {{Translational research platforms integrating clinical and omics data: A review of publicly available solutions}},
volume = {16},
year = {2015}
}
@article{Palmisano2016a,
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming-Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmisano et al. - 2016 - OpenGeneMed a portable, flexible and customizable informatics hub for the coordination of next-generation sequ.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {jul},
number = {5},
pages = {bbw059},
publisher = {Oxford University Press},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
volume = {18},
year = {2016}
}
@article{Hashem2015,
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. {\textcopyright} 2014 Elsevier Ltd.},
author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
doi = {10.1016/j.is.2014.07.006},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Big data,Cloud computing,Hadoop},
month = {jan},
pages = {98--115},
pmid = {1476196123},
publisher = {Pergamon},
title = {{The rise of "big data" on cloud computing: Review and open research issues}},
url = {http://www.sciencedirect.com/science/article/pii/S0306437914001288},
volume = {47},
year = {2015}
}
@article{Ren2015,
abstract = {The data explosion in the last decade is revolutionizing diagnostics research and the healthcare industry, offering both opportunities and challenges. These high-throughput "omics" techniques have generated more scientific data in the last few years than in the entire history of mankind. Here we present a brief summary of how "big data" have influenced early diagnosis of complex diseases. We will also review some of the most commonly used "omics" techniques and their applications in diagnostics. Finally, we will discuss the issues brought by these new techniques when translating laboratory discoveries to clinical practice.},
author = {Ren, Guomin and Krawetz, Roman},
doi = {10.3109/1354750X.2015.1105499},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Krawetz - 2015 - Applying computation biology and &quotbig data&quot to develop multiplex diagnostics for complex chronic diseases.pdf:pdf},
issn = {1366-5804},
journal = {Biomarkers : biochemical indicators of exposure, response, and susceptibility to chemicals},
keywords = {Chronic disease,computational biology,diagnostics,osteoarthritis},
number = {8},
pages = {533--9},
pmid = {26809774},
publisher = {Taylor & Francis},
title = {{Applying computation biology and "big data" to develop multiplex diagnostics for complex chronic diseases such as osteoarthritis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26809774 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822 http://www.ncbi.nlm.nih.gov/pubmed/26809774%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822},
volume = {20},
year = {2015}
}
@article{Li2014,
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {2210-3244},
journal = {Genomics, proteomics & bioinformatics},
month = {oct},
number = {5},
pages = {187--9},
pmid = {25462151},
publisher = {Elsevier},
title = {{Big biological data: challenges and opportunities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25462151 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4411415},
volume = {12},
year = {2014}
}
@misc{Louie2007,
abstract = {Genomic medicine aims to revolutionize health care by applying our growing understanding of the molecular basis of disease. Research in this arena is data intensive, which means data sets are large and highly heterogeneous. To create knowledge from data, researchers must integrate these large and diverse data sets. This presents daunting informatic challenges such as representation of data that is suitable for computational inference (knowledge representation), and linking heterogeneous data sets (data integration). Fortunately, many of these challenges can be classified as data integration problems, and technologies exist in the area of data integration that may be applied to these challenges. In this paper, we discuss the opportunities of genomic medicine as well as identify the informatics challenges in this domain. We also review concepts and methodologies in the field of data integration. These data integration concepts and methodologies are then aligned with informatics challenges in genomic medicine and presented as potential solutions. We conclude this paper with challenges still not addressed in genomic medicine and gaps that remain in data integration research to facilitate genomic medicine. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Louie, Brenton and Mork, Peter and Martin-Sanchez, Fernando and Halevy, Alon and Tarczy-Hornoch, Peter},
booktitle = {Journal of Biomedical Informatics},
doi = {10.1016/j.jbi.2006.02.007},
file = {:home/jennifer/Descargas/1-s2.0-S1532046406000244-main.pdf:pdf},
isbn = {1532-0480 (Electronic)\n1532-0464 (Linking)},
issn = {15320464},
keywords = {Biomedical informatics,Data integration,Genomic medicine,Genomics,Knowledge representation},
month = {feb},
number = {1},
pages = {5--16},
pmid = {16574494},
publisher = {Academic Press},
title = {{Data integration and genomic medicine}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046406000244},
volume = {40},
year = {2007}
}
@article{Triplet2014,
abstract = {To facilitate the integration and querying of genomics data, a number of generic data warehousing frameworks have been developed. They differ in their design and capabilities, as well as their intended audience. We provide a comprehensive and quantitative review of those genomic data warehousing frameworks in the context of large-scale systems biology. We reviewed in detail four genomic data warehouses (BioMart, BioXRT, InterMine and PathwayTools) freely available to the academic community. We quantified 20 aspects of the warehouses, covering the accuracy of their responses, their computational requirements and development efforts. Performance of the warehouses was evaluated under various hardware configurations to help laboratories optimize hardware expenses. Each aspect of the benchmark may be dynamically weighted by scientists using our online tool BenchDW (http://warehousebenchmark.fungalgenomics.ca/benchmark/) to build custom warehouse profiles and tailor our results to their specific needs.},
author = {Triplet, T. and Butler, G.},
doi = {10.1093/bib/bbt031},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {jul},
number = {4},
pages = {471--483},
pmid = {23673292},
title = {{A review of genomic data warehousing systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23673292 https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbt031},
volume = {15},
year = {2014}
}
@article{Deng2011,
abstract = {BACKGROUND The popularity of massively parallel exome and transcriptome sequencing projects demands new data mining tools with a comprehensive set of features to support a wide range of analysis tasks. RESULTS SeqGene, a new data mining tool, supports mutation detection and annotation, dbSNP and 1000 Genome data integration, RNA-Seq expression quantification, mutation and coverage visualization, allele specific expression (ASE), differentially expressed genes (DEGs) identification, copy number variation (CNV) analysis, and gene expression quantitative trait loci (eQTLs) detection. We also developed novel methods for testing the association between SNP and expression and identifying genotype-controlled DEGs. We showed that the results generated from SeqGene compares favourably to other existing methods in our case studies. CONCLUSION SeqGene is designed as a general-purpose software package. It supports both paired-end reads and single reads generated on most sequencing platforms; it runs on all major types of computers; it supports arbitrary genome assemblies for arbitrary organisms; and it scales well to support both large and small scale sequencing projects. The software homepage is http://seqgene.sourceforge.net.},
author = {Deng, Xutao},
doi = {10.1186/1471-2105-12-267},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng - 2011 - SeqGene a comprehensive software solution for mining exome- and transcriptome- sequencing data.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
month = {jun},
pages = {267},
pmid = {21714929},
publisher = {BioMed Central},
title = {{SeqGene: a comprehensive software solution for mining exome- and transcriptome- sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21714929 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3148209},
volume = {12},
year = {2011}
}
@book{Klug2013,
abstract = {10a. ed. Acompaña código de acceso de 12 meses a los recursos online. Incluye Pearson eText. Índice analítico. Glosario: p. [892]-917.},
author = {Klug, William S. and {Pascual Calaforra}, Luis F.},
isbn = {9788415552482},
publisher = {Pearson Educacin{\'{o}}n},
title = {{Conceptos de genn{\'{e}}tica}},
url = {https://www.casadellibro.com/libro-conceptos-de-genetica/9788415552482/2547204},
year = {2013}
}
@misc{Illumina2017,
author = {Illumina},
title = {{Whole Exome Sequencing | Detect exonic variants}},
url = {http://www.illumina.com/techniques/sequencing/dna-sequencing/targeted-resequencing/exome-sequencing.html},
urldate = {2017-11-15},
year = {2017}
}
@incollection{Kulski2016,
author = {Kulski, Jerzy K.},
booktitle = {Next Generation Sequencing - Advances, Applications and Challenges},
doi = {10.5772/61964},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulski - 2016 - Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications(2).pdf:pdf},
month = {jan},
publisher = {InTech},
title = {{Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications}},
url = {http://www.intechopen.com/books/next-generation-sequencing-advances-applications-and-challenges/next-generation-sequencing-an-overview-of-the-history-tools-and-omic-applications},
year = {2016}
}
@article{Pei,
abstract = {High-throughput next generation sequencing (NGS) has been quickly adapted into many aspects of biomedical research and begun to engage with the clinical practice. The latter aspect will enable the application of genomic knowl- edge into clinical practice in this and next decades and will profoundly change the diagnosis, prognosis and treatment of many human diseases. It will further demand both philosophical and medical curriculum reforms in the training of our future physicians. However, significant huddles need to be overcome before an ultimate application},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {{Pei Hui}},
doi = {10.1007/128},
eprint = {15334406},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pei Hui - 2012 - Next Generation Sequencing Chemistry, Technology and Applications.pdf:pdf},
isbn = {0002-9297},
issn = {0392856X},
journal = {Chemical Diagnostics Topics in Current Chemistry},
keywords = {genomic medicine,next generation sequencing {\'{a}}},
number = {1},
pages = {1--18},
pmid = {28642624},
title = {{Next Generation Sequencing: Chemistry, Technology and Applications}},
volume = {336},
year = {2012}
}
@article{Arias-blanco2015,
author = {Arias-blanco, Juan Felipe and Fonseca-mendoza, Dora Janeth and Gamboa-garay, Oscar},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arias-blanco, Fonseca-mendoza, Gamboa-garay - 2015 - FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2.pdf:pdf},
journal = {Revista Colombiana de Obstetricia y Ginecolog{\'{i}}a},
number = {4},
pages = {287--296},
title = {{FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2 EN UNA MUESTRA DE MUJERES COLOMBIANAS CON SOSPECHA DE S{\'{I}}NDROME DE C{\'{A}}NCER DE MAMA HEREDITARIO: SERIE DE CASOS}},
url = {http://www.nature.com/articles/srep12376},
volume = {65},
year = {2015}
}
@article{Rishishwar2015,
abstract = {The human dimension of the Columbian Exchange entailed substantial genetic admixture between ancestral source populations from Africa, the Americas and Europe, which had evolved separately for many thousands of years. We sought to address the implications of the creation of admixed American genomes, containing novel allelic combinations, for human health and fitness via analysis of an admixed Colombian population from Medellin. Colombian genomes from Medellin show a wide range of three-way admixture contributions from ancestral source populations. The primary ancestry component for the population is European (average = 74.6%, range = 45.0%-96.7%), followed by Native American (average = 18.1%, range = 2.1%-33.3%) and African (average = 7.3%, range = 0.2%-38.6%). Locus-specific patterns of ancestry were evaluated to search for genomic regions that are enriched across the population for particular ancestry contributions. Adaptive and innate immune system related genes and pathways are particularly over-represented among ancestry-enriched segments, including genes (HLA-B and MAPK10) that are involved in defense against endemic pathogens such as malaria. Genes that encode functions related to skin pigmentation (SCL4A5) and cutaneous glands (EDAR) are also found in regions with anomalous ancestry patterns. These results suggest the possibility that ancestry-specific loci were differentially retained in the modern admixed Colombian population based on their utility in the New World environment.},
author = {Rishishwar, Lavanya and Conley, Andrew B. and Wigington, Charles H. and Wang, Lu and Valderrama-Aguirre, Augusto and {King Jordan}, I.},
doi = {10.1038/srep12376},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rishishwar et al. - 2015 - Ancestry, admixture and fitness in Colombian genomes.pdf:pdf},
isbn = {2045-2322 (ISSNLinking)},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {12376},
pmid = {26197429},
publisher = {Nature Publishing Group},
title = {{Ancestry, admixture and fitness in Colombian genomes}},
url = {http://www.nature.com/articles/srep12376},
volume = {5},
year = {2015}
}
@article{Li2017,
abstract = {In 2015, the American College of Medical Genetics and Genomics (ACMG) and the Association for Molecular Pathology (AMP) published updated standards and guidelines for the clinical interpretation of sequence variants with respect to human diseases on the basis of 28 criteria. However, variability between individual interpreters can be extensive because of reasons such as the different understandings of these guidelines and the lack of standard algorithms for implementing them, yet computational tools for semi-automated variant interpretation are not available. To address these problems, we propose a suite of methods for implementing these criteria and have developed a tool called InterVar to help human reviewers interpret the clinical significance of variants. InterVar can take a pre-annotated or VCF file as input and generate automated interpretation on 18 criteria. Furthermore, we have developed a companion web server, wInterVar, to enable user-friendly variant interpretation with an automated interpretation step and a manual adjustment step. These tools are especially useful for addressing severe congenital or very early-onset developmental disorders with high penetrance. Using results from a few published sequencing studies, we demonstrate the utility of InterVar in significantly reducing the time to interpret the clinical significance of sequence variants.},
author = {Li, Quan and Wang, Kai},
doi = {10.1016/j.ajhg.2017.01.004},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Wang - 2017 - InterVar Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines.pdf:pdf},
isbn = {0002-9297},
issn = {15376605},
journal = {American Journal of Human Genetics},
keywords = {ACMG,ANNOVAR,ClinVar,InterVar,clinical interpretation,genetic diagnosis,variant annotation,variant interpretation},
number = {2},
pages = {267--280},
pmid = {28132688},
publisher = {ElsevierCompany.},
title = {{InterVar: Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines}},
url = {http://dx.doi.org/10.1016/j.ajhg.2017.01.004},
volume = {100},
year = {2017}
}
@misc{CoriellInstitute,
author = {{Coriell Institute}},
title = {1000 genomes project},
url = {https://catalog.coriell.org/0/Sections/Collections/NHGRI/1000Clm.aspx?PgId=675&coll=HG}
}
@article{Lopez2017,
author = {Lopez, Javier and Coll, Jacobo and Haimel, Matthias and Kandasamy, Swaathi and Tarraga, Joaquin and Furio-tari, Pedro and Bari, Wasim and Bleda, Marta and Rueda, Antonio and Rendon, Augusto and Dopazo, Joaquin and Medina, Ignacio},
doi = {10.1093/nar/gkx445},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez et al. - 2017 - HGVA the Human Genome Variation Archive.pdf:pdf},
pages = {1--6},
title = {{HGVA: the Human Genome Variation Archive}},
year = {2017}
}
@article{Hegde2017,
abstract = {Context.—With the decrease in the cost of sequencing, the clinical testing paradigm has shifted from single gene to gene panel and now whole-exome and whole-genome sequencing. Clinical laboratories are rapidly implementing next-generation sequencing–based whole-exome and whole-genome sequencing. Because a large number of targets are covered by whole-exome and whole-genome sequencing, it is critical that a laboratory perform appropriate validation studies, develop a quality assurance and quality control program, and participate in proficiency testing. Objective.—To provide recommendations for whole-exome and whole-genome sequencing assay design, validation, and implementation for the detection of germ-line variants associated in inherited disorders. Data Sources.—An example of trio sequencing, filtration and annotation of variants, and phenotypic consideration to arrive at clinical diagnosis is discussed. Conclusions.—It is critical that clinical laboratories planning to implement whole-exome and whole-genome sequencing design and validate the assay to specifications and ensure adequate performance prior to implementa-tion. Test design specifications, including variant filtering and annotation, phenotypic consideration, guidance on consenting options, and reporting of incidental findings, are provided. These are important steps a laboratory must take to validate and implement whole-exome and whole-genome sequencing in a clinical setting for germline variants in inherited disorders.},
author = {Hegde, Madhuri and Santani, Avni and Mao, Rong and Ferreira-Gonzalez, Andrea and Weck, Karen E. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2016-0622-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hegde et al. - 2017 - Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {798--805},
pmid = {28362156},
title = {{Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants in inherited disease}},
volume = {141},
year = {2017}
}
@article{Santani2017,
abstract = {Context.—The number of targeted next-generation se-quencing (NGS) panels for genetic diseases offered by clinical laboratories is rapidly increasing. Before an NGS-based test is implemented in a clinical laboratory, appropriate validation studies are needed to determine the performance characteristics of the test. Objective.—To provide examples of assay design and validation of targeted NGS gene panels for the detection of germline variants associated with inherited disorders. Data Sources.—The approaches used by 2 clinical laboratories for the development and validation of targeted NGS gene panels are described. Important design and validation considerations are examined. Conclusions.—Clinical laboratories must validate per-formance specifications of each test prior to implementa-tion. Test design specifications and validation data are provided, outlining important steps in validation of targeted NGS panels by clinical diagnostic laboratories.},
author = {Santani, Avni and Murrell, Jill and Funke, Birgit and Yu, Zhenming and Hegde, Madhuri and Mao, Rong and Ferreira-Gonzalez, Andrea and Voelkerding, Karl V. and Weck, Karen E.},
doi = {10.5858/arpa.2016-0517-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santani et al. - 2017 - Development and validation of targeted next-generation sequencing panels for detection of germline variants in i.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {787--797},
title = {{Development and validation of targeted next-generation sequencing panels for detection of germline variants in inherited diseases}},
volume = {141},
year = {2017}
}
@article{Lauzon2016,
author = {Lauzon, David and Kanzki, Beatriz and Dupuy, Victor and April, Alain and Phillips, Michael S. and Tremblay, Johanne and Hamet, Pavel},
doi = {10.1109/CHASE.2016.79},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lauzon et al. - 2016 - Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS).pdf:pdf},
isbn = {9781509009435},
journal = {Proceedings - 2016 IEEE 1st International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2016},
keywords = {Big Data,GWAS health systems provenance,GWAS visualization,dbSNP discrepancies,open-source},
pages = {382--387},
title = {{Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS)}},
year = {2016}
}
@article{Sujansky2001,
abstract = {The rapid expansion of biomedical knowledge, reduction in computing costs, and spread of internet access have created an ocean of electronic data. The decentralized nature of our scientific community and healthcare system, however, has resulted in a patchwork of diverse, or heterogeneous, database implementations, making access to and aggregation of data across databases very difficult. The database heterogeneity problem applies equally to clinical data describing individual patients and biological data characterizing our genome. Specifically, databases are highly heterogeneous with respect to the data models they employ, the data schemas they specify, the query languages they support, and the terminologies they recognize. Heterogeneous database systems attempt to unify disparate databases by providing uniform conceptual schemas that resolve representational heterogeneities, and by providing querying capabilities that aggregate and integrate distributed data. Research in this area has applied a variety of database and knowledge-based techniques, including semantic data modeling, ontology definition, query translation, query optimization, and terminology mapping. Existing systems have addressed heterogeneous database integration in the realms of molecular biology, hospital information systems, and application portability.},
author = {Sujansky, W},
doi = {10.1006/jbin.2001.1024},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sujansky - 2001 - Heterogeneous database integration in biomedicine.pdf:pdf},
isbn = {1532-0464},
issn = {1532-0464},
journal = {Journal of biomedical informatics},
keywords = {ancient mariner,data warehouse,data warehouse.,database,database integration,drink,everywhere,federated database,heterogeneous database,nor any drop to,samuel taylor coleridge,the rime of the,water},
number = {2001},
pages = {285--298},
pmid = {11977810},
title = {{Heterogeneous database integration in biomedicine.}},
volume = {34},
year = {2001}
}
@article{Urbanczyk2016,
author = {Urbanczyk, Tomas and Peter, Lukas},
doi = {10.1016/j.ifacol.2016.12.047},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urbanczyk, Peter - 2016 - Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Co.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Database,MySQL,RFID,The Urgent department,environment analysis},
number = {25},
pages = {278--283},
publisher = {Elsevier B.V.},
title = {{Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Concept}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2405896316326830},
volume = {49},
year = {2016}
}
@article{Seren2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4788},
author = {Seren, {\"{U}}mit and Grimm, Dominik and Fitz, Joffrey and Weigel, Detlef and Nordborg, Magnus and Borgwardt, Karsten and Korte, Arthur},
doi = {10.1093/nar/gkw986},
eprint = {arXiv:1212.4788},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seren et al. - 2016 - AraPheno a public database for Arabidopsis thaliana phenotypes.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {October 2016},
pages = {gkw986},
pmid = {27924043},
title = {{AraPheno: a public database for Arabidopsis thaliana phenotypes}},
url = {http://nar.oxfordjournals.org/lookup/doi/10.1093/nar/gkw986},
volume = {45},
year = {2016}
}
@article{Cook2016,
abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI's vast public data resources.},
author = {Cook, Charles E. and Bergman, Mary Todd and Finn, Robert D. and Cochrane, Guy and Birney, Ewan and Apweiler, Rolf},
doi = {10.1093/nar/gkv1352},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook et al. - 2016 - The European Bioinformatics Institute in 2016 Data growth and integration.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D20--D26},
pmid = {26673705},
title = {{The European Bioinformatics Institute in 2016: Data growth and integration}},
volume = {44},
year = {2016}
}
@article{Harold2016,
abstract = {Motivation: Integrating heterogeneous datasets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access, data filtering, format conversions, identifier mapping, among further diverse operations. Data integration is especially important when annotating next generation sequencing data, where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic locations, such a single nucleotide variants or genes. Each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose. However, software that always produces all available data is difficult to maintain and quickly leads to an excess of data, creating an information overload rather than the desired goal-oriented and integrated result. Results: We present SoFIA, a framework for workflow-driven data integration with a focus on genomic annotation. SoFIA conceptualizes workflow templates as comprehensive workflows that cover as many data integration operations as possible in a given domain. However, these templates are not intended to be executed as a whole; instead, when given an integration task consisting of a set of input data and a set of desired output data, SoFIA derives a minimal workflow that completes the task. These workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work. Using a comprehensive genome annotation template, we highlight the flexibility, extensibility and power of the framework using real-life case studies},
author = {Harold, Liam and Mamlouk, Soulafa and Brandt, J{\"{o}}rgen and Sers, Christine and Lesser, Ulf},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harold et al. - 2016 - SoFIA a data integration framework for annotating high-throughput datasets.pdf:pdf},
journal = {Bioinformatics (Oxford, England)},
number = {17},
pages = {2590--2597},
title = {{SoFIA a data integration framework for annotating high-throughput datasets}},
volume = {32},
year = {2016}
}
@article{Paila2013,
abstract = {注释你 Mutation\r\n可以研究家系遗传病},
archivePrefix = {arXiv},
arxivId = {1304.4860},
author = {Paila, Umadevi and Chapman, Brad A. and Kirchner, Rory and Quinlan, Aaron R.},
doi = {10.1371/journal.pcbi.1003153},
eprint = {1304.4860},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paila et al. - 2013 - GEMINI Integrative Exploration of Genetic Variation and Genome Annotations.pdf:pdf},
isbn = {10.1371/journal.pcbi.1003153},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {7},
pmid = {23874191},
title = {{GEMINI: Integrative Exploration of Genetic Variation and Genome Annotations}},
volume = {9},
year = {2013}
}
@article{Wu2014,
abstract = {Exome sequencing has been widely used in detecting pathogenic nonsynonymous single nucleotide variants (SNVs) for human inherited diseases. However, traditional statistical genetics methods are ineffective in analyzing exome sequencing data, due to such facts as the large number of sequenced variants, the presence of non-negligible fraction of pathogenic rare variants or de novo mutations, and the limited size of affected and normal populations. Indeed, prevalent applications of exome sequencing have been appealing for an effective computational method for identifying causative nonsynonymous SNVs from a large number of sequenced variants. Here, we propose a bioinformatics approach called SPRING (Snv PRioritization via the INtegration of Genomic data) for identifying pathogenic nonsynonymous SNVs for a given query disease. Based on six functional effect scores calculated by existing methods (SIFT, PolyPhen2, LRT, MutationTaster, GERP and PhyloP) and five association scores derived from a variety of genomic data sources (gene ontology, protein-protein interactions, protein sequences, protein domain annotations and gene pathway annotations), SPRING calculates the statistical significance that an SNV is causative for a query disease and hence provides a means of prioritizing candidate SNVs. With a series of comprehensive validation experiments, we demonstrate that SPRING is valid for diseases whose genetic bases are either partly known or completely unknown and effective for diseases with a variety of inheritance styles. In applications of our method to real exome sequencing data sets, we show the capability of SPRING in detecting causative de novo mutations for autism, epileptic encephalopathies and intellectual disability. We further provide an online service, the standalone software and genome-wide predictions of causative SNVs for 5,080 diseases at http://bioinfo.au.tsinghua.edu.cn/spring.},
author = {Wu, Jiaxin and Li, Yanda and Jiang, Rui},
doi = {10.1371/journal.pgen.1004237},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Li, Jiang - 2014 - Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Se.pdf:pdf},
isbn = {1553-7404 (Electronic)\r1553-7390 (Linking)},
issn = {15537404},
journal = {PLoS Genetics},
number = {3},
pmid = {24651380},
title = {{Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Sequencing Studies}},
volume = {10},
year = {2014}
}
@article{Wang2014,
abstract = {Motivation: The transition/transversion (Ti/Tv) ratio and heterozygous/nonreference-homozygous (het/nonref-hom) ratio have been commonly computed in genetic studies as a quality control (QC) measurement. Additionally, these two ratios are helpful in our understanding of the patterns of DNA sequence evolution.Results: To thoroughly understand these two genomic measures, we performed a study using 1000 Genomes Project (1000G) released genotype data (N = 1092). An additional two datasets (N = 581 and N = 6) were used to validate our findings from the 1000G dataset. We compared the two ratios among continental ancestry, genome regions and gene functionality. We found that the Ti/Tv ratio can be used as a quality indicator for single nucleotide polymorphisms inferred from high-throughput sequencing data. The Ti/Tv ratio varies greatly by genome region and functionality, but not by ancestry. The het/nonref-hom ratio varies greatly by ancestry, but not by genome regions and functionality. Furthermore, extreme guanine + cytosine content (either high or low) is negatively associated with the Ti/Tv ratio magnitude. Thus, when performing QC assessment using these two measures, care must be taken to apply the correct thresholds based on ancestry and genome region. Failure to take these considerations into account at the QC stage will bias any following analysis.Contact: yan.guo@vanderbilt.eduSupplementary information: Supplementary data are available at Bioinformatics online. },
author = {Wang, Jing and Raskin, Leon and Samuels, David C. and Shyr, Yu and Guo, Yan},
doi = {10.1093/bioinformatics/btu668},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Genome measures used for quality control are dependent on gene function and ancestry.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {3},
pages = {318--323},
pmid = {25297068},
title = {{Genome measures used for quality control are dependent on gene function and ancestry}},
volume = {31},
year = {2014}
}
@article{Guo2013,
abstract = {Advances in next-generation sequencing (NGS) technologies have greatly improved our ability to detect genomic variants for biomedical research. In particular, NGS technologies have been recently applied with great success to the discovery of mutations associated with the growth of various tumours and in rare Mendelian diseases. The advance in NGS technologies has also created significant challenges in bioinformatics. One of the major challenges is quality control of the sequencing data. In this review, we discuss the proper quality control procedures and parameters for Illumina technology-based human DNA re-sequencing at three different stages of sequencing: raw data, alignment and variant calling. Monitoring quality control metrics at each of the three stages of NGS data provides unique and independent evaluations of data quality from differing perspectives. Properly conducting quality control protocols at all three stages and correctly interpreting the quality control results are crucial to ensure a successful and meaningful study.},
author = {Guo, Yan and Ye, Fei and Sheng, Quanghu and Clark, Travis and Samuels, David C.},
doi = {10.1093/bib/bbt069},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2013 - Three-stage quality control strategies for DNA re-sequencing data.pdf:pdf},
isbn = {1477-4054 (Electronic)\r1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Alignment,FASTQ,Quality control,Sequencing,Variant calling},
number = {6},
pages = {879--889},
pmid = {24067931},
title = {{Three-stage quality control strategies for DNA re-sequencing data}},
volume = {15},
year = {2013}
}
@article{Parla,
abstract = {Background: Human exome resequencing using commercial target capture kits has been and is being used for sequencing large numbers of individuals to search for variants associated with various human diseases. We rigorously evaluated the capabilities of two solution exome capture kits. These analyses help clarify the strengths and limitations of those data as well as systematically identify variables that should be considered in the use of those data. Results: Each exome kit performed well at capturing the targets they were designed to capture, which mainly corresponds to the consensus coding sequences (CCDS) annotations of the human genome. In addition, based on their respective targets, each capture kit coupled with high coverage Illumina sequencing produced highly accurate nucleotide calls. However, other databases, such as the Reference Sequence collection (RefSeq), define the exome more broadly, and so not surprisingly, the exome kits did not capture these additional regions. Conclusions: Commercial exome capture kits provide a very efficient way to sequence select areas of the genome at very high accuracy. Here we provide the data to help guide critical analyses of sequencing data derived from these products. Background},
author = {Parla, Jennifer S and Iossifov, Ivan and Grabill, Ian and Spector, Mona S and Kramer, Melissa and Mccombie, W Richard},
doi = {10.1186/gb-2011-12-9-r97},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parla et al. - Unknown - A comparative analysis of exome capture.pdf:pdf},
title = {{A comparative analysis of exome capture}}
}
@article{Matthijs2015,
abstract = {We present, on behalf of EuroGentest and the European Society of Human Genetics, guidelines for the evaluation and validation of next-generation sequencing (NGS) applications for the diagnosis of genetic disorders. The work was performed by a group of laboratory geneticists and bioinformaticians, and discussed with clinical geneticists, industry and patients' representatives, and other stakeholders in the field of human genetics. The statements that were written during the elaboration of the guidelines are presented here. The background document and full guidelines are available as supplementary material. They include many examples to assist the laboratories in the implementation of NGS and accreditation of this service. The work and ideas presented by others in guidelines that have emerged elsewhere in the course of the past few years were also considered and are acknowledged in the full text. Interestingly, a few new insights that have not been cited before have emerged during the preparation of the guidelines. The most important new feature is the presentation of a 'rating system' for NGS-based diagnostic tests. The guidelines and statements have been applauded by the genetic diagnostic community, and thus seem to be valuable for the harmonization and quality assurance of NGS diagnostics in Europe. Next-generation sequencing (NGS) allows for the fast generation of thousands to millions of base pairs of DNA sequence of an individual patient. The relatively fast emergence and the great success of these technologies in research herald a new era in genetic diagnostics. However, the new technologies bring challenges, both at the technical level and in terms of data management, as well as for the interpreta-tion of the results and for counseling. We believe that all these aspects warrant consideration of what the precise role of NGS in diagnostics will be, today and tomorrow. Before even embarking on acquisition of machines and skills for performing NGS in diagnostics, many issues have to be dealt with. It is in this context that we propose the guidelines. These guidelines mostly deal with NGS testing in the context of rare and mostly monogenic diseases. They mainly focus on the targeted analysis of gene panels, either through specific capture assays, or by extracting data from whole-exome sequencing. In principle, whole-genome sequencing may – and shortly will – also be used to extract similar information. In that case, the guidelines would still apply, but because whole-genome sequencing would also allow detecting other molecular features of disease, they would have to be extended accordingly. The different aspects of NGS and diagnostics were discussed during three workshops. The first took place in Leuven, 25–26 February 2013. The preliminary views were presented during the EuroGentest Scientific Meeting in Prague, 7–8 March 2013. The second was an editorial workshop in Leuven, 1–2 October 2013, where the different people involved in writing the document came together to discuss the layout of the document and prepare},
author = {Matthijs, Gert and Souche, Erika and Alders, Mari{\"{e}}lle and Corveleyn, Anniek and Eck, Sebastian and Feenstra, Ilse and Race, Val{\'{e}}rie and Sistermans, Erik and Sturm, Marc and Weiss, Marjan and Yntema, Helger and Bakker, Egbert and Scheffer, Hans and Bauer, Peter},
doi = {10.1038/ejhg.2015.226},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthijs et al. - 2015 - Guidelines for diagnostic next-generation sequencing.pdf:pdf},
journal = {European Journal of Human Genetics},
number = {10},
pages = {2--5},
title = {{Guidelines for diagnostic next-generation sequencing}},
volume = {24},
year = {2015}
}
@article{FigueroaFranco2014,
abstract = {La investigaci{\'{o}}n, recuperaci{\'{o}}n e identificaci{\'{o}}n de cuerpos relacionados con la desaparici{\'{o}}n de personas dentro del marco del conflicto armado en Colombia se ha llevado a cabo por parte del Estado: Fiscal{\'{i}}a General de la Naci{\'{o}}n (FGN), Polic{\'{i}}a Nacional, Departamento Administrativo de Seguridad (DAS) e Instituto Nacional de Medicina Legal y Ciencias Forenses (INMLCF). El an{\'{a}}lisis de ADN corresponde a una etapa del proceso de identificaci{\'{o}}n cuando otros abordajes –como el dactilosc{\'{o}}pico, odontol{\'{o}}gico y/o antropol{\'{o}}gico– no han permitido la identificaci{\'{o}}n fehaciente. Este an{\'{a}}lisis es de uso sistem{\'{a}}tico en laboratorios forenses a nivel mundial para comparar la informaci{\'{o}}n gen{\'{e}}tica de cad{\'{a}}veres en condici{\'{o}}n de no identificados con los posibles familiares de personas reportadas como desaparecidas. El presente estudio revis{\'{o}} la documentaci{\'{o}}n asociada a 154 solicitudes de identificaci{\'{o}}n recibidas durante 2009, analizadas por los peritos del Grupo de Gen{\'{e}}tica del INMLCF, sede Bogot{\'{a}}, D. C., logrando contribuir a la identificaci{\'{o}}n positiva de 95 cuerpos que inicialmente se encontraban en condici{\'{o}}n de no identificados (CNI). La mayor{\'{i}}a de muestras del estudio correspondieron a cuerpos exhumados de la regi{\'{o}}n Caribe colombiana. En el 80% de las solicitudes se realiz{\'{o}} cotejo gen{\'{e}}tico entre perfiles gen{\'{e}}ticos de familiares y de restos humanos; en el 20% los perfiles gen{\'{e}}ticos se almacenaron en la “Base Nacional de Perfiles Gen{\'{e}}ticos de Aplicaci{\'{o}}n Judicial”, conocida como CODIS por sus siglas en ingl{\'{e}}s (Combined DNA Index System). De las muestras {\'{o}}seas analizadas, el f{\'{e}}mur y la tibia arrojaron mejores resultados. El 77% de los cotejos fueron no exclusiones, el 9% resultados negativos o no concluyentes y el 14% exclusiones.},
author = {{Figueroa Franco}, Ruth Marl{\'{e}}n and {Romero M{\'{a}}rtinez}, Rosa Elena and {Terreros Ib{\'{a}}{\~{n}}ez}, Grace Alejandra and {Alava Narv{\'{a}}ez}, Mar{\'{i}}a Cristina and {Vicu{\~{n}}a Giraldo}, Gloria Carolina and {Mart{\'{i}}n La Rotta}, Claudia Jannet},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueroa Franco et al. - 2014 - Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lis.pdf:pdf},
journal = {Revista Colombiana de medicina Legal y Ciencias Forense},
keywords = {Cromosomas,Mitochondrial ADN},
number = {Revista Colombiana de Medicina Legal y Ciencias Forenses},
pages = {8--14},
title = {{Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lisis Gen{\'{e}}tico De Restos Humanos Durante 2009 En El Inmlcf De Bogot{\'{a}}, D. C.}},
volume = {2},
year = {2014}
}
@misc{,
annote = {NULL},
pages = {http://www.infobae.com/america/america--latina/2016},
title = {{Colombia se propuso identificar a 16.000 v{\'{i}}ctimas del conflicto armado con las FARC}},
url = {http://www.infobae.com/america/america-latina/2016},
urldate = {2016-11-20}
}
@article{Eduardoff2015,
abstract = {Abstract Next generation sequencing (NGS) offers the opportunity to analyse forensic DNA samples and obtain massively parallel coverage of targeted short sequences with the variants they carry. We evaluated the levels of sequence coverage, genotyping precision, sensitivity and mixed DNA patterns of a prototype version of the first commercial forensic NGS kit: the HID-Ion AmpliSeq™ Identity Panel with 169-markers designed for the Ion PGM™ system. Evaluations were made between three laboratories following closely matched Ion PGM™ protocols and a simple validation framework of shared DNA controls. The sequence coverage obtained was extensive for the bulk of SNPs targeted by the HID-Ion AmpliSeq™ Identity Panel. Sensitivity studies showed 90-95% of SNP genotypes could be obtained from 25 to 100 pg of input DNA. Genotyping concordance tests included Coriell cell-line control DNA analyses checked against whole-genome sequencing data from 1000 Genomes and Complete Genomics, indicating a very high concordance rate of 99.8%. Discordant genotypes detected in rs1979255, rs1004357, rs938283, rs2032597 and rs2399332 indicate these loci should be excluded from the panel. Therefore, the HID-Ion AmpliSeq™ Identity Panel and Ion PGM™ system provide a sensitive and accurate forensic SNP genotyping assay. However, low-level DNA produced much more varied sequence coverage and in forensic use the Ion PGM™ system will require careful calibration of the total samples loaded per chip to preserve the genotyping reliability seen in routine forensic DNA. Furthermore, assessments of mixed DNA indicate the user's control of sequence analysis parameter settings is necessary to ensure mixtures are detected robustly. Given the sensitivity of Ion PGM™, this aspect of forensic genotyping requires further optimisation before massively parallel sequencing is applied to routine casework.},
author = {Eduardoff, M. and Santos, C. and {De La Puente}, M. and Gross, T. E. and Fondevila, M. and Strobl, C. and Sobrino, B. and Ballard, D. and Schneider, P. M. and Carracedo and Lareu, M. V. and Parson, W. and Phillips, C.},
doi = {10.1016/j.fsigen.2015.04.007},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eduardoff et al. - 2015 - Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Identification SNPs,Ion PGM™,Ion Torrent,Massively parallel sequencing,Next generation sequencing},
pages = {110--121},
pmid = {25955683},
publisher = {Elsevier Ireland Ltd},
title = {{Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the Ion PGM™}},
url = {http://dx.doi.org/10.1016/j.fsigen.2015.04.007},
volume = {17},
year = {2015}
}
@misc{,
annote = {NULL},
booktitle = {La Patria},
pages = {http://www.lapatria.com/nacional/adn--clave--para--id},
title = {{ADN, clave para identificar 311 cuerpos}},
url = {http://www.lapatria.com/nacional/adn-clave-para-id},
urldate = {2016-11-20}
}
@article{Morimoto2016,
abstract = {We developed a new approach for pairwise kinship analysis in forensic genetics based on chromosomal sharing between two individuals. Here, we defined "index of chromosome sharing" (ICS) calculated using 174,254 single nucleotide polymorphism (SNP) loci typed by SNP microarray and genetic length of the shared segments from the genotypes of two individuals. To investigate the expected ICS distributions from first- to fifth-degree relatives and unrelated pairs, we used computationally generated genotypes to consider the effect of linkage disequilibrium and recombination. The distributions were used for probabilistic evaluation of the pairwise kinship analysis, such as likelihood ratio (LR) or posterior probability, without allele frequencies and haplotype frequencies. Using our method, all actual sample pairs from volunteers showed significantly high LR values (i.e., ≥ 108); therefore, we can distinguish distant relationships (up to the fifth-degree) from unrelated pairs based on LR. Moreover, we can determine accurate degrees of kinship in up to third-degree relationships with a probability of > 80% using the criterion of posterior probability ≥ 0.90, even if the kinship of the pair is totally unpredictable. This approach greatly improves pairwise kinship analysis of distant relationships, specifically in cases involving identification of disaster victims or missing persons.},
author = {Morimoto, Chie and Manabe, Sho and Kawaguchi, Takahisa and Kawai, Chihiro and Fujimoto, Shuntaro and Hamano, Yuya and Yamada, Ryo and Matsuda, Fumihiko and Tamaki, Keiji},
doi = {10.1371/journal.pone.0160287},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morimoto et al. - 2016 - Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
number = {7},
pages = {e0160287},
pmid = {27472558},
title = {{Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.}},
url = {http://dx.plos.org/10.1371/journal.pone.0160287%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27472558%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4966930},
volume = {11},
year = {2016}
}
@misc{FBI,
author = {FBI},
pages = {https://www.fbi.gov/services/laboratory/biometric--},
title = {{Combined DNA Index System (CODIS)}}
}
@book{Yunis2002,
author = {Yunis, E and Yunis, J},
publisher = {Temis S.A.},
title = {{El ADN en la Identificaci{\'{o}}n Humana}},
year = {2002}
}
@article{Buchard2016,
abstract = {The HID-Ion AmpliSeq™ Identity Panel is a next-generation sequencing assay with 90 autosomal and 34 Y-chromosome SNPs that are amplified in one PCR step and subsequently sequenced using the Ion Personal Genome Machine (Ion PGM™) System. This assay was validated for relationship testing in our ISO 17025 accredited laboratory in 2015. Here, the essential parts of the validation report submitted to the Danish Accreditation Fund are presented. A total of 100 unrelated Danes were typed in duplicates and the locus balance, heterozygote balance (Hb) and noise levels were analysed in detail. Two loci were disregarded for casework because genotyping was uncertain. Hb for rs7520386 was skewed and high levels of noise were observed in rs576261. Three general acceptance criteria for analysis of single-source samples were defined: (i) sequencing depth > 200 reads, (ii) noise level < 3% and (iii) Hb > 0.3. A Python script named SNPonPGM was developed to assist the analyst by highlighting loci that do not fulfil the general acceptance criteria. Furthermore, SNPonPGM has functions that reduce the hands-on time of the reporting officer to a few minutes per case. Mixtures with DNA from two individuals in a 1:24 ratio were readily identified using the three criteria and the SNPonPGM script.},
author = {Buchard, Anders and Kampmann, Marie-Louise and Poulsen, Lena and B{\o}rsting, Claus and Morling, Niels},
doi = {10.1002/elps.201600269},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchard et al. - 2016 - ISO 17025 validation of a next-generation sequencing assay for relationship testing(3).pdf:pdf},
issn = {1522-2683},
journal = {Electrophoresis},
keywords = {Forensic genetics,ISO 17025 accreditation,Kinship testing,Next-generation sequencing,SNPs},
pages = {1--10},
pmid = {27709635},
title = {{ISO 17025 validation of a next-generation sequencing assay for relationship testing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27709635},
year = {2016}
}
@article{Guo2016,
abstract = {The HID-Ion AmpliSeq??? Identity Panel (the HID Identity Panel) is designed to detect 124-plex single nucleotide polymorphisms (SNPs) with next generation sequencing (NGS) technology on the Ion Torrent PGM??? platform, including 90 individual identification SNPs (IISNPs) on autosomal chromosomes and 34 lineage informative SNPs (LISNPs) on Y chromosome. In this study, we evaluated performance for the HID Identity Panel to provide a reference for NGS-SNP application, focusing on locus strand balance, locus coverage balance, heterozygote balance, and background signals. Besides, several experiments were carried out to find out improvements and limitations of this panel, including studies of species specificity, repeatability and concordance, sensitivity, mixtures, case-type samples and degraded samples, population genetics and pedigrees following the Scientific Working Group on DNA Analysis Methods (SWGDAM) guidelines. In addition, Southern and Northern Chinese Han were investigated to assess applicability of this panel. Results showed this panel led to cross-reactivity with primates to some extent but rarely with non-primate animals. Repeatable and concordant genotypes could be obtained in triplicate with one exception at rs7520386. Full profiles could be obtained from 100 pg input DNA, but the optimal input DNA would be 1 ng???200 pg with 21 initial PCR cycles. A sample with ???20% minor contributor could be considered as a mixture by the number of homozygotes, and full profiles belonging to minor contributors could be detected between 9:1 and 1:9 mixtures with known reference profiles. Also, this assay could be used for case-type samples and degraded samples. For autosomal SNPs (A-SNPs), FST across all 90 loci was not significantly different between Southern and Northern Chinese Han or between male and female samples. All A-SNP loci were independent in Chinese Han population. Except for 18 loci with He <0.4, most of the A-SNPs in the HID Identity Panel presented high polymorphisms. Forensic parameters were calculated as >99.999% for combined discrimination power (CDP), 0.999999724 for combined power of exclusion (CPE), 1.390 ?? 1011 for combined likelihood ratio (CLR) of trios, and 2.361 ?? 106 for CLR of motherless duos. For Y-SNPs, a total of 8 haplotypes were observed with the value of 0.684 for haplotype diversity. As a whole, the HID Identity Panel is a well-performed, robust, reliable and high informative NGS-SNP assay and it can fully meet requirements for individual identification and paternity testing in forensic science.},
author = {Guo, Fei and Zhou, Yishu and Song, He and Zhao, Jinling and Shen, Hongying and Zhao, Bin and Liu, Feng and Jiang, Xianhua},
doi = {10.1016/j.fsigen.2016.07.021},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2016 - Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Evaluation,HID-Ion AmpliSeq???,Identity Panel,Ion Torrent PGM???,Next generation sequencing (NGS),Population genetics,Single nucleotide polymorphism (SNP)},
pages = {73--84},
pmid = {27500651},
publisher = {Elsevier Ireland Ltd},
title = {{Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform}},
url = {http://dx.doi.org/10.1016/j.fsigen.2016.07.021},
volume = {25},
year = {2016}
}
@article{Huttenhower2010,
abstract = {The aim of this work is to be able to publish the information concerning communication with cancer patients as recommended in England. The observation and the study protocol during the stay abroad have been given the opportunity to stylize specific information on the methodology of communication of important information to terminally ill patients. It seems readily apparent as they characterized by both technical precision and sensivity to emotions and descriptions for the individual patient. How is shared by all chronic pain is predominantly complex emotion, a mix of additions and perceived physical and emotional pain - emotional. Because accurate information is beneficial to the patient and that really is not turned, so to speak, a "bullet" it is necessary that you have created, over time, a concrete "therapeutic alliance" between body physician, patient and possibly family. This arises, for sure, even at first accepted the patient during the clinical visit attentive to detail, is renewed in the definition of the common objective to be achieved, so analgesia and it is expressed in the certainty that the physician provides all the resources realistically available. It is then up to the sensitivity of the operator, doctor and/or nurse, described in the "take charge" find, from time to time, the words and manners, verbal and nonverbal, to respond fully to questions of the patient same.},
author = {Huttenhower, Curtis and Hofmann, Oliver},
doi = {10.1371/journal.pcbi.1000779},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huttenhower, Hofmann - 2010 - A quick guide to large-scale genomic data mining.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {5},
pages = {1--6},
pmid = {20523745},
title = {{A quick guide to large-scale genomic data mining}},
volume = {6},
year = {2010}
}
@article{Pandey2016a,
abstract = {Traditional Sanger sequencing as well as Next-Generation Sequencing have been used for the identification of disease causing mutations in human molecular research. The majority of currently available tools are developed for research and explorative purposes and often do not provide a complete, efficient, one-stop solution. As the focus of currently developed tools is mainly on NGS data analysis, no integrative solution for the analysis of Sanger data is provided and consequently a one-stop solution to analyze reads from both sequencing platforms is not available. We have therefore developed a new pipeline called MutAid to analyze and interpret raw sequencing data produced by Sanger or several NGS sequencing platforms. It performs format conversion, base calling, quality trimming, filtering, read mapping, variant calling, variant annotation and analysis of Sanger and NGS data under a single platform. It is capable of analyzing reads from multiple patients in a single run to create a list of potential disease causing base substitutions as well as insertions and deletions. MutAid has been developed for expert and non-expert users and supports four sequencing platforms including Sanger, Illumina, 454 and Ion Torrent. Furthermore, for NGS data analysis, five read mappers including BWA, TMAP, Bowtie, Bowtie2 and GSNAP and four variant callers including GATK-HaplotypeCaller, SAMTOOLS, Freebayes and VarScan2 pipelines are supported. MutAid is freely available at https://sourceforge.net/projects/mutaid.},
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1371/journal.pone.0147697},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - MutAid Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human m.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--22},
pmid = {26840129},
title = {{MutAid: Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human molecular genetics}},
volume = {11},
year = {2016}
}
@article{Hehir-Kwa2016,
abstract = {Structural variation (SV) represents a major source of differences between individual human genomes and has been linked to disease phenotypes. However, the majority of studies pro-vide neither a global view of the full spectrum of these variants nor integrate them into reference panels of genetic variation. Here, we analyse whole genome sequencing data of 769 individuals from 250 Dutch families, and provide a haplotype-resolved map of 1.9 million genome variants across 9 different variant classes, including novel forms of complex indels, and retrotransposition-mediated insertions of mobile elements and processed RNAs. A large proportion are previously under reported variants sized between 21 and 100 bp. We detect 4 megabases of novel sequence, encoding 11 new transcripts. Finally, we show 191 known, trait-associated SNPs to be in strong linkage disequilibrium with SVs and demonstrate that our panel facilitates accurate imputation of SVs in unrelated individuals.},
author = {Hehir-Kwa, Jayne Y and Marschall, Tobias and Kloosterman, Wigard P and Francioli, Laurent C},
doi = {10.1038/ncomms12989},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hehir-Kwa et al. - 2016 - ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural var.pdf:pdf},
journal = {Nature Communications},
number = {11},
title = {{ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural variants}},
volume = {7},
year = {2016}
}
@article{Trapnell2009,
abstract = {MOTIVATION: A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.\n\nRESULTS: We mapped the RNA-Seq reads from a recent mammalian RNA-Seq experiment and recovered more than 72% of the splice junctions reported by the annotation-based software from that study, along with nearly 20,000 previously unreported junctions. The TopHat pipeline is much faster than previous systems, mapping nearly 2.2 million reads per CPU hour, which is sufficient to process an entire RNA-Seq experiment in less than a day on a standard desktop computer. We describe several challenges unique to ab initio splice site discovery from RNA-Seq reads that will require further algorithm development.\n\nAVAILABILITY: TopHat is free, open-source software available from http://tophat.cbcb.umd.edu.\n\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L.},
doi = {10.1093/bioinformatics/btp120},
eprint = {9605103},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trapnell, Pachter, Salzberg - 2009 - TopHat Discovering splice junctions with RNA-Seq.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {9},
pages = {1105--1111},
pmid = {19289445},
primaryClass = {cs},
title = {{TopHat: Discovering splice junctions with RNA-Seq}},
volume = {25},
year = {2009}
}
@article{Conesa2016,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Micha{\l} Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
doi = {10.1186/s13059-016-0881-8},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conesa et al. - 2016 - A survey of best practices for RNA-seq data analysis.pdf:pdf},
isbn = {1474-760X (Electronic)\r1474-7596 (Linking)},
issn = {1474-760X},
journal = {Genome Biology},
keywords = {Animal Genetics and Genomics,Bioinformatics,Evolutionary Biology,Human Genetics,Microbial Genetics and Genomics,Plant Genetics & Genomics},
number = {1},
pages = {13},
pmid = {26813401},
title = {{A survey of best practices for RNA-seq data analysis}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0881-8},
volume = {17},
year = {2016}
}
@article{Wang2009,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
doi = {10.1038/nrg2484},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gerstein, Snyder - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
keywords = {Animals,Base Sequence,Chromosome Mapping,Exons,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Genetic,Molecular Sequence Data,RNA,RNA: analysis,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Transcription, Genetic},
number = {1},
pages = {57--63},
pmid = {19015660},
title = {{RNA-Seq: a revolutionary tool for transcriptomics.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2949280&tool=pmcentrez&rendertype=abstract},
volume = {10},
year = {2009}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@article{Medina2016,
abstract = {As sequencing technologies progress, the amount of data produced grows exponentially, shifting the bottleneck of discovery towards the data analysis phase. In particular, currently available mapping solutions for RNA-seq leave room for improvement in terms of sensitivity and performance, hindering an efficient analysis of transcriptomes by massive sequencing. Here, we present an innovative approach that combines re-engineering, optimization and parallelization. This solution results in a significant increase of mapping sensitivity over a wide range of read lengths and substantial shorter runtimes when compared with current RNA-seq mapping methods available.},
author = {Medina, I and T{\'{a}}rraga, J and Mart{\'{i}}nez, H and Barrachina, S and Castillo, M I and Paschall, J and Salavert-Torres, J and Blanquer-Espert, I and Hern{\'{a}}ndez-Garc{\'{i}}a, V and Quintana-Ort{\'{i}}, E S and Dopazo, J},
doi = {10.1093/dnares/dsv039},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medina et al. - 2016 - Highly sensitive and ultrafast read mapping for RNA-seq analysis.pdf:pdf},
issn = {1756-1663},
journal = {DNA research : an international journal for rapid publication of reports on genes and genomes},
keywords = {burrows-wheeler transform,high-performance computing,mapping,rna-seq},
number = {January},
pages = {dsv039},
pmid = {26740642},
title = {{Highly sensitive and ultrafast read mapping for RNA-seq analysis.}},
url = {http://dnaresearch.oxfordjournals.org/content/early/2016/01/05/dnares.dsv039.full},
volume = {23},
year = {2016}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@inproceedings{Moore99,
author = {Moore, R and Lopes, J},
booktitle = {TEMPLATE'06, 1st International Conference on Template Production},
publisher = {SCITEPRESS},
title = {{Paper templates}},
year = {1999}
}
@article{Brueffer2015,
abstract = {Summary: TopHat is a popular spliced junction mapper for RNA sequencing data, and writes files in the BAM format - the binary version of the Sequence Alignment/Map (SAM) format. BAM is the standard exchange format for aligned sequencing reads, thus correct format implementation is paramount for software interoperability and correct analysis. However, TopHat writes its unmapped reads in a way that is not compatible with other software that implements the SAM/BAM format. We have developed TopHat-Recondition, a post-processor for TopHat unmapped reads that restores read information in the proper format. TopHat-Recondition thus enables downstream software to process the plethora of BAM files written by TopHat. Availability and implementation: TopHat-Recondition is implemented in Python using the Pysam library and is freely available under a 2-clause BSD license on GitHub: https://github.com/cbrueffer/tophat-recondition. Contact: christian.brueffer@med.lu.se, lao.saal@med.lu.se},
author = {Brueffer, Christian and Saal, Lao H},
doi = {10.1101/033530},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brueffer, Saal - 2015 - TopHat-Recondition A post-processor for TopHat unmapped reads.pdf:pdf},
issn = {1471-2105},
journal = {bioRxiv},
keywords = {deep sequencing,rna-seq,sequence alignment,sequence analysis},
pages = {1--6},
pmid = {27142976},
publisher = {BMC Bioinformatics},
title = {{TopHat-Recondition: A post-processor for TopHat unmapped reads}},
url = {http://dx.doi.org/10.1186/s12859-016-1058-x},
volume = {2},
year = {2015}
}
@inproceedings{Buchanan2012,
author = {Buchanan, Carrie C. and Wallace, John R. and Frase, Alex T. and Torstenson, Eric S. and Pendergrass, Sarah A. and Ritchie, Marylyn D.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29066-4_18},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchanan et al. - 2012 - A biologically informed method for detecting associations with rare variants.pdf:pdf},
isbn = {9783642290657},
issn = {03029743},
keywords = {Collapsing Tool,Pathway Analysis,Prior Knowledge,Rare Variants},
title = {{A biologically informed method for detecting associations with rare variants}},
year = {2012}
}
@article{Williams2011,
abstract = {Editorial Biological data mining is playing an increasingly important role throughout the spectrum of biological and biomedical research with broad implications for the understanding of life science questions such as the tree of life and practical applica-tions of such knowledge to improving human health. Perhaps nowhere is data mining needed more than the emerging discipline of precision medicine. The ability to predict individual risk of presenting with a disease or response to treatment is at the core of the concept of precision medicine, which is gaining ever-increasing levels of traction in the era of technology-driven measurement of biological systems. This has become especially important with the new Presidential initiative on precision medi-cine in the United States [1]. It is obvious to the readers of BioData Mining that this will require careful analyses of large and often complex data sets to best translate information into increasingly individualized risk. Here we ask why improved and appropriate data mining is not only positive but a vast improvement on most current analyses of genomic data. The answer lies to some extent in elucidating the present practice of -omic analyses and how we will need to expand it. Many current -omic approaches rely on univariate and linear analyses that can often miss the underlying architecture of complex traits. For example, univariate analyses of single genetic markers for association with disease risk, prognosis, or drug response that are the analytical standards for genetic analyses of human disease, and have been promoted as a means to develop personalized or more recently precision medicine, make many assumptions about architecture. Given the interest in precision medicine, it is important to ask explicitly what is being assayed in these types of studies that have been argued, incorrectly we believe, as the precursors to precision medicine. Most human geneticists study the association of genetic variants, be they common or rare, assessed across moderate to large samples of cases and controls. The effect of each allelic substitution is then measured as it associates with a particular phenotype. These estimates can provide useful population level risks; however, they are simply the average effect of an allelic substitution across the population, not necessarily predictive of results in an individual or a subgroup. The concept of average allelic effect is one that is well developed in quantitative genetics, but by its very name is suggestive not of precision medicine but of average medicine. Hence, it is possible in a large outbreeding},
author = {Williams, Scott M and Moore, Jason H},
doi = {10.1186/s13040-015-0049-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Moore - 2011 - Lumping versus splitting the need for biological data mining in precision medicine.pdf:pdf},
title = {{Lumping versus splitting: the need for biological data mining in precision medicine}},
volume = {8},
year = {2011}
}
@article{Farid2016,
abstract = {In this paper, we introduce a new adaptive rule-based classifier for multi-class classification of biological data, where several problems of classifying biological data are addressed: overfitting, noisy instances and class-imbalance data. It is well known that rules are interesting way for representing data in a human interpretable way. The proposed rule-based classifier combines the random subspace and boosting approaches with ensemble of decision trees to construct a set of classification rules without involving global optimisation. The classifier considers random subspace approach to avoid overfitting, boosting approach for classifying noisy instances and ensemble of decision trees to deal with class-imbalance problem. The classifier uses two popular classification techniques: decision tree and k-nearest-neighbor algorithms. Decision trees are used for evolving classification rules from the training data, while k-nearest-neighbor is used for analysing the misclassified instances and removing vagueness between the contradictory rules. It considers a series of k iterations to develop a set of classification rules from the training data and pays more attention to the misclassified instances in the next iteration by giving it a boosting flavour. This paper particularly focuses to come up with an optimal ensemble classifier that will help for improving the prediction accuracy of DNA variant identification and classification task. The performance of proposed classifier is tested with compared to well-approved existing machine learning and data mining algorithms on genomic data (148 Exome data sets) of Brugada syndrome and 10 real benchmark life sciences data sets from the UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed classifier has exemplary classification accuracy on different types of biological data. Overall, the proposed classifier offers good prediction accuracy to new DNA variants classification where noisy and misclassified variants are optimised to increase test performance.},
annote = {NULL},
author = {Farid, Dewan Md. and Al-Mamun, Mohammad Abdullah and Manderick, Bernard and Nowe, Ann},
doi = {10.1016/j.eswa.2016.08.008},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farid et al. - 2016 - An adaptive rule-based classifier for mining big biological data.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Brugada syndrome,Classification,Decision tree,Genomic data,Rule-based classifier},
pages = {305--316},
title = {{An adaptive rule-based classifier for mining big biological data}},
volume = {64},
year = {2016}
}
@article{Stelzer,
abstract = {GeneCards, the human gene compendium, enables researchers to effectively navigate and inter-relate the wide universe of human genes, diseases, variants, proteins, cells, and biological pathways. Our recently launched Version 4 has a revamped infrastructure facilitating faster data updates, better-targeted data queries, and friendlier user experience. It also provides a stronger foundation for the GeneCards suite of companion databases and analysis tools. Improved data unification includes gene-disease links via MalaCards and merged biological pathways via PathCards, as well as drug information and proteome expression. VarElect, another suite member, is a phenotype prioritizer for next-generation sequencing, leveraging the GeneCards and MalaCards knowledgebase. It au-tomatically infers direct and indirect scored associations between hundreds or even thousands of variant-containing genes and disease phenotype terms. Var-Elect's capabilities, either independently or within TGex, our comprehensive variant analysis pipeline, help prepare for the challenge of clinical projects that involve thousands of exome/genome NGS analyses. C},
author = {Stelzer, Gil and Rosen, Naomi and Plaschkes, Inbar and Zimmerman, Shahar and Twik, Michal and Fishilevich, Simon and Stein, Tsippi Iny and Nudel, Ron and Lieder, Iris and Mazor, Yaron and Kaplan, Sergey and Dahary, Dvir and Warshawsky, David and Guan-Golan, Yaron and Kohn, Asher and Rappaport, Noa and Safran, Marilyn and Lancet, Doron},
doi = {10.1002/cpbi.5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer et al. - Unknown - The GeneCards Suite From Gene Data Mining to Disease Genome Sequence Analyses.pdf:pdf},
journal = {Curr. Protoc. Bioinform},
keywords = {Elect,Gene,Var,biological database r bioinformatics r diseases r},
number = {1},
pages = {1--130},
title = {{The GeneCards Suite: From Gene Data Mining to Disease Genome Sequence Analyses}},
volume = {5430}
}
@article{Sims2014,
abstract = {Sequencing technologies have placed a wide range of genomic analyses within the capabilities of many laboratories. However, sequencing costs often set limits to the amount of sequences that can be generated and, consequently, the biological outcomes that can be achieved from an experimental design. In this Review, we discuss the issue of sequencing depth in the design of next-generation sequencing experiments. We review current guidelines and precedents on the issue of coverage, as well as their underlying considerations, for four major study designs, which include de novo genome sequencing, genome resequencing, transcriptome sequencing and genomic location analyses (for example, chromatin immunoprecipitation followed by sequencing (ChIP-seq) and chromosome conformation capture (3C)).},
author = {Sims, David and Sudbery, Ian and Ilott, Nicholas E and Heger, Andreas and Ponting, Chris P},
doi = {10.1038/nrg3642},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sims et al. - 2014 - Genomics is extending its reach into diverse fields of biomedical research from agriculture to clinical diag- nosti.pdf:pdf},
isbn = {1471-0056},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
number = {2},
pages = {121--32},
pmid = {24434847},
title = {{Sequencing depth and coverage: key considerations in genomic analyses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24434847},
volume = {15},
year = {2014}
}
@article{Baes2014a,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle(2).pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Field2015,
abstract = {A diversity of tools is available for identification of variants from genome sequence data. Given the current complexity of incorporating external software into a genome analysis infrastructure, a tendency exists to rely on the results from a single tool alone. The quality of the output variant calls is highly variable however, depending on factors such as sequence library quality as well as the choice of short-read aligner, variant caller, and variant caller filtering strategy. Here we present a two-part study first using the high quality 'genome in a bottle' reference set to demonstrate the significant impact the choice of aligner, variant caller, and variant caller filtering strategy has on overall variant call quality and further how certain variant callers outperform others with increased sample contamination, an important consideration when analyzing sequenced cancer samples. This analysis confirms previous work showing that combining variant calls of multiple tools results in the best quality resultant variant set, for either specificity or sensitivity, depending on whether the intersection or union, of all variant calls is used respectively. Second, we analyze a melanoma cell line derived from a control lymphocyte sample to determine whether software choices affect the detection of clinically important melanoma risk-factor variants finding that only one of the three such variants is unanimously detected under all conditions. Finally, we describe a cogent strategy for implementing a clinical variant detection pipeline; a strategy that requires careful software selection, variant caller filtering optimizing, and combined variant calls in order to effectively minimize false negative variants. While implementing such features represents an increase in complexity and computation the results offer indisputable improvements in data quality.},
author = {Field, Matthew A. and Cho, Vicky and Andrews, T. Daniel and Goodnow, Chris C.},
doi = {10.1371/journal.pone.0143199},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Field et al. - 2015 - Reliably detecting clinically important variants requires both combined variant calls and optimized filtering stra.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--19},
title = {{Reliably detecting clinically important variants requires both combined variant calls and optimized filtering strategies}},
volume = {10},
year = {2015}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (2).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (2)}}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas (1)}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Reviews3.csv:csv},
title = {{Reviews3}}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma.png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma}}
}
@article{ORawe2013,
abstract = {BACKGROUND: To facilitate the clinical implementation of genomic medicine by next-generation sequencing, it will be critically important to obtain accurate and consistent variant calls on personal genomes. Multiple software tools for variant calling are available, but it is unclear how comparable these tools are or what their relative merits in real-world scenarios might be.\n\nMETHODS: We sequenced 15 exomes from four families using commercial kits (Illumina HiSeq 2000 platform and Agilent SureSelect version 2 capture kit), with approximately 120X mean coverage. We analyzed the raw data using near-default parameters with five different alignment and variant-calling pipelines (SOAP, BWA-GATK, BWA-SNVer, GNUMAP, and BWA-SAMtools). We additionally sequenced a single whole genome using the sequencing and analysis pipeline from Complete Genomics (CG), with 95% of the exome region being covered by 20 or more reads per base. Finally, we validated 919 single-nucleotide variations (SNVs) and 841 insertions and deletions (indels), including similar fractions of GATK-only, SOAP-only, and shared calls, on the MiSeq platform by amplicon sequencing with approximately 5000X mean coverage.\n\nRESULTS: SNV concordance between five Illumina pipelines across all 15 exomes was 57.4%, while 0.5 to 5.1% of variants were called as unique to each pipeline. Indel concordance was only 26.8% between three indel-calling pipelines, even after left-normalizing and intervalizing genomic coordinates by 20 base pairs. There were 11% of CG variants falling within targeted regions in exome sequencing that were not called by any of the Illumina-based exome analysis pipelines. Based on targeted amplicon sequencing on the MiSeq platform, 97.1%, 60.2%, and 99.1% of the GATK-only, SOAP-only and shared SNVs could be validated, but only 54.0%, 44.6%, and 78.1% of the GATK-only, SOAP-only and shared indels could be validated. Additionally, our analysis of two families (one with four individuals and the other with seven), demonstrated additional accuracy gained in variant discovery by having access to genetic data from a multi-generational family.\n\nCONCLUSIONS: Our results suggest that more caution should be exercised in genomic medicine settings when analyzing individual genomes, including interpreting positive and negative findings with scrutiny, especially for indels. We advocate for renewed collection and sequencing of multi-generational families to increase the overall accuracy of whole genomes.},
author = {O'Rawe, Jason and Jiang, Tao and Sun, Guangqing and Wu, Yiyang and Wang, Wei and Hu, Jingchu and Bodily, Paul and Tian, Lifeng and Hakonarson, Hakon and Johnson, W Evan and Wei, Zhi and Wang, Kai and Lyon, Gholson J},
doi = {10.1186/gm432},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Rawe et al. - 2013 - Low concordance of multiple variant-calling pipelines practical implications for exome and genome sequencing.pdf:pdf},
isbn = {1756-994X (Print)},
issn = {1756-994X},
journal = {Genome medicine},
number = {3},
pages = {28},
pmid = {23537139},
title = {{Low concordance of multiple variant-calling pipelines: practical implications for exome and genome sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3706896&tool=pmcentrez&rendertype=abstract},
volume = {5},
year = {2013}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (1).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (1)}}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - scatter-mode (1).png:png},
title = {scatter-mode (1)}
}
@article{Zhou2013,
abstract = {Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7-8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis. QC-Chain is publicly available at: http://www.computationalbioenergy.org/qc-chain.html.},
author = {Zhou, Qian and Su, Xiaoquan and Wang, Anhui and Xu, Jian and Ning, Kang},
doi = {10.1371/journal.pone.0060234},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2013 - QC-Chain Fast and Holistic Quality Control Method for Next-Generation Sequencing Data.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {23565205},
title = {{QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data}},
volume = {8},
year = {2013}
}
@article{Pandey2016,
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1186/s12859-016-0915-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - ClinQC a tool for quality control and cleaning of Sanger and NGS data in clinical research.pdf:pdf},
isbn = {14712105 (Electronic)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Sanger sequencing,Next generation sequencing,Quali,molecular diagnostic testing,next generation sequencing,quality control,sanger sequencing},
number = {1},
pages = {56},
pmid = {26830926},
publisher = {BMC Bioinformatics},
title = {{ClinQC: a tool for quality control and cleaning of Sanger and NGS data in clinical research}},
url = {http://www.biomedcentral.com/1471-2105/17/56},
volume = {17},
year = {2016}
}
@article{Xu2016,
author = {Xu, Zhiwei and Chi, Xuebin and Xiao, Nong},
doi = {10.1093/nsr/nww001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Chi, Xiao - 2016 - High-Performance Computing Environment A Review of Twenty Years of Experiments in China.pdf:pdf},
issn = {2053714X},
keywords = {22-dec-2015,24-sep-2015,25-dec-2015,accepted,cyberinfrastructure,e-science environment,middleware,received,revised,supercomputing},
pages = {1--24},
title = {{High-Performance Computing Environment : A Review of Twenty Years of Experiments in China}},
year = {2016}
}
@misc{Watson1953,
abstract = {A structure for nucleic acid has already been proposed by Pauling and Corey 1 . They kindly made their manuscript available to us in advance of publication. Their model consists of three intertwined chains, with the phosphates near},
author = {Watson, James D and Crick, Francis H C},
booktitle = {Nature},
doi = {10.1097/BLO.0b013e3181468780},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Crick - 1953 - Molecular structure of nucleic acids.pdf:pdf},
isbn = {0226284158},
issn = {0028-0836},
keywords = {nucleic acids},
number = {4356},
pages = {737--738},
pmid = {1943},
title = {{Molecular structure of nucleic acids}},
url = {http://www.nature.com/physics/looking-back/crick/%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/13054692},
volume = {171},
year = {1953}
}
@article{DanecekPAutonA2011,
abstract = {SUMMARY: The Variant Call Format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing, and also provides a general Perl API. AVAILABILITY: http://vcftools.sourceforge.net CONTACT: rd@sanger.ac.uk.},
author = {Danecek, P and Auton, A and Abecasis, G and Albers, C and Banks, E and Depristo, M and Handsaker, R and Lunter, G and Marth, G and Sherry, S and McVean, G and Durbin, R},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danecek et al. - 2011 - The Variant Call Format and VCFtools.pdf:pdf},
journal = {Bioinformatics},
pages = {3--5},
title = {{The Variant Call Format and VCFtools}},
url = {http://bioinformatics.oxfordjournals.org/content/early/2011/06/07/bioinformatics.btr330.long},
year = {2011}
}
@book{Herraez2012,
address = {Barcelona},
author = {Herr{\'{a}}ez, Angel},
isbn = {978-84-8086-647-7},
pages = {241},
publisher = {Elsevier Ltd},
title = {{Biolog{\'{i}}a Molecular e Ingenier{\'{i}}a Gen{\'{e}}tica. 2{\textordfeminine} ed.}},
year = {2012}
}
@article{Rehm2013,
abstract = {Next-generation sequencing technologies have been and continue to be deployed in clinical laboratories, enabling rapid transformations in genomic medicine. These technologies have reduced the cost of large-scale sequencing by several orders of magnitude, and continuous advances are being made. It is now feasible to analyze an individual's near-complete exome or genome to assist in the diagnosis of a wide array of clinical scenarios. Next-generation sequencing technologies are also facilitating further advances in therapeutic decision making and disease prediction for at-risk patients. However, with rapid advances come additional challenges involving the clinical validation and use of these constantly evolving technologies and platforms in clinical laboratories. To assist clinical laboratories with the validation of next-generation sequencing methods and platforms, the ongoing monitoring of next-generation sequencing testing to ensure quality results, and the interpretation and reporting of variants found using these technologies, the American College of Medical Genetics and Genomics has developed the following professional standards and guidelines.},
author = {Rehm, Heidi L and Bale, Sherri J and Bayrak-Toydemir, Pinar and Berg, Jonathan S and Brown, Kerry K and Deignan, Joshua L and Friez, Michael J and Funke, Birgit H and Hegde, Madhuri R and Lyon, Elaine},
doi = {10.1038/gim.2013.92},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehm et al. - 2013 - ACMG clinical laboratory standards for next-generation sequencing.pdf:pdf},
isbn = {1098-3600},
issn = {1530-0366},
journal = {Genetics in medicine : official journal of the American College of Medical Genetics},
keywords = {DNA,DNA: instrumentation,DNA: methods,DNA: standards,Exome,Genetic Testing,Genetic Testing: standards,Genome,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: standards,Human,Humans,Laboratories,Laboratories: standards,Reproducibility of Results,Sequence Analysis,Translational Medical Research,United States},
number = {9},
pages = {733--47},
pmid = {23887774},
title = {{ACMG clinical laboratory standards for next-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4098820&tool=pmcentrez&rendertype=abstract},
volume = {15},
year = {2013}
}
@article{Tsai2016,
abstract = {Effective implementation of precision medicine will be enhanced by a thorough understanding of each patient's genetic composition to better treat his or her presenting symptoms or mitigate the onset of disease. This ideally includes the sequence information of a complete genome for each individual. At Partners HealthCare Personalized Medicine, we have developed a clinical process for whole genome sequencing (WGS) with application in both healthy individuals and those with disease. In this manuscript, we will describe our bioinformatics strategy to efficiently process and deliver genomic data to geneticists for clinical interpretation. We describe the handling of data from FASTQ to the final variant list for clinical review for the final report. We will also discuss our methodology for validating this workflow and the cost implications of running WGS.},
author = {Tsai, Ellen and Shakbatyan, Rimma and Evans, Jason and Rossetti, Peter and Graham, Chet and Sharma, Himanshu and Lin, Chiao-Feng and Lebo, Matthew},
doi = {10.3390/jpm6010012},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai et al. - 2016 - Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine.pdf:pdf},
issn = {2075-4426},
journal = {Journal of Personalized Medicine},
keywords = {NGS,WGS,bioinformatics,clinical sequencing,next generation sequencing,precision medicine,validation},
number = {1},
pages = {12},
pmid = {26927186},
title = {{Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine}},
url = {http://www.mdpi.com/2075-4426/6/1/12/htm http://www.mdpi.com/2075-4426/6/1/12},
volume = {6},
year = {2016}
}
@misc{Babraham2016,
author = {{Babraham Bioinformatics}},
booktitle = {www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules},
title = {{FASTQC manual}},
url = {http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules/},
urldate = {2016-06-25},
year = {2016}
}
@article{Cleary2014,
author = {Cleary, John G and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S and Inglis, Stuart and Irvine, Sean A and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M},
doi = {10.1089/cmb.2014.0029},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and {<}i{>}De Novo{<}/i{>} Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@book{,
title = {{FastQC Manual}},
url = {https://biof-edu.colorado.edu/videos/dowell-short-read-class/day-4/fastqc-manual}
}
@book{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - FastQC Manual.pdf:pdf},
title = {{FastQC Manual}},
url = {https://biof-edu.colorado.edu/videos/dowell-short-read-class/day-4/fastqc-manual}
}
@article{Cock2009,
abstract = {FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.},
author = {Cock, Peter J A and Fields, Christopher J. and Goto, Naohisa and Heuer, Michael L. and Rice, Peter M.},
doi = {10.1093/nar/gkp1137},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cock et al. - 2009 - The Sanger FASTQ file format for sequences with quality scores, and the SolexaIllumina FASTQ variants.pdf:pdf},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {6},
pages = {1767--1771},
pmid = {20015970},
title = {{The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants}},
volume = {38},
year = {2009}
}
@article{Bamshad2011,
abstract = {Nature Reviews Genetics 12, 745 (2011). doi:10.1038/nrg3031},
archivePrefix = {arXiv},
arxivId = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
author = {Bamshad, Michael J and Ng, Sarah B and Bigham, Abigail W and Tabor, Holly K and Emond, Mary J and Nickerson, Deborah A and Shendure, Jay},
doi = {10.1038/nrg3031},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bamshad et al. - 2011 - Exome sequencing as a tool for Mendelian disease gene discovery.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Publishing Group},
keywords = {Alleles,Base Sequence,Exome,Exome: genetics,Genetic Predisposition to Disease,Genome, Human,Genome-Wide Association Study,Humans,Molecular Sequence Data,Pedigree,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {11},
pages = {745--755},
pmid = {21946919},
primaryClass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:},
publisher = {Nature Publishing Group},
title = {{Exome sequencing as a tool for Mendelian disease gene discovery}},
url = {http://dx.doi.org/10.1038/nrg3031%5Cnpapers2://publication/doi/10.1038/nrg3031},
volume = {12},
year = {2011}
}
@article{Frazer2009,
abstract = {The last few years have seen extensive efforts to catalogue human genetic variation and correlate it with phenotypic differences. Most common SNPs have now been assessed in genome-wide studies for statistical associations with many complex traits, including many important common diseases. Although these studies have provided new biological insights, only a limited amount of the heritable component of any complex trait has been identified and it remains a challenge to elucidate the functional link between associated variants and phenotypic traits. Technological advances, such as the ability to detect rare and structural variants, and a clear understanding of the challenges in linking different types of variation with phenotype, will be essential for future progress.},
author = {Frazer, KA and Murray, SS and Schork, NJ and Topol, EJ},
doi = {10.1038/nrg2554},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frazer et al. - 2009 - Human genetic variation and its contribution to complex traits.pdf:pdf},
isbn = {1471-0064 (Electronic)1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Reviews Genetics},
number = {4},
pages = {241--51},
pmid = {19293820},
title = {{Human genetic variation and its contribution to complex traits}},
volume = {10},
year = {2009}
}
@misc{Tetreault2015a,
abstract = {Whole-exome sequencing (WES) represents a significant breakthrough in the field of human genetics. This technology has largely contributed to the identification of new disease-causing genes and is now entering clinical laboratories. WES represents a powerful tool for diagnosis and could reduce the ‘diagnostic odyssey' for many patients. In this review, we present a technical overview of WES analysis, variants annotation and interpretation in a clinical setting. We evaluate the usefulness of clinical WES in different clinical indications, such as rare diseases, cancer and complex diseases. Finally, we discuss the efficacy of WES as a diagnostic tool and the impact on patient management.},
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
booktitle = {Expert Review of Molecular Diagnostics},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
keywords = {cancer,diagnostic,rare diseases,variants detection,whole-exome sequencing},
language = {en},
month = {may},
publisher = {Informa Healthcare},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities}},
url = {http://www.tandfonline.com/doi/abs/10.1586/14737159.2015.1039516?journalCode=iero20#.VxmiWVg_zcU.mendeley},
year = {2015}
}
@article{Lazaridis2016,
abstract = {OBJECTIVE: To describe the experience and outcome of performing whole-exome sequencing (WES) for resolution of patients on a diagnostic odyssey in the first 18 months of an individualized medicine clinic (IMC).

PATIENTS AND METHODS: The IMC offered WES to physicians of Mayo Clinic practice for patients with suspected genetic disease. DNA specimens of the proband and relatives were submitted to WES laboratories. We developed the Genomic Odyssey Board with multidisciplinary expertise to determine the appropriateness for IMC services, review WES reports, and make the final decision about whether the exome findings explain the disease. This study took place from September 30, 2012, to March 30, 2014.

RESULTS: In the first 18 consecutive months, the IMC received 82 consultation requests for patients on a diagnostic odyssey. The Genomic Odyssey Board deferred 7 cases and approved 75 cases to proceed with WES. Seventy-one patients met with an IMC genomic counselor. Fifty-one patients submitted specimens for WES testing, and the results have been received for all. There were 15 cases in which a diagnosis was made on the basis of WES findings; thus, the positive diagnostic yield of this practice was 29%. The mean cost per patient for this service was approximately $8000. Medicaid supported 27% of the patients, and 38% of patients received complete or partial insurance coverage.

CONCLUSION: The significant diagnostic yield, moderate cost, and notable health marketplace acceptance for WES compared with conventional genetic testing make the former method a rational diagnostic approach for patients on a diagnostic odyssey.},
author = {Lazaridis, Konstantinos N and Schahl, Kimberly A and Cousin, Margot A and Babovic-Vuksanovic, Dusica and Riegert-Johnson, Douglas L and Gavrilova, Ralitza H and McAllister, Tammy M and Lindor, Noralane M and Abraham, Roshini S and Ackerman, Michael J and Pichurin, Pavel N and Deyle, David R and Gavrilov, Dimitar K and Hand, Jennifer L and Klee, Eric W and Stephens, Michael C and Wick, Myra J and Atkinson, Elizabeth J and Linden, David R and Ferber, Matthew J and Wieben, Eric D and Farrugia, Gianrico},
doi = {10.1016/j.mayocp.2015.12.018},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
issn = {1942-5546},
journal = {Mayo Clinic proceedings},
language = {English},
month = {mar},
number = {3},
pages = {297--307},
pmid = {26944241},
publisher = {Elsevier},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
url = {http://www.mayoclinicproceedings.org/article/S0025619616000240/fulltext},
volume = {91},
year = {2016}
}
@article{McKenna2009,
abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS—the 1000 Genome pilot alone includes nearly five terabases—make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management in- frastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide poly- morphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas},
author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and Depristo, Mark A},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenna et al. - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Cold Spring Harbor Laboratory Press Resource},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data}},
volume = {20},
year = {2009}
}
@article{KonstantinosN.LazaridisMD;KimberlyA.SchahlCGC;MargotA.CousinPhD;DusicaBabovic-VuksanovicMD;DouglasL.Riegert-JohnsonMD;RalitzaH.GavrilovaMD;TammyM.McAllisterMA;NoralaneM.LindorMD;RoshiniS.AbrahamPhD;MichaelJ.Ac2016,
author = {{Konstantinos N. Lazaridis, MD; Kimberly A. Schahl, CGC; Margot A. Cousin, PhD; Dusica Babovic-Vuksanovic, MD; Douglas L. Riegert-Johnson, MD; Ralitza H. Gavrilova, MD; Tammy M. McAllister, MA; Noralane M. Lindor, MD; Roshini S. Abraham, PhD; Michael J. Ac}, MD; and the Individualized Medicine Clinic Members},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
journal = {Mayo Clinic Proceedings},
number = {3},
pages = {29},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
volume = {91},
year = {2016}
}
@article{Tetreault2015,
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
journal = {Expert Review of Molecular Diagnostics},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities.}},
year = {2015}
}
@misc{Xuan2013,
abstract = {The advent of next generation sequencing (NGS) technologies has revolutionized the field of genomics, enabling fast and cost-effective generation of genome-scale sequence data with exquisite resolution and accuracy. Over the past years, rapid technological advances led by academic institutions and companies have continued to broaden NGS applications from research to the clinic. A recent crop of discoveries have highlighted the medical impact of NGS technologies on Mendelian and complex diseases, particularly cancer. However, the ever-increasing pace of NGS adoption presents enormous challenges in terms of data processing, storage, management and interpretation as well as sequencing quality control, which hinder the translation from sequence data into clinical practice. In this review, we first summarize the technical characteristics and performance of current NGS platforms. We further highlight advances in the applications of NGS technologies towards the development of clinical diagnostics and therapeutics. Common issues in NGS workflows are also discussed to guide the selection of NGS platforms and pipelines for specific research purposes. ?? 2012.},
author = {Xuan, Jiekun and Yu, Ying and Qing, Tao and Guo, Lei and Shi, Leming},
booktitle = {Cancer Letters},
doi = {10.1016/j.canlet.2012.11.025},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xuan et al. - 2013 - Next-generation sequencing in the clinic Promises and challenges.pdf:pdf},
isbn = {1872-7980 (Electronic)\r0304-3835 (Linking)},
issn = {03043835},
keywords = {Bioinformatics,Clinical applications,Exome sequencing,FFPE,RNA-Seq,Tumor heterogeneity,Whole-genome sequencing},
number = {2},
pages = {284--295},
pmid = {23174106},
title = {{Next-generation sequencing in the clinic: Promises and challenges}},
volume = {340},
year = {2013}
}
@article{Lescai2014,
abstract = {The choice of an appropriate variant calling pipeline for exome sequencing data is becoming increasingly more important in translational medicine projects and clinical contexts. Within GOSgene, which facilitates genetic analysis as part of a joint effort of the University College London and the Great Ormond Street Hospital, we aimed to optimize a variant calling pipeline suitable for our clinical context. We implemented the GATK/Queue framework and evaluated the performance of its two callers: the classical UnifiedGenotyper and the new variant discovery tool HaplotypeCaller. We performed an experimental validation of the loss-of-function (LoF) variants called by the two methods using Sequenom technology. UnifiedGenotyper showed a total validation rate of 97.6% for LoF single-nucleotide polymorphisms (SNPs) and 92.0% for insertions or deletions (INDELs), whereas HaplotypeCaller was 91.7% for SNPs and 55.9% for INDELs. We confirm that GATK/Queue is a reliable pipeline in translational medicine and clinical context. We conclude that in our working environment, UnifiedGenotyper is the caller of choice, being an accurate method, with a high validation rate of error-prone calls like LoF variants. We finally highlight the importance of experimental validation, especially for INDELs, as part of a standard pipeline in clinical environments.},
author = {Lescai, Francesco and Marasco, Elena and Bacchelli, Chiara and Stanier, Philip and Mantovani, Vilma and Beales, Philip},
doi = {10.1002/mgg3.42},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lescai et al. - 2014 - Identification and validation of loss of function variants in clinical contexts.pdf:pdf},
issn = {2324-9269},
journal = {Molecular genetics & genomic medicine},
keywords = {gatk,pipelines,sequencing,variant calling},
number = {1},
pages = {58--63},
pmid = {24498629},
title = {{Identification and validation of loss of function variants in clinical contexts.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3907911&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{Li2012,
abstract = {Family samples, which can be enriched for rare causal variants by focusing on families with multiple extreme individuals and which facilitate detection of de novo mutation events, provide an attractive resource for next-generation sequencing studies. Here, we describe, implement, and evaluate a likelihood-based framework for analysis of next generation sequence data in family samples. Our framework is able to identify variant sites accurately and to assign individual genotypes, and can handle de novo mutation events, increasing the sensitivity and specificity of variant calling and de novo mutation detection. Through simulations we show explicit modeling of family relationships is especially useful for analyses of low-frequency variants and that genotype accuracy increases with the number of individuals sequenced per family. Compared with the standard approach of ignoring relatedness, our methods identify and accurately genotype more variants, and have high specificity for detecting de novo mutation events. The improvement in accuracy using our methods over the standard approach is particularly pronounced for low-frequency variants. Furthermore the family-aware calling framework dramatically reduces Mendelian inconsistencies and is beneficial for family-based analysis. We hope our framework and software will facilitate continuing efforts to identify genetic factors underlying human diseases.},
author = {Li, Bingshan and Chen, Wei and Zhan, Xiaowei and Busonero, Fabio and Sanna, Serena and Sidore, Carlo and Cucca, Francesco and Kang, Hyun M. and Abecasis, Gon??alo R.},
doi = {10.1371/journal.pgen.1002944},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families.pdf:pdf},
isbn = {1553-7404 (Electronic)\n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {10},
pmid = {23055937},
title = {{A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families}},
volume = {8},
year = {2012}
}
@article{Li2014a,
abstract = {MOTIVATION: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods.\n\nRESULTS: We made 10 single nucleotide polymorphism and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10-15 kb, but the error rate of post-filtered calls is reduced to 1 in 100-200 kb without significant compromise on the sensitivity. Availability and implementation: BWA-MEM alignment and raw variant calls are available at http://bit.ly/1g8XqRt scripts and miscellaneous data at https://github.com/lh3/varcmp.\n\nCONTACT: hengli@broadinstitute.org Supplementary information: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.0929v1},
author = {Li, Heng},
doi = {10.1093/bioinformatics/btu356},
eprint = {arXiv:1404.0929v1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2014 - Toward better understanding of artifacts in variant calling from high-coverage samples.pdf:pdf},
isbn = {1367-4803},
issn = {13674811},
journal = {Bioinformatics (Oxford, England)},
number = {20},
pages = {2843--2851},
pmid = {24974202},
title = {{Toward better understanding of artifacts in variant calling from high-coverage samples}},
volume = {30},
year = {2014}
}
@article{Cleary2014,
author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M.},
doi = {10.1089/cmb.2014.0029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleary et al. - 2014 - Joint Variant and iDe Novoi Mutation Identification on Pedigrees from High-Throughput Sequencing Data.pdf:pdf},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and <i>De Novo</i> Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@article{Cartwright2012,
abstract = {Recent advances in high-throughput DNA sequencing technologies and associated statistical analyses have enabled in-depth analysis of whole-genome sequences. As this technology is applied to a growing number of individual human genomes, entire families are now being sequenced. Information contained within the pedigree of a sequenced family can be leveraged when inferring the donors' genotypes. The presence of a de novo mutation within the pedigree is indicated by a violation of Mendelian inheritance laws. Here, we present a method for probabilistically inferring genotypes across a pedigree using high-throughput sequencing data and producing the posterior probability of de novo mutation at each genomic site examined. This framework can be used to disentangle the effects of germline and somatic mutational processes and to simultaneously estimate the effect of sequencing error and the initial genetic variation in the population from which the founders of the pedigree arise. This approach is examined in detail through simulations and areas for method improvement are noted. By applying this method to data from members of a well-defined nuclear family with accurate pedigree information, the stage is set to make the most direct estimates of the human mutation rate to date.},
author = {Cartwright, Reed A and Keebler, Jonathan E M and Carolina, North and Stone, Eric A and Hussin, Julie},
doi = {10.2202/1544-6115.1713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright et al. - 2012 - S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput S.pdf:pdf},
isbn = {1544-6115 (Electronic)\r1544-6115 (Linking)},
issn = {1544-6115},
journal = {Statistical applications in genetics and molecular biology},
keywords = {Algorithms,Alleles,Computer Simulation,DNA Mutational Analysis,Family,Genetic,Genome,Genotype,High-Throughput Nucleotide Sequencing,Human,Humans,Models,Mutation,Pedigree,Probability,ROC Curve},
number = {2},
pmid = {22499693},
title = {{S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3728889&tool=pmcentrez&rendertype=abstract%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/22499693%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3728889},
volume = {11},
year = {2012}
}
@article{Warden2014,
abstract = {The Genome Analysis Toolkit (GATK) is commonly used for variant calling of single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels) from short-read sequencing data aligned against a reference genome. There have been a number of variant calling comparisons against GATK, but an equally comprehensive comparison for VarScan not yet been performed. More specifically, we compare (1) the effects of different pre-processing steps prior to variant calling with both GATK and VarScan, (2) VarScan variants called with increasingly conservative parameters, and (3) filtered and unfiltered GATK variant calls (for both the UnifiedGenotyper and the HaplotypeCaller). Variant calling was performed on three datasets (1 targeted exon dataset and 2 exome datasets), each with approximately a dozen subjects. In most cases, pre-processing steps (e.g., indel realignment and quality score base recalibration using GATK) had only a modest impact on the variant calls, but the importance of the pre-processing steps varied between datasets and variant callers. Based upon concordance statistics presented in this study, we recommend GATK users focus on "high-quality" GATK variants by filtering out variants flagged as low-quality. We also found that running VarScan with a conservative set of parameters (referred to as "VarScan-Cons") resulted in a reproducible list of variants, with high concordance (>97%) to high-quality variants called by the GATK UnifiedGenotyper and HaplotypeCaller. These conservative parameters result in decreased sensitivity, but the VarScan-Cons variant list could still recover 84-88% of the high-quality GATK SNPs in the exome datasets. This study also provides limited evidence that VarScan-Cons has a decreased false positive rate among novel variants (relative to high-quality GATK SNPs) and that the GATK HaplotypeCaller has an increased false positive rate for indels (relative to VarScan-Cons and high-quality GATK UnifiedGenotyper indels). More broadly, we believe the metrics used for comparison in this study can be useful in assessing the quality of variant calls in the context of a specific experimental design. As an example, a limited number of variant calling comparisons are also performed on two additional variant callers.},
author = {Warden, Charles D. and Adamson, Aaron W. and Neuhausen, Susan L. and Wu, Xiwei},
doi = {10.7717/peerj.600},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warden et al. - 2014 - Detailed comparison of two popular variant calling packages for exome and targeted exon studies.pdf:pdf},
isbn = {2167-8359 (Electronic)},
issn = {2167-8359},
journal = {PeerJ},
keywords = {exome,gatk,small indel,snp,targeted sequencing,variant calling,varscan},
pages = {e600},
pmid = {25289185},
title = {{Detailed comparison of two popular variant calling packages for exome and targeted exon studies}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4184249&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{Moore2010,
abstract = {MOTIVATION: The sequencing of the human genome has made it possible to identify an informative set of >1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs). The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing. This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies. However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing. One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology. Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context. There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype-phenotype relationship that is characterized by significant heterogeneity and gene-gene and gene-environment interaction. We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases. The goal of this review is to identify and discuss those GWAS challenges that will require computational methods.},
author = {Moore, Jason H. and Asselbergs, Folkert W. and Williams, Scott M.},
doi = {10.1093/bioinformatics/btp713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Asselbergs, Williams - 2010 - Bioinformatics challenges for genome-wide association studies.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pages = {445--455},
pmid = {20053841},
title = {{Bioinformatics challenges for genome-wide association studies}},
volume = {26},
year = {2010}
}
@article{Bao2014,
abstract = {The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigentic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing vaiants and discover gene targets. In this review, we outline the general framework of whole exome processing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis and summarization in the field of bioinformatics.},
author = {Bao, Riyue and Huang, Lei and Andrade, Jorge and Tan, Wei and Kibbe, Warren a and Jiang, Hongmei and Feng, Gang},
doi = {10.4137/CIN.S13779.Received},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bao et al. - 2014 - Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequenci.pdf:pdf},
isbn = {1176-9351 (Electronic)\r1176-9351 (Linking)},
issn = {1176-9351},
journal = {Libertas Academica},
keywords = {10,13,4137,67,82 doi,a,and data management for,and statistical analysis of,applications,bao et al,big data,cancer data,cancer informatics 2014,cin,citation,classification,indel,next generation sequencing,predictive modelling,review of current methods,s13779,s2,sequence alignment,snv,supplement,the bioinformatics analysis of,variant analysis,whole exome sequencing},
pages = {67--82},
pmid = {25288881},
title = {{Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing}},
volume = {13},
year = {2014}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{Li2009,
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
title = {{Fast and accurate short read alignment with Burrows–Wheeler transform}},
volume = {25},
year = {2009}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803$\$r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.$\$n$\$nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.$\$n$\$nDATA SOURCES: Literature review and authors' experience.$\$n$\$nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M and Durtschi, Jacob D and Margraf, Rebecca L and Voelkerding, Karl V},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)$\$r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Cornish2015,
abstract = {High-throughput sequencing, especially of exomes, is a popular diagnostic tool, but it is difficult to determine which tools are the best at analyzing this data. In this study, we use the NIST Genome in a Bottle results as a novel resource for validation of our exome analysis pipeline. We use six different aligners and five different variant callers to determine which pipeline, of the 30 total, performs the best on a human exome that was used to help generate the list of variants detected by the Genome in a Bottle Consortium. Of these 30 pipelines, we found that Novoalign in conjunction with GATK UnifiedGenotyper exhibited the highest sensitivity while maintaining a low number of false positives for SNVs. However, it is apparent that indels are still difficult for any pipeline to handle with none of the tools achieving an average sensitivity higher than 33% or a Positive Predictive Value (PPV) higher than 53%. Lastly, as expected, it was found that aligners can play as vital a role in variant detection as variant callers themselves},
author = {Cornish, Adam and Guda, Chittibabu},
doi = {10.1155/2015/456479},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cornish, Guda - 2015 - A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference.pdf:pdf},
issn = {23146141},
journal = {BioMed Research International},
number = {BioMed Research International},
pages = {11},
pmid = {26539496},
title = {{A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference}},
volume = {2015},
year = {2015}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{%} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P and Houwing-Duistermaat, Jeanine J and {Eline Slagboom}, P and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bash - 2015 - VARIATION INTERPRETATION PREDICTORS PRINCIPLES, TYPES, PERFORMANCE AND CHOICE.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Wang2010,
abstract = {High-throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes, but it remains a challenge to pinpoint a small subset of functionally important variants. To fill these unmet needs, we developed the ANNOVAR tool to annotate single nucleotide variants (SNVs) and insertions/deletions, such as examining their functional consequence on genes, inferring cytogenetic bands, reporting functional importance scores, finding variants in conserved regions, or identifying variants reported in the 1000 Genomes Project and dbSNP. ANNOVAR can utilize annotation databases from the UCSC Genome Browser or any annotation data set conforming to Generic Feature Format version 3 (GFF3). We also illustrate a 'variants reduction' protocol on 4.7 million SNVs and indels from a human genome, including two causal mutations for Miller syndrome, a rare recessive disease. Through a stepwise procedure, we excluded variants that are unlikely to be causal, and identified 20 candidate genes including the causal gene. Using a desktop computer, ANNOVAR requires ∼4 min to perform gene-based annotation and ∼15 min to perform variants reduction on 4.7 million variants, making it practical to handle hundreds of human genomes in a day. ANNOVAR is freely available at http://www.openbioinformatics.org/annovar/.},
author = {Wang, Kai and Li, Mingyao and Hakonarson, Hakon},
doi = {10.1093/nar/gkq603},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Li, Hakonarson - 2010 - ANNOVAR functional annotation of genetic variants from high-throughput sequencing data.pdf:pdf},
isbn = {0305-1048},
issn = {13624962},
journal = {Nucleic acids research},
number = {16},
pages = {e164},
pmid = {20601685},
title = {{ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data}},
volume = {38},
year = {2010}
}
@book{Quinlan2014,
abstract = {Technological advances have enabled the use of DNA sequencing as a flexible tool to characterize genetic variation and to measure the activity of diverse cellular phenomena such as gene isoform expression and transcription factor binding. Extracting biological insight from the experiments enabled by these advances demands the analysis of large, multi-dimensional datasets. This unit describes the use of the BEDTools toolkit for the exploration of high-throughput genomics datasets. Several protocols are presented for common genomic analyses, demonstrating how simple BEDTools operations may be combined to create bespoke pipelines addressing complex questions. Curr. Protoc. Bioinform. 47:11.12.1-11.12.34. {\textcopyright} 2014 by John Wiley & Sons, Inc.},
author = {Quinlan, Aaron R.},
booktitle = {Current Protocols in Bioinformatics},
doi = {10.1002/0471250953.bi1112s47},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan - 2014 - BEDTools The Swiss-Army tool for genome feature analysis.pdf:pdf},
isbn = {0471250953},
issn = {1934340X},
keywords = {Bioinformatics,Genome analysis,Genome features,Genome intervals,Genomics},
pages = {11.12.1--11.12.34},
pmid = {25199790},
title = {{BEDTools: The Swiss-Army tool for genome feature analysis}},
volume = {2014},
year = {2014}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.\n\nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3% (87.0%) and average genotype concordance in correctly identified SNVs was 99.2% (99.2%) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.\n\nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Li2009a,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Quinlan2010,
abstract = {MOTIVATION: Testing for correlations between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.\n\nRESULTS: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format. BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features. The tools are extremely efficient and allow the user to compare large datasets (e.g. next-generation sequencing data) with both public and custom genome annotation tracks. BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.\n\nAVAILABILITY AND IMPLEMENTATION: BEDTools was written in C++. Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtools\n\nCONTACT: aaronquinlan@gmail.com; imh4y@virginia.edu\n\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Quinlan, Aaron R. and Hall, Ira M.},
doi = {10.1093/bioinformatics/btq033},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan, Hall - 2010 - BEDTools A flexible suite of utilities for comparing genomic features.pdf:pdf},
isbn = {1367-4811 (Electronic)\n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {841--842},
pmid = {20110278},
title = {{BEDTools: A flexible suite of utilities for comparing genomic features}},
volume = {26},
year = {2010}
}
@article{Liu2013,
abstract = {Next generation sequencing (NGS) has been leading the genetic study of human disease into an era of unprecedented productivity. Many bioinformatics pipelines have been developed to call variants from NGS data. The performance of these pipelines depends crucially on the variant caller used and on the calling strategies implemented. We studied the performance of four prevailing callers, SAMtools, GATK, glftools and Atlas2, using single-sample and multiple-sample variant-calling strategies. Using the same aligner, BWA, we built four single-sample and three multiple-sample calling pipelines and applied the pipelines to whole exome sequencing data taken from 20 individuals. We obtained genotypes generated by Illumina Infinium HumanExome v1.1 Beadchip for validation analysis and then used Sanger sequencing as a "gold-standard" method to resolve discrepancies for selected regions of high discordance. Finally, we compared the sensitivity of three of the single-sample calling pipelines using known simulated whole genome sequence data as a gold standard. Overall, for single-sample calling, the called variants were highly consistent across callers and the pairwise overlapping rate was about 0.9. Compared with other callers, GATK had the highest rediscovery rate (0.9969) and specificity (0.99996), and the Ti/Tv ratio out of GATK was closest to the expected value of 3.02. Multiple-sample calling increased the sensitivity. Results from the simulated data suggested that GATK outperformed SAMtools and glfSingle in sensitivity, especially for low coverage data. Further, for the selected discrepant regions evaluated by Sanger sequencing, variant genotypes called by exome sequencing versus the exome array were more accurate, although the average variant sensitivity and overall genotype consistency rate were as high as 95.87% and 99.82%, respectively. In conclusion, GATK showed several advantages over other variant callers for general purpose NGS analyses. The GATK pipelines we developed perform very well.},
author = {Liu, Xiangtao and Han, Shizhong and Wang, Zuoheng and Gelernter, Joel and Yang, Bao Zhu},
doi = {10.1371/journal.pone.0075619},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Variant Callers for Next-Generation Sequencing Data A Comparison Study.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--11},
pmid = {24086590},
title = {{Variant Callers for Next-Generation Sequencing Data: A Comparison Study}},
volume = {8},
year = {2013}
}
@article{Ghoneim2014,
abstract = {BACKGROUND: Insertions/deletions (indels) are the second most common type of genomic variant and the most common type of structural variant. Identification of indels in next generation sequencing data is a challenge, and algorithms commonly used for indel detection have not been compared on a research cohort of human subject genomic data. Guidelines for the optimal detection of biologically significant indels are limited. We analyzed three sets of human next generation sequencing data (48 samples of a 200 gene target exon sequencing, 45 samples of whole exome sequencing, and 2 samples of whole genome sequencing) using three algorithms for indel detection (Pindel, Genome Analysis Tool Kit's UnifiedGenotyper and HaplotypeCaller). RESULTS: We observed variation in indel calls across the three algorithms. The intersection of the three tools comprised only 5.70% of targeted exon, 19.52% of whole exome, and 14.25% of whole genome indel calls. The majority of the discordant indels were of lower read depth and likely to be false positives. When software parameters were kept consistent across the three targets, HaplotypeCaller produced the most reliable results. Pindel results did not validate well without adjustments to parameters to account for varied read depth and number of samples per run. Adjustments to Pindel's M (minimum support for event) parameter improved both concordance and validation rates. Pindel was able to identify large deletions that surpassed the length capabilities of the GATK algorithms. CONCLUSIONS: Despite the observed variability in indel identification, we discerned strengths among the individual algorithms on specific data sets. This allowed us to suggest best practices for indel calling. Pindel's low validation rate of indel calls made in targeted exon sequencing suggests that HaplotypeCaller is better suited for short indels and multi-sample runs in targets with very high read depth. Pindel allows for optimization of minimum support for events and is best used for detection of larger indels at lower read depths.},
author = {Ghoneim, Dalia H and Myers, Jason R and Tuttle, Emily and Paciorkowski, Alex R},
doi = {10.1186/1756-0500-7-864},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghoneim et al. - 2014 - Comparison of insertiondeletion calling algorithms on human next-generation sequencing data.pdf:pdf},
isbn = {1756-0500 (Electronic)\r1756-0500 (Linking)},
issn = {1756-0500},
journal = {BMC research notes},
keywords = {concordance,gatk,indels,next generation sequencing,pindel,validation},
number = {1},
pages = {864},
pmid = {25435282},
title = {{Comparison of insertion/deletion calling algorithms on human next-generation sequencing data.}},
url = {http://www.biomedcentral.com/1756-0500/7/864},
volume = {7},
year = {2014}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@book{Auwera2014,
author = {Auwera, Geraldine A Van Der and Carneiro, Mauricio O and Hartl, Chris and Poplin, Ryan and Levy-moonshine, Ami and Jordan, Tadeusz and Shakir, Khalid and Roazen, David and Thibault, Joel and Banks, Eric and Garimella, Kiran V and Altshuler, David and Gabriel, Stacey and Depristo, Mark A},
booktitle = {Curr Protoc Bioinformatics},
doi = {10.1002/0471250953.bi1110s43.From},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auwera et al. - 2014 - From FastQ data to high confidence varant calls the Genonme Analysis Toolkit best practices pipeline.pdf:pdf},
isbn = {0471250953},
keywords = {exome,genotyping,ngs,variant detection,wgs},
number = {1110},
title = {{From FastQ data to high confidence varant calls: the Genonme Analysis Toolkit best practices pipeline}},
volume = {11},
year = {2014}
}
@article{Yang2015,
abstract = {Recent developments in sequencing techniques have enabled rapid and high-throughput generation of sequence data, democratizing the ability to compile information on large amounts of genetic variations in individual laboratories. However, there is a growing gap between the generation of raw sequencing data and the extraction of meaningful biological information. Here, we describe a protocol to use the ANNOVAR (ANNOtate VARiation) software to facilitate fast and easy variant annotations, including gene-based, region-based and filter-based annotations on a variant call format (VCF) file generated from human genomes. We further describe a protocol for gene-based annotation of a newly sequenced nonhuman species. Finally, we describe how to use a user-friendly and easily accessible web server called wANNOVAR to prioritize candidate genes for a Mendelian disease. The variant annotation protocols take 5-30 min of computer time, depending on the size of the variant file, and 5-10 min of hands-on time. In summary, through the command-line tool and the web server, these protocols provide a convenient means to analyze genetic variants generated in humans and other species.},
author = {Yang, H and Wang, K},
doi = {10.1038/nprot.2015.105},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Wang - 2015 - Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR.pdf:pdf},
isbn = {1750-2799 (Electronic)\r1750-2799 (Linking)},
issn = {1750-2799},
journal = {Nat Protoc},
number = {10},
pages = {1556--1566},
pmid = {26379229},
publisher = {Nature Publishing Group},
title = {{Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26379229%5Cnhttp://www.nature.com/nprot/journal/v10/n10/pdf/nprot.2015.105.pdf},
volume = {10},
year = {2015}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95% confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P. and Houwing-Duistermaat, Jeanine J. and {Eline Slagboom}, P. and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Niroula2016,
abstract = {Next generation sequencing (NGS) methods have revolutionized the speed of generating variation information. Sequence data have a plethora of applications and will increasingly be used for disease diagnosis. Interpretation of the identified variants is usually not possible with experimental methods. This has caused a bottleneck that many computational methods aim at addressing. Fast and efficient methods for explaining the significance and mechanisms of detected variants are required for efficient precision/personalized medicine. Computational prediction methods have been developed in three areas to address the issue. There are generic tolerance (pathogenicity) predictors for filtering harmful variants. Gene/protein/disease-specific tools are available for some applications. Mechanism and effect-specific computer programs aim at explaining the consequences of variations. Here, we discuss the different types of predictors and their applications. We review available variation databases and prediction methods useful for variation interpretation. We discuss how the performance of methods is assessed and summarize existing assessment studies. A brief introduction is provided to the principles of the methods developed for variation interpretation as well as guidelines for how to choose the optimal tools and where the field is heading in the future. This article is protected by copyright. All rights reserved.},
author = {Niroula, Abhishek and Vihinen, Mauno},
doi = {10.1002/humu.22987},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niroula, Vihinen - 2016 - Variation Interpretation Predictors Principles, Types, Performance and Choice.pdf:pdf},
issn = {1098-1004},
journal = {Human mutation},
month = {mar},
pmid = {26987456},
title = {{Variation Interpretation Predictors: Principles, Types, Performance and Choice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26987456},
year = {2016}
}
@article{Neto2014,
abstract = {Inherited myopathies are a heterogeneous group of disabling disorders with still barely understood pathological mechanisms. Around 40% of afflicted patients remain without a molecular diagnosis after exclusion of known genes. The advent of high-throughput sequencing has opened avenues to the discovery of new implicated genes, but a working list of prioritized candidate genes is necessary to deal with the complexity of analyzing large-scale sequencing data. Here we used an integrative data mining strategy to analyze the genetic network linked to myopathies, derive specific signatures for inherited myopathy and related disorders, and identify and rank candidate genes for these groups. Training sets of genes were selected after literature review and used in Manteia, a public web-based data mining system, to extract disease group signatures in the form of enriched descriptor terms, which include functional annotation, human and mouse phenotypes, as well as biological pathways and protein interactions. These specific signatures were then used as an input to mine and rank candidate genes, followed by filtration against skeletal muscle expression and association with known diseases. Signatures and identified candidate genes highlight both potential common pathological mechanisms and allelic disease groups. Recent discoveries of gene associations to diseases, like B3GALNT2, GMPPB and B3GNT1 to congenital muscular dystrophies, were prioritized in the ranked lists, suggesting a posteriori validation of our approach and predictions. We show an example of how the ranked lists can be used to help analyze high-throughput sequencing data to identify candidate genes, and highlight the best candidate genes matching genomic regions linked to myopathies without known causative genes. This strategy can be automatized to generate fresh candidate gene lists, which help cope with database annotation updates as new knowledge is incorporated.},
author = {Neto, Osorio Abath and Tassy, Olivier and Biancalana, Val?? Rie and Zanoteli, Edmar and Pourqui??, Olivier and Laporte, Jocelyn},
doi = {10.1371/journal.pone.0110888},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto et al. - 2014 - Integrative data mining highlights candidate genes for monogenic myopathies.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {25353622},
title = {{Integrative data mining highlights candidate genes for monogenic myopathies}},
volume = {9},
year = {2014}
}
@article{Li2015,
abstract = {Background: Conditions associated with sudden cardiac arrest/death (SCA/D) in youth often have a genetic etiology. While SCA/D is uncommon, a pro-active family screening approach may identify these inherited structural and electrical abnormalities prior to symptomatic events and allow appropriate surveillance and treatment. This study investigated the diagnostic utility of exome sequencing (ES) by evaluating the capture and coverage of genes related to SCA/D. Methods: Samples from 102 individuals (13 with known molecular etiologies for SCA/D, 30 individuals without known molecular etiologies for SCA/D and 59 with other conditions) were analyzed following exome capture and sequencing at an average read depth of 100X. Reads were mapped to human genome GRCh37 using Novoalign, and post-processing and analysis was done using Picard and GATK. A total of 103 genes (2,190 exons) related to SCA/D were used as a primary filter. An additional 100 random variants within the targeted genes associated with SCA/D were also selected and evaluated for depth of sequencing and coverage. Although the primary objective was to evaluate the adequacy of depth of sequencing and coverage of targeted SCA/D genes and not for primary diagnosis, all patients who had SCA/D (known or unknown molecular etiologies) were evaluated with the project's variant analysis pipeline to determine if the molecular etiologies could be successfully identified. Results: The majority of exons (97.6 %) were captured and fully covered on average at minimum of 20x sequencing depth. The proportion of unique genomic positions reported within poorly covered exons remained small (4 %). Exonic regions with less coverage reflect the need to enrich these areas to improve coverage. Despite limitations in coverage, we identified 100 % of cases with a prior known molecular etiology for SCA/D, and analysis of an additional 30 individuals with SCA/D but no known molecular etiology revealed a diagnostic answer in 5/30 (17 %). We also demonstrated 95 % of 100 randomly selected reported variants within our targeted genes would have been picked up on ES based on our coverage analysis. Conclusions: ES is a helpful clinical diagnostic tool for SCA/D given its potential to successfully identify a molecular diagnosis, but clinicians should be aware of limitations of available platforms from technical and diagnostic perspectives.},
author = {Li, Mindy H. and Abrudan, Jenica L. and Dulik, Matthew C. and Sasson, Ariella and Brunton, Joshua and Jayaraman, Vijayakumar and Dugan, Noreen and Haley, Danielle and Rajagopalan, Ramakrishnan and Biswas, Sawona and Sarmady, Mahdi and DeChene, Elizabeth T. and Deardorff, Matthew A. and Wilkens, Alisha and Noon, Sarah E. and Scarano, Maria I. and Santani, Avni B. and White, Peter S. and Pennington, Jeffrey and Conlin, Laura K. and Spinner, Nancy B. and Krantz, Ian D. and Vetter, Victoria L.},
doi = {10.1186/s40246-015-0038-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sud.pdf:pdf},
issn = {1479-7364},
journal = {Human Genomics},
number = {1},
pages = {15},
pmid = {26187847},
publisher = {Human Genomics},
title = {{Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sudden cardiac arrest/sudden cardiac death}},
url = {http://www.humgenomics.com/content/9/1/15/abstract%5Cnhttp://www.humgenomics.com/content/9/1/15%5Cnhttp://www.humgenomics.com/content/pdf/s40246-015-0038-y.pdf},
volume = {9},
year = {2015}
}
@article{Pabinger2014,
abstract = {Recent advances in genome sequencing technologies provide unprecedented opportunities to characterize individual genomic landscapes and identify mutations relevant for diagnosis and therapy. Specifically, whole-exome sequencing using next-generation sequencing (NGS) technologies is gaining popularity in the human genetics community due to the moderate costs, manageable data amounts and straightforward interpretation of analysis results. While whole-exome and, in the near future, whole-genome sequencing are becoming commodities, data analysis still poses significant challenges and led to the development of a plethora of tools supporting specific parts of the analysis workflow or providing a complete solution. Here, we surveyed 205 tools for whole-genome/whole-exome sequencing data analysis supporting five distinct analytical steps: quality assessment, alignment, variant identification, variant annotation and visualization. We report an overview of the functionality, features and specific requirements of the individual tools. We then selected 32 programs for variant identification, variant annotation and visualization, which were subjected to hands-on evaluation using four data sets: one set of exome data from two patients with a rare disease for testing identification of germline mutations, two cancer data sets for testing variant callers for somatic mutations, copy number variations and structural variations, and one semi-synthetic data set for testing identification of copy number variations. Our comprehensive survey and evaluation of NGS tools provides a valuable guideline for human geneticists working on Mendelian disorders, complex diseases and cancers.},
archivePrefix = {arXiv},
arxivId = {209},
author = {Pabinger, Stephan and Dander, Andreas and Fischer, Maria and Snajder, Rene and Sperk, Michael and Efremova, Mirjana and Krabichler, Birgit and Speicher, Michael R. and Zschocke, Johannes and Trajanoski, Zlatko},
doi = {10.1093/bib/bbs086},
eprint = {209},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pabinger et al. - 2014 - A survey of tools for variant analysis of next-generation genome sequencing data.pdf:pdf},
isbn = {4351290037310},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics tools,Cancer,Mendelian disorders,Next-generation sequencing,Variants},
number = {2},
pages = {256--278},
pmid = {23341494},
title = {{A survey of tools for variant analysis of next-generation genome sequencing data}},
volume = {15},
year = {2014}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\n\nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\n\nAVAILABILITY: http://maq.sourceforge.net.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Fisch2015a,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics_pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Mckenna2010,
abstract = {The concept of absorptive capacity was introduced by Cohen and Levinthal in 1989. Since then it has been enhanced through reconceptualizations and extended by various empirical studies. Despite the growing interest in absorptive capacity it is unclear what this large stream of papers has collectively accomplished. The used definitions, antecedents, components and outcomes of the construct are extremely heterogeneous. Due to this heterogeneity, the empirical study of the construct remains difficult. There is no standard measure and no standard method of measurement, which can be used in empirical research. To bring more clarity into this research area, this paper provides a critical review of previous empirical treatments of absorptive capacity. For this purpose, different methods of measurement are classified in the following way: within quantitative methods proxy indicators and perceptive instruments are differentiated. Proxy indicators use single firm-level data for measuring absorptive capacity and can be input-oriented (R&D efforts, R&D human capital) or output-oriented (R&D patents, R&D publications). Perceptive instruments imply that researchers develop single questions or a set of questions, which reflect absorptive capacity or parts of it at the operational level. The main weakness of both proxy indicators and perceptive instruments is that they don't meet the complexity and emergence of the construct. Only few qualitative studies have started to adopt a new perspective, recognizing the process and practice-based character of absorptive capacity. In summary, the critical review prints out the necessity of advancing research in this area. For this reason, we set out to develop an alternative approach to capture absorptive capacity. It is a practice-oriented approach that allows studying actual absorptive practices in real world situations and enables researchers to capture the complex, embedded, and context-dependent patterns of acting.},
author = {Schmidt, Stephanie},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Proceedings of the International Conference on Intellectual Capital, Knowledge Management & Organizational Learning},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data.}},
volume = {20},
year = {2009}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics{_}pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M and Loguercio, Salvatore and Su, Andrew I},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Hwang2015,
abstract = {The success of clinical genomics using next generation sequencing (NGS) requires the accurate and consistent identification of personal genome variants. Assorted variant calling methods have been developed, which show low concordance between their calls. Hence, a systematic comparison of the variant callers could give important guidance to NGS-based clinical genomics. Recently, a set of high-confident variant calls for one individual (NA12878) has been published by the Genome in a Bottle (GIAB) consortium, enabling performance benchmarking of different variant calling pipelines. Based on the gold standard reference variant calls from GIAB, we compared the performance of thirteen variant calling pipelines, testing combinations of three read aligners-BWA-MEM, Bowtie2, and Novoalign-and four variant callers-Genome Analysis Tool Kit HaplotypeCaller (GATK-HC), Samtools mpileup, Freebayes and Ion Proton Variant Caller (TVC), for twelve data sets for the NA12878 genome sequenced by different platforms including Illumina2000, Illumina2500, and Ion Proton, with various exome capture systems and exome coverage. We observed different biases toward specific types of SNP genotyping errors by the different variant callers. The results of our study provide useful guidelines for reliable variant identification from deep sequencing of personal genomes.},
author = {Hwang, Sohyun and Kim, Eiru and Lee, Insuk and Marcotte, Edward M.},
doi = {10.1038/srep17875},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang et al. - 2015 - Systematic comparison of variant calling pipelines using gold standard personal exome variants.pdf:pdf},
isbn = {8010628980},
issn = {2045-2322},
journal = {Scientific Reports},
number = {December},
pages = {17875},
pmid = {26639839},
publisher = {Nature Publishing Group},
title = {{Systematic comparison of variant calling pipelines using gold standard personal exome variants}},
url = {http://www.nature.com/articles/srep17875%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26639839%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4671096},
volume = {5},
year = {2015}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.\n\nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.\n\nDATA SOURCES: Literature review and authors' experience.\n\nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M. and Durtschi, Jacob D. and Margraf, Rebecca L. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)\r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics_pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Handl2005,
author = {Handl, J. and Knowles, J. and Kell, D. B.},
doi = {10.1093/bioinformatics/bti517},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handl, Knowles, Kell - 2005 - Computational cluster validation in post-genomic data analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {3201--3212},
title = {{Computational cluster validation in post-genomic data analysis}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bti517},
volume = {21},
year = {2005}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192%7B&%7Dtool=pmcentrez%7B&%7Drendertype=abstract},
volume = {9},
year = {2014}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells, Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,Sequence Analysis, RNA,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192&tool=pmcentrez&rendertype=abstract},
volume = {9},
year = {2014}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/},
urldate = {2015-11-04}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey#0},
volume = {16},
year = {2004}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@misc{Paez2012,
abstract = {El contexto din{{\'{a}}}mico y competitivo de la organizaci{{\'{o}}}n actual exige permanentes soluciones inform{{\'{a}}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{{\'{o}}}n innovadora al problema del manejo de datos enmarcando dicha soluci{{\'{o}}}n mayormente, desde el punto de vista tecnol{{\'{o}}}gico, sin considerar los aspectos organizacionales y metodol{{\'{o}}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
issn = {0120-341X},
keywords = {Sistemas de informaci{{\'{o}}}n en administraci{{\'{o}}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{{\'{o}}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en%7B&%7DbtnG=Search%7B&%7Dq=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey%7B#%7D0},
volume = {16},
year = {2004}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
pages = {51--68},
title = {{Salud P{{\'{u}}}blica • Epidemiolog{{\'{i}}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{Staccini2014,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and {Vanden Berghe}, Wim and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - 2013 - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
number = {2},
pages = {3066--3076},
pmid = {24162173},
title = {{A primer to frequent itemset mining for bioinformatics.}},
url = {http://link.springer.com/10.1007/978-2-8178-0478-1_13%5Cnhttp://link.springer.com/chapter/10.1007/978-2-8178-0478-1_13 http://linkinghub.elsevier.com/retrieve/pii/S0957417408000195%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24162173},
volume = {36},
year = {2013}
}
@article{Parnell2014,
abstract = {BACKGROUND: Genetic understanding of complex traits has developed immensely over the past decade but remains hampered by incomplete descriptions of contribution to phenotypic variance. Gene-environment (GxE) interactions are one of these contributors and in the guise of diet and physical activity are important modulators of cardiometabolic phenotypes and ensuing diseases.\n\nRESULTS: We mined the scientific literature to collect GxE interactions from 386 publications for blood lipids, glycemic traits, obesity anthropometrics, vascular measures, inflammation and metabolic syndrome, and introduce CardioGxE, a gene-environment interaction resource. We then analyzed the genes and SNPs supporting cardiometabolic GxEs in order to demonstrate utility of GxE SNPs and to discern characteristics of these important genetic variants. We were able to draw many observations from our extensive analysis of GxEs. 1) The CardioGxE SNPs showed little overlap with variants identified by main effect GWAS, indicating the importance of environmental interactions with genetic factors on cardiometabolic traits. 2) These GxE SNPs were enriched in adaptation to climatic and geographical features, with implications on energy homeostasis and response to physical activity. 3) Comparison to gene networks responding to plasma cholesterol-lowering or regression of atherosclerotic plaques showed that GxE genes have a greater role in those responses, particularly through high-energy diets and fat intake, than do GWAS-identified genes for the same traits. Other aspects of the CardioGxE dataset were explored.\n\nCONCLUSIONS: Overall, we demonstrate that SNPs supporting cardiometabolic GxE interactions often exhibit transcriptional effects or are under positive selection. Still, not all such SNPs can be assigned potential functional or regulatory roles often because data are lacking in specific cell types or from treatments that approximate the environmental factor of the GxE. With research on metabolic related complex disease risk embarking on genome-wide GxE interaction tests, CardioGxE will be a useful resource.},
author = {Parnell, Laurence D and Blokker, Britt A and Dashti, Hassan S and Nesbeth, Paula-Dene and Cooper, Brittany Elle and Ma, Yiyi and Lee, Yu-Chi and Hou, Ruixue and Lai, Chao-Qiang and Richardson, Kris and Ordov{\'{a}}s, Jos{\'{e}} M},
doi = {10.1186/1756-0381-7-21},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnell et al. - 2014 - CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.pdf:pdf},
issn = {1756-0381},
journal = {BioData mining},
pages = {21},
pmid = {25368670},
title = {{CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4217104&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {2014}
}
@article{Pirooznia2014,
abstract = {BACKGROUND: The processing and analysis of the large scale data generated by next-generation sequencing (NGS) experiments is challenging and is a burgeoning area of new methods development. Several new bioinformatics tools have been developed for calling sequence variants from NGS data. Here, we validate the variant calling of these tools and compare their relative accuracy to determine which data processing pipeline is optimal. RESULTS: We developed a unified pipeline for processing NGS data that encompasses four modules: mapping, filtering, realignment and recalibration, and variant calling. We processed 130 subjects from an ongoing whole exome sequencing study through this pipeline. To evaluate the accuracy of each module, we conducted a series of comparisons between the single nucleotide variant (SNV) calls from the NGS data and either gold-standard Sanger sequencing on a total of 700 variants or array genotyping data on a total of 9,935 single-nucleotide polymorphisms. A head to head comparison showed that Genome Analysis Toolkit (GATK) provided more accurate calls than SAMtools (positive predictive value of 92.55% vs. 80.35%, respectively). Realignment of mapped reads and recalibration of base quality scores before SNV calling proved to be crucial to accurate variant calling. GATK HaplotypeCaller algorithm for variant calling outperformed the UnifiedGenotype algorithm. We also showed a relationship between mapping quality, read depth and allele balance, and SNV call accuracy. However, if best practices are used in data processing, then additional filtering based on these metrics provides little gains and accuracies of >99% are achievable. CONCLUSIONS: Our findings will help to determine the best approach for processing NGS data to confidently call variants for downstream analyses. To enable others to implement and replicate our results, all of our codes are freely available at http://metamoodics.org/wes.},
author = {Pirooznia, Mehdi and Kramer, Melissa and Parla, Jennifer and Goes, Fernando S and Potash, James B and McCombie, W Richard and Zandi, Peter P},
doi = {10.1186/1479-7364-8-14},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pirooznia et al. - 2014 - Validation and assessment of variant calling pipelines for next-generation sequencing.pdf:pdf},
isbn = {1479-7364 (Electronic)\r1473-9542 (Linking)},
issn = {1479-7364},
journal = {Human genomics},
keywords = {Bipolar Disorder,Bipolar Disorder: genetics,DNA,Data Interpretation,Exome,High-Throughput Nucleotide Sequencing,Humans,Polymorphism,Sequence Analysis,Single Nucleotide,Software,Statistical},
number = {1},
pages = {14},
pmid = {25078893},
title = {{Validation and assessment of variant calling pipelines for next-generation sequencing.}},
url = {http://www.humgenomics.com/content/8/1/14},
volume = {8},
year = {2014}
}
@article{Weitschek2014,
author = {Weitschek, Emanuel and Fiscon, Giulia and Felici, Giovanni},
doi = {10.1186/1756-0381-7-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitschek, Fiscon, Felici - 2014 - Supervised DNA Barcodes species classification analysis, comparisons and results.pdf:pdf},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {DNA Barcoding,Species identification,Supervised classification methods},
number = {1},
pages = {4},
publisher = {BioData Mining},
title = {{Supervised DNA Barcodes species classification: analysis, comparisons and results}},
url = {http://www.biodatamining.org/content/7/1/4},
volume = {7},
year = {2014}
}
@article{Mohammed2014,
abstract = {The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis. This so called “big data” challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields. Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability. It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structured/unstructured data. The MapReduce programming framework uses two tasks common in functional programming: Map and Reduce. MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters. Compared with existing parallel processing paradigms (e.g. grid computing and graphical processing unit (GPU)), MapReduce and Hadoop have two advantages: 1) fault-tolerant storage resulting in reliable data processing by replicating the computing tasks, and cloning the data chunks on different computing nodes across the computing cluster; 2) high-throughput data processing via a batch processing framework and the Hadoop distributed file system (HDFS). Data are stored in the HDFS and made available to the slave nodes for computation. In this paper, we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields. The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization, and opens up new opportunities in the emerging era of big data analytics. The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools. This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields.},
author = {Mohammed, Emad a. and Far, Behrouz H. and Naugler, Christopher},
doi = {10.1186/1756-0381-7-22},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammed, Far, Naugler - 2014 - Applications of the MapReduce programming framework to clinical big data analysis current landscape and.pdf:pdf},
isbn = {1756-0381 (Linking)},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {Big data,Bioinformatics,Clinical big data analysis,Clinical data analysis,Distributed programming,Hadoop,MapReduce},
number = {1},
pages = {1--23},
pmid = {25383096},
title = {{Applications of the MapReduce programming framework to clinical big data analysis: current landscape and future trends}},
url = {http://link.springer.com/article/10.1186/1756-0381-7-22%5Cnhttp://link.springer.com/content/pdf/10.1186/1756-0381-7-22.pdf},
volume = {7},
year = {2014}
}
@article{Bacardit2014,
abstract = {Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented.},
author = {Bacardit, Jaume and Widera, Pawe{\l} and Lazzarini, Nicola and Krasnogor, Natalio},
doi = {10.1089/big.2014.0023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacardit et al. - 2014 - Hard Data Analytics Problems Make for Better Data Analysis Algorithms Bioinformatics as an Example.pdf:pdf},
issn = {2167-6461},
journal = {Big data},
number = {3},
pages = {164--176},
pmid = {25276500},
title = {{Hard Data Analytics Problems Make for Better Data Analysis Algorithms: Bioinformatics as an Example.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4174911&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{Hannah-Shmouni2015,
author = {Hannah-Shmouni, Fady and Seidelmann, Sara B. and Sirrs, Sandra and Mani, Arya and Jacoby, Daniel},
doi = {10.1016/j.cjca.2015.07.735},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannah-Shmouni et al. - 2015 - The Genetic Challenges and Opportunities in Advanced Heart Failure.pdf:pdf},
issn = {0828282X},
journal = {Canadian Journal of Cardiology},
number = {11},
pages = {1338--1350},
publisher = {Elsevier Ltd},
title = {{The Genetic Challenges and Opportunities in Advanced Heart Failure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0828282X15013161},
volume = {31},
year = {2015}
}
@article{Maharjan2011,
author = {Maharjan, Merina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maharjan - 2011 - Genome Analysis with MapReduce.pdf:pdf},
pages = {1--23},
title = {{Genome Analysis with MapReduce}},
year = {2011}
}
@article{Kashyap2015,
abstract = {Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.},
archivePrefix = {arXiv},
arxivId = {1506.05101},
author = {Kashyap, Hirak and Ahmed, Hasin Afzal and Hoque, Nazrul and Roy, Swarup and Bhattacharyya, Dhruba Kumar},
eprint = {1506.05101},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kashyap et al. - 2014 - Big Data Analytics in Bioinformatics A Machine Learning Perspective.pdf:pdf},
journal = {Journal of Latex Class Files},
number = {9},
pages = {1--20},
title = {{Big Data Analytics in Bioinformatics: A Machine Learning Perspective}},
url = {http://arxiv.org/abs/1506.05101},
volume = {13},
year = {2014}
}
@article{Li2014,
abstract = {In ''Omics'' era of the life sciences, data is presented in many forms, which represent the information at various levels of bio-logical systems, including data about genome, transcriptome, epigenome, proteome, metabolome, molecular imaging, molec-ular pathways, different population of people and clinical/med-ical records. The biological data is big, and its scale has already been well beyond petabyte (PB) even exabyte (EB). Nobody doubts that the biological data will create huge amount of val-ues, if scientists can overcome many challenges, e.g., how to handle the complexity of information, how to integrate the data from very heterogeneous resources, what kind of principles or standards to be adopted when facing with the big data. Tools and techniques for analyzing big biological data enable us to translate massive amount of information into a better under-standing of the basic biomedical mechanisms, which can be fur-ther applied to translational or personalized medicine. Today, big data is one of the hottest topics in information science, but its concept can be misleading or confusing. The name itself suggests huge amount of data, which, however, represents only one aspect. In general, big data has four impor-tant features, so called four V's: volume of data, velocity of processing the data, variability of data sources, and veracity of the data quality. These four hallmarks of big data require to be characterized by special theory and technology; however, currently there is no satisfactory solution. Now, more biolo-gists are involved with the big data due to the rapid advance of high-throughput biotechnologies. As an example, the Human Genome Project utilized the expertise, infrastructure, and people from 20 institutions and took 13 years of work with over $3 billion to determine the whole genome structure of approximately three billion nucleotides. But now we can sequence a whole human genome for $1000 and within three days. We have spent decades struggling to collect enough bio-logical and biomedical data, but when big data overwhelms us, are we ready to face the challenge? The new bottleneck to this problem in biology is how to reveal the essential mechanisms of biological systems by understanding the big noisy data. Life sciences today need more robust, expressive, computable, quantitative, accurate and precise ways to handle the big data. As a matter of fact, recent works in this area have already brought remarkable advantage and opportunities, which implies the central roles of bioinformatics and bioinformati-cians in the future research of the biological and biomedical fields. In the following text, we describe several aspects of big biological data based on our recent studies.},
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {1672-0229},
journal = {Genomics, Proteomics \& Bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Data Mining,Data Mining: methods,Gene Expression Profiling,High-Throughput Screening Assays,Humans,Software},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China},
title = {{Big Biological Data: Challenges and Opportunities Expanding volume of the big biological data and its bonanza}},
url = {http://www.sciencedirect.com/science/article/pii/S1672022914001041},
volume = {12},
year = {2014}
}
@article{Biesecker2014,
abstract = {Sequencing of the genome or exome for clinical applications, hereafter referred to as clinical genome and exome sequencing (CGES), has now entered medical practice.1 Several thousand CGES tests have already been ordered for patients, with the goal of establishing diagnoses for rare, clinically unrecognizable, or puzzling disorders that are suspected to be genetic in origin. We anticipate increases in the use of CGES, the key attribute of which — its breadth — distinguishes it from other forms of laboratory testing. The interrogation of variation in about 20,000 genes simultaneously can be a powerful and effective diagnostic method.2 CGES has been hailed as an important tool in the implementation of predictive and individualized medicine, and there is intense research interest in the clinical benefits and risks of sequencing for screening healthy persons3; however, current practice recommendations4 do not support the use of sequencing for this purpose, and for that reason we do not further address it here. We have also limited this overview of CGES to the analysis of germline sequence variants for diagnostic purposes and do not discuss the use of CGES to uncover somatic variants in cancer in order to individualize cancer therapy. Clinicians should understand the diagnostic indications for CGES so that they can effectively deploy it in their practices. Because the success rate of CGES for the identification of a causative variant is approximately 25%,5 it is important to understand the basis of this testing and how to select the patients most likely to benefit from it. Here, we summarize the technologies underlying CGES and offer our insights into how clinicians should order such testing, interpret the results, and communicate the results to their patients (an interactive graphic giving an overview of the process is available with the full text of this article at NEJM.org).},
author = {Biesecker, Leslie G. and Green, Robert C.},
doi = {10.1056/NEJMra1312543},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesecker, Green - 2014 - Diagnostic Clinical Genome and Exome Sequencing.pdf:pdf},
isbn = {1533-4406 (Electronic)\r0028-4793 (Linking)},
issn = {1533-4406},
journal = {New England Journal of Medicine},
keywords = {Exome,Genetic Counseling,Genetic Diseases, Inborn,Genetic Diseases, Inborn: diagnosis,Genetic Diseases, Inborn: genetics,Genetic Testing,Genome, Human,Humans,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {25},
pages = {2418--2425},
pmid = {24941179},
title = {{Diagnostic Clinical Genome and Exome Sequencing}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMra1312543},
volume = {370},
year = {2014}
}
@article{Bajcsy2005,
abstract = {Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.},
author = {Bajcsy, Peter and Han, Jiawei and Liu, Lei and Yang, Jiong},
doi = {10.1007/1-84628-059-1_2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bajcsy et al. - 2005 - Survey of Biodata Analysis from a Data Mining Perspective.pdf:pdf},
isbn = {1852336714},
journal = {Data Mining in Bioinformatics},
pages = {9--39},
title = {{Survey of Biodata Analysis from a Data Mining Perspective}},
url = {http://dx.doi.org/10.1007/1-84628-059-1_2},
year = {2005}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
doi = {10.1007/s13398-014-0173-7.2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cossio et al. - 2012 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Uma {\'{e}}tica para quantos?},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics & numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics & numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,epidural,esth{\'{e}}tique,est{\'{e}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,needle through a,nes corporales,perforaci{\'{o}}n corporal,piel,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{No Title No Title}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233&partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@article{Cheng2015,
author = {Cheng, Phil and Levesque, Mitch and Cheng, Phil F and Dummer, Reinhard and Levesque, Mitch P},
doi = {10.4414/smw.2015.14183},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2015 - Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in.pdf:pdf},
keywords = {cancer are somatic mutation,copy number,data mining,data mining, Genomics, transcriptomics, Cancer Gen,gene expres-,genomics,on the tcga data,portal for each,the cancer genome atlas,the data types listed,transcriptomics},
number = {October},
pages = {1--5},
title = {{Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in the era of precision cancer medicine}},
year = {2015}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for molecular biology : AMB},
pages = {4},
pmid = {17428327},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
volume = {2},
year = {2007}
}
@article{Kumari2014,
author = {Kumari, D Aruna and Bhavana, D Poojitha and Aditya, V Venkata Sai},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumari, Bhavana, Aditya - 2014 - Data Mining in Biodata Analysis.pdf:pdf},
number = {9},
pages = {1--3},
title = {{Data Mining in Biodata Analysis}},
volume = {14},
year = {2014}
}
@article{Systems2009,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - FASTQ format and data quality.pdf:pdf},
title = {{FASTQ format and data quality}},
year = {2009}
}
@article{Systems2009a,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - Structural variation and Medical Genomics.pdf:pdf},
title = {{Structural variation and Medical Genomics}},
year = {2009}
}
@article{Canuel2015,
author = {Canuel, V. and Rance, B. and Avillach, P. and Degoulet, P. and Burgun, A.},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data a review of publicly available solutions.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {biomedical research,clinical data,high-throughput technologies,information storage,translational medical research},
number = {2},
pages = {280--290},
title = {{Translational research platforms integrating clinical and omics data: a review of publicly available solutions}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu006},
volume = {16},
year = {2015}
}
@article{Dudley2010,
abstract = {With the continued exponential expansion of publicly available genomic data and access to low-cost, high-throughput molecular technologies for profiling patient populations, computational technologies and informatics are becoming vital considerations in genomic medicine. Although cloud computing technology is being heralded as a key enabling technology for the future of genomic research, available case studies are limited to applications in the domain of high-throughput sequence data analysis. The goal of this study was to evaluate the computational and economic characteristics of cloud computing in performing a large-scale data integration and analysis representative of research problems in genomic medicine. We find that the cloud-based analysis compares favorably in both performance and cost in comparison to a local computational cluster, suggesting that cloud computing technologies might be a viable resource for facilitating large-scale translational research in genomic medicine.},
author = {Dudley, Joel T and Pouliot, Yannick and Chen, Rong and Morgan, Alexander A and Butte, Atul J},
doi = {10.1186/gm172},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudley et al. - 2010 - Translational bioinformatics in the cloud an affordable alternative.pdf:pdf},
isbn = {1756-994X (Electronic)},
issn = {1756-994X},
journal = {Genome medicine},
number = {8},
pages = {51},
pmid = {20691073},
title = {{Translational bioinformatics in the cloud: an affordable alternative.}},
url = {http://genomemedicine.com/content/2/8/51},
volume = {2},
year = {2010}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Del et al. - 2014 - Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology.pdf:pdf},
pages = {51--68},
title = {{Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@misc{Paez2012,
abstract = {El contexto din{\'{a}}mico y competitivo de la organizaci{\'{o}}n actual exige permanentes soluciones inform{\'{a}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\'{o}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\'{o}}n mayormente, desde el punto de vista tecnol{\'{o}}gico, sin considerar los aspectos organizacionales y metodol{\'{o}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{a}}ez - 2012 - Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio.pdf:pdf},
issn = {0120-341X},
keywords = {Sistemas de informaci{\'{o}}n en administraci{\'{o}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Liu2014,
abstract = {Discriminative pattern mining is one of the most important techniques in data mining. This challenging task is concerned with finding a set of patterns that occur with disproportionate frequency in data sets with various class labels. Such patterns are of great value for group difference detection and classifier construction. Research on finding interesting discriminative patterns in class-labeled data evolves rapidly and lots of algorithms have been proposed to specifically address this problem. Discriminative pattern mining techniques have proven their considerable value in biological data analysis. The archetypical applications in bioinformatics include phosphorylation motif discovery, differentially expressed gene identification, discriminative genotype pattern detection, etc. In this article, we present an overview of discriminative pattern mining and the corresponding effective methods, and subsequently we illustrate their applications to tackling the bioinformatics problems. In the end, we give a general discussion of potential challenges and future work for this task.},
author = {Liu, Xiaoqing and Wu, Jun and Gu, Feiyang and Wang, Jie and He, Zengyou},
doi = {10.1093/bib/bbu042},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Discriminative pattern mining and its applications in bioinformatics.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {contrast sets,discriminative pattern mining,emerging patterns,subgroup discovery},
number = {October},
pages = {1--17},
title = {{Discriminative pattern mining and its applications in bioinformatics}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu042},
year = {2014}
}
@article{Naulaerts,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solu-tions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address vari-ations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinfor-matics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future poten-tial and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - Unknown - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
title = {{A primer to frequent itemset mining for bioinformatics}}
}
