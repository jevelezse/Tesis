Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Dewey2016,
abstract = {The DiscovEHR collaboration between the Regeneron Genetics Center and Geisinger Health System couples high-throughput sequencing to an integrated health care system using longitudinal electronic health records (EHRs).We sequenced the exomes of 50,726 adult participants in the DiscovEHR study to identify $\sim$4.2 million rare single-nucleotide variants and insertion/deletion events, of which $\sim$176,000 are predicted to result in a loss of gene function. Linking these data to EHR-derived clinical phenotypes, we find clinical associations supporting therapeutic targets, including genes encoding drug targets for lipid lowering, and identify previously unidentified rare alleles associated with lipid levels and other blood level traits. About 3.5% of individuals harbor deleterious variants in 76 clinically actionable genes. The DiscovEHR data set provides a blueprint for large-scale precision},
author = {Dewey, Frederick E. and Murray, Michael F. and Overton, John D. and Habegger, Lukas and Leader, Joseph B. and Fetterolf, Samantha N. and O'Dushlaine, Colm and {Van Hout}, Cristopher V. and Staples, Jeffrey and Gonzaga-Jauregui, Claudia and Metpally, Raghu and Pendergrass, Sarah A. and Giovanni, Monica A. and Kirchner, H. Lester and Balasubramanian, Suganthi and Abul-Husn, Noura S. and Hartzel, Dustin N. and Lavage, Daniel R. and Kost, Korey A. and Packer, Jonathan S. and Lopez, Alexander E. and Penn, John and Mukherjee, Semanti and Gosalia, Nehal and Kanagaraj, Manoj and Li, Alexander H. and Mitnaul, Lyndon J. and Adams, Lance J. and Person, Thomas N. and Praveen, Kavita and Marcketta, Anthony and Lebo, Matthew S. and Austin-Tse, Christina A. and Mason-Suares, Heather M. and Bruse, Shannon and Mellis, Scott and Phillips, Robert and Stahl, Neil and Murphy, Andrew and Economides, Aris and Skelding, Kimberly A. and Still, Christopher D. and Elmore, James R. and Borecki, Ingrid B. and Yancopoulos, George D. and Davis, F. Daniel and Faucett, William A. and Gottesman, Omri and Ritchie, Marylyn D. and Shuldiner, Alan R. and Reid, Jeffrey G. and Ledbetter, David H. and Baras, Aris and Carey, David J.},
doi = {10.1126/science.aaf6814},
file = {:home/jennifer/Descargas/dewey2016.pdf:pdf},
isbn = {10.1126/science.aaf6814},
issn = {10959203},
journal = {Science},
number = {6319},
pmid = {28008009},
title = {{Distribution and clinical impact of functional variants in 50,726 whole-exome sequences from the DiscovEHR study}},
volume = {354},
year = {2016}
}
@article{Crisan2012,
abstract = {Next generation sequencing has now enabled a cost-effective enumeration of the full mutational complement of a tumor genome-in particular single nucleotide variants (SNVs). Most current computational and statistical models for analyzing next generation sequencing data, however, do not account for cancer-specific biological properties, including somatic segmental copy number alterations (CNAs)-which require special treatment of the data. Here we present CoNAn-SNV (Copy Number Annotated SNV): a novel algorithm for the inference of single nucleotide variants (SNVs) that overlap copy number alterations. The method is based on modelling the notion that genomic regions of segmental duplication and amplification induce an extended genotype space where a subset of genotypes will exhibit heavily skewed allelic distributions in SNVs (and therefore render them undetectable by methods that assume diploidy). We introduce the concept of modelling allelic counts from sequencing data using a panel of Binomial mixture models where the number of mixtures for a given locus in the genome is informed by a discrete copy number state given as input. We applied CoNAn-SNV to a previously published whole genome shotgun data set obtained from a lobular breast cancer and show that it is able to discover 21 experimentally revalidated somatic non-synonymous mutations in a lobular breast cancer genome that were not detected using copy number insensitive SNV detection algorithms. Importantly, ROC analysis shows that the increased sensitivity of CoNAn-SNV does not result in disproportionate loss of specificity. This was also supported by analysis of a recently published lymphoma genome with a relatively quiescent karyotype, where CoNAn-SNV showed similar results to other callers except in regions of copy number gain where increased sensitivity was conferred. Our results indicate that in genomically unstable tumors, copy number annotation for SNV detection will be critical to fully characterize the mutational landscape of cancer genomes.},
author = {Crisan, Anamaria and Goya, Rodrigo and Ha, Gavin and Ding, Jiarui and Prentice, Leah M and Oloumi, Arusha and Senz, Janine and Zeng, Thomas and Tse, Kane and Delaney, Allen and Marra, Marco A and Huntsman, David G and Hirst, Martin and Aparicio, Sam and Shah, Sohrab},
doi = {10.1371/journal.pone.0041551},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Cancer,DNA Copy Number Variations,Genes,Genetic,Genome,Humans,Models,Mutation,NGS,Neoplasm,Neoplasms,Neoplasms: genetics,SNP,Software},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {jan},
number = {8},
pages = {e41551},
pmid = {22916110},
title = {{Mutation discovery in regions of segmental cancer genome amplifications with CoNAn-SNV: a mixture model for next generation sequencing of tumors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3420914&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {2012}
}
@article{Firpi2010,
abstract = {Recent large-scale chromatin states mapping efforts have revealed characteristic chromatin modification signatures for various types of functional DNA elements. Given the important influence of chromatin states on gene regulation and the rapid accumulation of genome-wide chromatin modification data, there is a pressing need for computational methods to analyze these data in order to identify functional DNA elements. However, existing computational tools do not exploit data transformation and feature extraction as a means to achieve a more accurate prediction.},
author = {Firpi, Hiram a and Ucar, Duygu and Tan, Kai},
doi = {10.1093/bioinformatics/btq248},
file = {::;::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Chromatin,Chromatin: chemistry,Gene Expression Regulation,HeLa Cells,Histone Code,Histones,Histones: metabolism,Humans,Neural Networks (Computer),Nucleic Acid,Regulatory Sequences},
month = {jul},
number = {13},
pages = {1579--86},
pmid = {20453004},
title = {{Discover regulatory DNA elements using chromatin signatures and artificial neural network.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2887052&tool=pmcentrez&rendertype=abstract},
volume = {26},
year = {2010}
}
@article{Roth2012,
abstract = {MOTIVATION: Identification of somatic single nucleotide variants (SNVs) in tumour genomes is a necessary step in defining the mutational landscapes of cancers. Experimental designs for genome-wide ascertainment of somatic mutations now routinely include next-generation sequencing (NGS) of tumour DNA and matched constitutional DNA from the same individual. This allows investigators to control for germline polymorphisms and distinguish somatic mutations that are unique to the tumour, thus reducing the burden of labour-intensive and expensive downstream experiments needed to verify initial predictions. In order to make full use of such paired datasets, computational tools for simultaneous analysis of tumour-normal paired sequence data are required, but are currently under-developed and under-represented in the bioinformatics literature. RESULTS: In this contribution, we introduce two novel probabilistic graphical models called JointSNVMix1 and JointSNVMix2 for jointly analysing paired tumour-normal digital allelic count data from NGS experiments. In contrast to independent analysis of the tumour and normal data, our method allows statistical strength to be borrowed across the samples and therefore amplifies the statistical power to identify and distinguish both germline and somatic events in a unified probabilistic framework. AVAILABILITY: The JointSNVMix models and four other models discussed in the article are part of the JointSNVMix software package available for download at http://compbio.bccrc.ca CONTACT: sshah@bccrc.ca SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Roth, Andrew and Ding, Jiarui and Morin, Ryan and Crisan, Anamaria and Ha, Gavin and Giuliany, Ryan and Bashashati, Ali and Hirst, Martin and Turashvili, Gulisa and Oloumi, Arusha and Marra, Marco A and Aparicio, Samuel and Shah, Sohrab P},
doi = {10.1093/bioinformatics/bts053},
file = {::;::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Cancer Bioinformatics,Computational Biology,Computational Biology: methods,DNA,DNA Mutational Analysis,DNA Mutational Analysis: methods,Humans,Models,Mutation,NGS,Neoplasm,Neoplasm: genetics,Neoplasms,Neoplasms: genetics,SNP,SNV,Software,Statistical},
mendeley-tags = {Cancer Bioinformatics,NGS,SNP,SNV,Software},
month = {apr},
number = {7},
pages = {907--13},
pmid = {22285562},
title = {{JointSNVMix: a probabilistic model for accurate detection of somatic mutations in normal/tumour paired next-generation sequencing data.}},
url = {http://bioinformatics.oxfordjournals.org/content/28/7/907 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3315723&tool=pmcentrez&rendertype=abstract},
volume = {28},
year = {2012}
}
@article{Schroeder2013,
abstract = {ABSTRACT: Cancer genomics projects employ high-throughput technologies to identify the complete catalog of somatic alterations that characterize the genome, transcriptome and epigenome of cohorts of tumor samples. Examples include projects carried out by the International Cancer Genome Consortium (ICGC) and The Cancer Genome Atlas (TCGA). A crucial step in the extraction of knowledge from the data is the exploration by experts of the different alterations, as well as the multiple relationships between them. To that end, the use of intuitive visualization tools that can integrate different types of alterations with clinical data is essential to the field of cancer genomics. Here, we review effective and common visualization techniques for exploring oncogenomics data and discuss a selection of tools that allow researchers to effectively visualize multidimensional oncogenomics datasets. The review covers visualization methods employed by tools such as Circos, Gitools, the Integrative Genomics Viewer, Cytoscape, Savant Genome Browser, StratomeX and platforms such as cBio Cancer Genomics Portal, IntOGen, the UCSC Cancer Genomics Browser, the Regulome Explorer and the Cancer Genome Workbench.},
author = {Schroeder, Michael P and Gonzalez-Perez, Abel and Lopez-Bigas, Nuria},
doi = {10.1186/gm413},
file = {::},
issn = {1756-994X},
journal = {Genome medicine},
keywords = {Cancer Genomics,Visualization},
mendeley-tags = {Cancer Genomics,Visualization},
month = {jan},
number = {1},
pages = {9},
pmid = {23363777},
title = {{Visualizing multidimensional cancer genomics data.}},
url = {http://genomemedicine.com/content/5/1/9 http://www.ncbi.nlm.nih.gov/pubmed/23363777},
volume = {5},
year = {2013}
}
@article{Chen2013a,
abstract = {BACKGROUND: Genome-wide association studies (GWAS) have identified many common polymorphisms associated with complex traits. However, these associated common variants explain only a small fraction of the phenotypic variances, leaving a substantial portion of genetic heritability unexplained. As a result, searches for "missing" heritability are drawing increasing attention, particularly for rare variant studies that often require a large sample size and, thus, extensive sequencing effort. Although the development of next generation sequencing (NGS) technologies has made it possible to sequence a large number of reads economically and efficiently, it is still often cost prohibitive to sequence thousands of individuals that are generally required for association studies. A more efficient and cost-effective design would involve pooling the genetic materials of multiple individuals together and then sequencing the pools, instead of the individuals. This pooled sequencing approach has improved the plausibility of association studies for rare variants, while, at the same time, posed a great challenge to the pooled sequencing data analysis, essentially because individual sample identity is lost, and NGS sequencing errors could be hard to distinguish from low frequency alleles. RESULTS: A unified approach for estimating minor allele frequency, SNP calling and association studies based on pooled sequencing data using an expectation maximization (EM) algorithm is developed in this paper. This approach makes it possible to study the effects of minor allele frequency, sequencing error rate, number of pools, number of individuals in each pool, and the sequencing depth on the estimation accuracy of minor allele frequencies. We show that the naive method of estimating minor allele frequencies by taking the fraction of observed minor alleles can be significantly biased, especially for rare variants. In contrast, our EM approach can give an unbiased estimate of the minor allele frequency under all scenarios studied in this paper. A SNP calling approach, EM-SNP, for pooled sequencing data based on the EM algorithm is then developed and compared with another recent SNP calling method, SNVer. We show that EM-SNP outperforms SNVer in terms of the fraction of db-SNPs among the called SNPs, as well as transition/transversion (Ti/Tv) ratio. Finally, the EM approach is used to study the association between variants and type I diabetes. CONCLUSIONS: The EM-based approach for the analysis of pooled sequencing data can accurately estimate minor allele frequencies, call SNPs, and find associations between variants and complex traits. This approach is especially useful for studies involving rare variants.},
author = {Chen, Quan and Sun, Fengzhu},
doi = {10.1186/1471-2164-14-S1-S1},
file = {::},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Cancer,Databases,Diabetes Mellitus,Gene Frequency,Genetic,Genome-Wide Association Study,Genotype,High-Throughput Nucleotide Sequencing,Humans,Internet,NGS,Polymorphism,SNP,Single Nucleotide,Software,Type 1,Type 1: genetics},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {jan},
pages = {S1},
pmid = {23369070},
title = {{A unified approach for allele frequency estimation, SNP detection and association studies based on pooled sequencing data using EM algorithms.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3549804&tool=pmcentrez&rendertype=abstract},
volume = {14 Suppl 1},
year = {2013}
}
@article{Christoforides2013,
abstract = {BACKGROUND: The field of cancer genomics has rapidly adopted next-generation sequencing (NGS) in order to study and characterize malignant tumors with unprecedented resolution. In particular for cancer, one is often trying to identify somatic mutations--changes specific to a tumor and not within an individual's germline. However, false positive and false negative detections often result from lack of sufficient variant evidence, contamination of the biopsy by stromal tissue, sequencing errors, and the erroneous classification of germline variation as tumor-specific. RESULTS: We have developed a generalized Bayesian analysis framework for matched tumor/normal samples with the purpose of identifying tumor-specific alterations such as single nucleotide mutations, small insertions/deletions, and structural variation. We describe our methodology, and discuss its application to other types of paired-tissue analysis such as the detection of loss of heterozygosity as well as allelic imbalance. We also demonstrate the high level of sensitivity and specificity in discovering simulated somatic mutations, for various combinations of a) genomic coverage and b) emulated heterogeneity. CONCLUSION: We present a Java-based implementation of our methods named Seurat, which is made available for free academic use. We have demonstrated and reported on the discovery of different types of somatic change by applying Seurat to an experimentally-derived cancer dataset using our methods; and have discussed considerations and practices regarding the accurate detection of somatic events in cancer genomes. Seurat is available at https://sites.google.com/site/seuratsomatic.},
author = {Christoforides, Alexis and Carpten, John D and Weiss, Glen J and Demeure, Michael J and {Von Hoff}, Daniel D and Craig, David W},
doi = {10.1186/1471-2164-14-302},
file = {::},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Bayes Theorem,Biological,Cancer,Genomics,Genomics: methods,Humans,Models,Mutation,NGS,Neoplasms,Neoplasms: genetics,SNP,Software},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {jan},
number = {1},
pages = {302},
pmid = {23642077},
title = {{Identification of somatic mutations in cancer through Bayesian-based analysis of sequenced genome pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23642077 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3751438&tool=pmcentrez&rendertype=abstract},
volume = {14},
year = {2013}
}
@article{Yost2013,
abstract = {SUMMARY: We present Mutascope, a sequencing analysis pipeline specifically developed for the identification of somatic variants present at low-allelic fraction from high-throughput sequencing of amplicons from matched tumor-normal specimen. Using datasets reproducing tumor genetic heterogeneity, we demonstrate that Mutascope has a higher sensitivity and generates fewer false-positive calls than tools designed for shotgun sequencing or diploid genomes.

AVAILABILITY: Freely available on the web at http://sourceforge.net/projects/mutascope/.

CONTACT: oharismendy@ucsd.edu

SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Yost, Shawn E and Alakus, Hakan and Matsui, Hiroko and Schwab, Richard B and Jepsen, Kristen and Frazer, Kelly A and Harismendy, Olivier},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {DNA Mutational Analysis,DNA Mutational Analysis: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Humans,Internet,Mutation,Neoplasms,Neoplasms: genetics,Software},
month = {aug},
number = {15},
pages = {1908--9},
title = {{Mutascope: sensitive detection of somatic mutations from deep amplicon sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3712217&tool=pmcentrez&rendertype=abstract},
volume = {29},
year = {2013}
}
@article{Ding2011,
abstract = {MOTIVATION: The study of cancer genomes now routinely involves using next-generation sequencing technology (NGS) to profile tumours for single nucleotide variant (SNV) somatic mutations. However, surprisingly few published bioinformatics methods exist for the specific purpose of identifying somatic mutations from NGS data and existing tools are often inaccurate, yielding intolerably high false prediction rates. As such, the computational problem of accurately inferring somatic mutations from paired tumour/normal NGS data remains an unsolved challenge. RESULTS: We present the comparison of four standard supervised machine learning algorithms for the purpose of somatic SNV prediction in tumour/normal NGS experiments. To evaluate these approaches (random forest, Bayesian additive regression tree, support vector machine and logistic regression), we constructed 106 features representing 3369 candidate somatic SNVs from 48 breast cancer genomes, originally predicted with naive methods and subsequently revalidated to establish ground truth labels. We trained the classifiers on this data (consisting of 1015 true somatic mutations and 2354 non-somatic mutation positions) and conducted a rigorous evaluation of these methods using a cross-validation framework and hold-out test NGS data from both exome capture and whole genome shotgun platforms. All learning algorithms employing predictive discriminative approaches with feature selection improved the predictive accuracy over standard approaches by statistically significant margins. In addition, using unsupervised clustering of the ground truth 'false positive' predictions, we noted several distinct classes and present evidence suggesting non-overlapping sources of technical artefacts illuminating important directions for future study. AVAILABILITY: Software called MutationSeq and datasets are available from http://compbio.bccrc.ca.},
author = {Ding, Jiarui and Bashashati, Ali and Roth, Andrew and Oloumi, Arusha and Tse, Kane and Zeng, Thomas and Haffari, Gholamreza and Hirst, Martin and Marra, Marco A and Condon, Anne and Aparicio, Samuel and Shah, Sohrab P},
doi = {10.1093/bioinformatics/btr629},
file = {::;::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Breast Neoplasms,Breast Neoplasms: genetics,Cluster Analysis,Exome,Female,Genetic,Genome,Humans,Models,Mutation,NGS data analysis,Neoplasms,Polymorphism,SNV calling,Single Nucleotide,Software,Support Vector Machines},
mendeley-tags = {NGS data analysis,SNV calling},
month = {jan},
number = {2},
pages = {167--75},
pmid = {22084253},
title = {{Feature-based classifiers for somatic mutation detection in tumour-normal paired sequencing data.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btr629v1 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3259434&tool=pmcentrez&rendertype=abstract},
volume = {28},
year = {2012}
}
@article{Abo2014,
author = {Abo, Ryan P and Ducar, Matthew and Garcia, Elizabeth P and Thorner, Aaron R and Rojas-Rudilla, Vanesa and Lin, Ling and Sholl, Lynette M and Hahn, William C and Meyerson, Matthew and Lindeman, Neal I and {Van Hummelen}, Paul and MacConaill, Laura E},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Fusions,NGS,SV,Software},
mendeley-tags = {Fusions,NGS,SV,Software},
number = {3},
title = {{BreaKmer: detection of structural variation in targeted massively parallel sequencing data using kmers.}},
volume = {43},
year = {2014}
}
@article{Wang2013b,
abstract = {BACKGROUND: Driven by high throughput next generation sequencing technologies and the pressing need to decipher cancer genomes, computational approaches for detecting somatic single nucleotide variants (sSNVs) have undergone dramatic improvements during the past two years. The recently developed tools typically compare a tumor sample directly with a matched normal sample at each variant locus in order to increase the accuracy of sSNV calling. These programs also address the detection of sSNVs at low allele frequencies, allowing for the study of tumor heterogeneity, cancer subclones, and mutation evolution in cancer development.

METHODS: We used whole genome sequencing (Illumina Genome Analyzer IIx platform) of a melanoma sample and matched blood, whole exome sequencing (Illumina HiSeq 2000 platform) of 18 lung tumor-normal pairs and 7 lung cancer cell lines to evaluate 6 tools for sSNV detection: EBCall, JointSNVMix, MuTect, SomaticSniper, Strelka, and VarScan 2, with a focus on MuTect and VarScan 2, two widely used publicly available software tools. Default/suggested parameters were used to run these tools. The missense sSNVs detected in these samples were validated through PCR and direct sequencing of genomic DNA from the samples. We also simulated 10 tumor-normal pairs to explore the ability of these programs to detect low allelic-frequency sSNVs.

RESULTS: Out of the 237 sSNVs successfully validated in our cancer samples, VarScan 2 and MuTect detected the most of any tools, i.e. 204 and 192, respectively. MuTect identified 11 more low-coverage validated sSNVs than VarScan 2, but missed 11 more sSNVs with alternate alleles in normal samples than VarScan 2. When examining the false calls of each tool using 169 invalidated sSNVs, we observed >63% false calls detected in the lung cancer cell lines had alternate alleles in normal samples. Additionally, from our simulation data, VarScan 2 identified more sSNVs than other tools, while MuTect characterized most low allelic-fraction sSNVs.

CONCLUSIONS: Our study explored the typical false positive and false negative detections that arise from the use of sSNV-calling tools. Our results suggest that despite recent progress, these tools have significant room for improvement, especially in the discrimination of low coverage/allelic-frequency sSNVs and sSNVs with alternate alleles in normal samples.},
author = {Wang, Qingguo and Jia, Peilin and Li, Fei and Chen, Haiquan and Ji, Hongbin and Hucks, Donald and Dahlman, Kimberly Brown and Pao, William and Zhao, Zhongming},
file = {::},
issn = {1756-994X},
journal = {Genome medicine},
month = {oct},
number = {10},
pages = {91},
title = {{Detecting somatic point mutations in cancer genome sequencing data: a comparison of mutation callers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24112718},
volume = {5},
year = {2013}
}
@article{Larson2012,
abstract = {MOTIVATION: The sequencing of tumors and their matched normals is frequently used to study the genetic composition of cancer. Despite this fact, there remains a dearth of available software tools designed to compare sequences in pairs of samples and identify sites that are likely to be unique to one sample. RESULTS: In this article, we describe the mathematical basis of our SomaticSniper software for comparing tumor and normal pairs. We estimate its sensitivity and precision, and present several common sources of error resulting in miscalls. AVAILABILITY AND IMPLEMENTATION: Binaries are freely available for download at http://gmt.genome.wustl.edu/somatic-sniper/current/, implemented in C and supported on Linux and Mac OS X. CONTACT: delarson@wustl.edu; lding@wustl.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Larson, David E and Harris, Christopher C and Chen, Ken and Koboldt, Daniel C and Abbott, Travis E and Dooling, David J and Ley, Timothy J and Mardis, Elaine R and Wilson, Richard K and Ding, Li},
doi = {10.1093/bioinformatics/btr665},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Genome,Human,Humans,Neoplasms,Neoplasms: genetics,Point Mutation,Polymorphism,Single Nucleotide,Software},
month = {feb},
number = {3},
pages = {311--7},
pmid = {22155872},
title = {{SomaticSniper: identification of somatic point mutations in whole genome sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22155872 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3268238&tool=pmcentrez&rendertype=abstract},
volume = {28},
year = {2012}
}
@article{Dienstmann2015,
abstract = {SUMMARY: Comprehensive genomic profiling is expected to revolutionize cancer therapy. In this Prospective, we present the prevalence of mutations and copy-number alterations with predictive associations across solid tumors at different levels of stringency for gene-drug targetability. More than 90% of The Cancer Genome Atlas samples have potentially targetable alterations, the majority with multiple events, illustrating the challenges for treatment prioritization given the complexity of the genomic landscape. Nearly 80% of the variants in rarely mutated oncogenes are of uncertain functional significance, reflecting the gap in our understanding of the relevance of many alterations potentially linked to therapeutic actions. Access to targeted agents in early clinical trials could affect treatment decision in 75% of patients with cancer. Prospective implementation of large-scale molecular profiling and standardized reports of predictive biomarkers are fundamental steps for making precision cancer medicine a reality. Cancer Discov; 5(2); 118-23. {\textcopyright}2015 AACR.},
author = {Dienstmann, R. and Jang, I. S. and Bot, B. and Friend, S. and Guinney, J.},
doi = {10.1158/2159-8290.CD-14-1118},
issn = {2159-8274},
journal = {Cancer Discovery},
keywords = {Actionability,Biomarkers,Cancer,Database},
language = {en},
mendeley-tags = {Actionability,Biomarkers,Cancer,Database},
month = {feb},
number = {2},
pages = {118--123},
pmid = {25656898},
publisher = {American Association for Cancer Research},
title = {{Database of Genomic Biomarkers for Cancer Drugs and Clinical Targetability in Solid Tumors}},
url = {http://cancerdiscovery.aacrjournals.org/content/5/2/118.full},
volume = {5},
year = {2015}
}
@article{Song2012,
abstract = {Tumour cellularity, the relative proportion of tumour and normal cells in a sample, affects the sensitivity of mutation detection, copy number analysis, cancer gene expression and methylation profiling. Tumour cellularity is traditionally estimated by pathological review of sectioned specimens; however this method is both subjective and prone to error due to heterogeneity within lesions and cellularity differences between the sample viewed during pathological review and tissue used for research purposes. In this paper we describe a statistical model to estimate tumour cellularity from SNP array profiles of paired tumour and normal samples using shifts in SNP allele frequency at regions of loss of heterozygosity (LOH) in the tumour. We also provide qpure, a software implementation of the method. Our experiments showed that there is a medium correlation 0.42 ([Formula: see text]-value=0.0001) between tumor cellularity estimated by qpure and pathology review. Interestingly there is a high correlation 0.87 ([Formula: see text]-value [Formula: see text] 2.2e-16) between cellularity estimates by qpure and deep Ion Torrent sequencing of known somatic KRAS mutations; and a weaker correlation 0.32 ([Formula: see text]-value=0.004) between IonTorrent sequencing and pathology review. This suggests that qpure may be a more accurate predictor of tumour cellularity than pathology review. qpure can be downloaded from https://sourceforge.net/projects/qpure/.},
author = {Song, Sarah and Nones, Katia and Miller, David and Harliwong, Ivon and Kassahn, Karin S. and Pinese, Mark and Pajic, Marina and Gill, Anthony J. and Johns, Amber L. and Anderson, Matthew and Holmes, Oliver and Leonard, Conrad and Taylor, Darrin and Wood, Scott and Xu, Qinying and Newell, Felicity and Cowley, Mark J. and Wu, Jianmin and Wilson, Peter and Fink, Lynn and Biankin, Andrew V. and Waddell, Nic and Grimmond, Sean M. and Pearson, John V.},
doi = {10.1371/journal.pone.0045835},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Cell Line,Computational Biology,Computational Biology: methods,DNA,Exons,Gene Expression Regulation,Gene Frequency,Genetic,Genome-Wide Association Study,Humans,Loss of Heterozygosity,Models,Mutation,Pancreatic Neoplasms,Pancreatic Neoplasms: genetics,Pancreatic Neoplasms: metabolism,Polymorphism,Regression Analysis,Sequence Analysis,Single Nucleotide,Software,Statistical,Tumor},
month = {jan},
number = {9},
pages = {e45835},
pmid = {23049875},
title = {{qpure: A tool to estimate tumor cellularity from genome-wide single-nucleotide polymorphism profiles.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3457972&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {2012}
}
@article{Gonzalez-Perez2012,
abstract = {Identifying cancer driver genes and pathways among all somatic mutations detected in a cohort of tumors is a key challenge in cancer genomics. Traditionally, this is done by prioritizing genes according to the recurrence of alterations that they bear. However, this approach has some known limitations, such as the difficulty to correctly estimate the background mutation rate, and the fact that it cannot identify lowly recurrently mutated driver genes. Here we present a novel approach, Oncodrive-fm, to detect candidate cancer drivers which does not rely on recurrence. First, we hypothesized that any bias toward the accumulation of variants with high functional impact observed in a gene or group of genes may be an indication of positive selection and can thus be used to detect candidate driver genes or gene modules. Next, we developed a method to measure this bias (FM bias) and applied it to three datasets of tumor somatic variants. As a proof of concept of our hypothesis we show that most of the highly recurrent and well-known cancer genes exhibit a clear FM bias. Moreover, this novel approach avoids some known limitations of recurrence-based approaches, and can successfully identify lowly recurrent candidate cancer drivers.},
author = {Gonzalez-Perez, Abel and Lopez-Bigas, Nuria},
doi = {10.1093/nar/gks743},
file = {::;::},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Bioinformatics,Cancer,Genes,Genetic Variation,Genomics,Genomics: methods,Humans,Method,Mutation,Neoplasm,Oncodrive-FM,SNP},
mendeley-tags = {Bioinformatics,Cancer,Method,Oncodrive-FM,SNP},
month = {nov},
number = {21},
pages = {e169},
pmid = {22904074},
title = {{Functional impact bias reveals cancer drivers.}},
url = {http://nar.oxfordjournals.org/content/40/21/e169.full?keytype=ref&ijkey=jWx48Ab6s74fm7I http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3505979&tool=pmcentrez&rendertype=abstract},
volume = {40},
year = {2012}
}
@article{Larson2013,
abstract = {SUMMARY: We have developed a novel Bayesian method, PurBayes, to estimate tumor purity and detect intratumor heterogeneity based on next-generation sequencing data of paired tumor-normal tissue samples, which uses finite mixture modeling methods. We demonstrate our approach using simulated data and discuss its performance under varying conditions. AVAILABILITY: PurBayes is implemented as an R package, and source code is available for download through CRAN at http://cran.r-project.org/package=PurBayes. CONTACT: larson.nicholas@mayo.edu SUPPLEMENTARY INFORMATION: Supplementary data are available online at Bioinformatics online.},
author = {Larson, Nicholas B and Fridley, Brooke L},
doi = {10.1093/bioinformatics/btt293},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Bayes Theorem,Genetic Variation,High-Throughput Nucleotide Sequencing,Humans,Neoplasms,Neoplasms: genetics,Software},
month = {aug},
number = {15},
pages = {1888--9},
pmid = {23749958},
title = {{PurBayes: estimating tumor cellularity and subclonality in next-generation sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23749958},
volume = {29},
year = {2013}
}
@article{Yoshihara2013,
abstract = {Infiltrating stromal and immune cells form the major fraction of normal cells in tumour tissue and not only perturb the tumour signal in molecular studies but also have an important role in cancer biology. Here we describe 'Estimation of STromal and Immune cells in MAlignant Tumours using Expression data' (ESTIMATE)--a method that uses gene expression signatures to infer the fraction of stromal and immune cells in tumour samples. ESTIMATE scores correlate with DNA copy number-based tumour purity across samples from 11 different tumour types, profiled on Agilent, Affymetrix platforms or based on RNA sequencing and available through The Cancer Genome Atlas. The prediction accuracy is further corroborated using 3,809 transcriptional profiles available elsewhere in the public domain. The ESTIMATE method allows consideration of tumour-associated normal cells in genomic and transcriptomic studies. An R-library is available on https://sourceforge.net/projects/estimateproject/.},
author = {Yoshihara, Kosuke and Shahmoradgoli, Maria and Mart{\'{i}}nez, Emmanuel and Vegesna, Rahulsimham and Kim, Hoon and Torres-Garcia, Wandaliz and Trevi{\~{n}}o, Victor and Shen, Hui and Laird, Peter W and Levine, Douglas A and Carter, Scott L and Getz, Gad and Stemke-Hale, Katherine and Mills, Gordon B and Verhaak, Roel G W},
doi = {10.1038/ncomms3612},
file = {::},
issn = {2041-1723},
journal = {Nature communications},
keywords = {Cancer,Tumor Purity},
language = {en},
mendeley-tags = {Cancer,Tumor Purity},
month = {jan},
pages = {2612},
pmid = {24113773},
publisher = {Nature Publishing Group},
title = {{Inferring tumour purity and stromal and immune cell admixture from expression data.}},
url = {http://www.nature.com/ncomms/2013/131011/ncomms3612/full/ncomms3612.html http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3826632&tool=pmcentrez&rendertype=abstract},
volume = {4},
year = {2013}
}
@article{Fischer2013,
abstract = {The spectrum of mutations discovered in cancer genomes can be explained by the activity of a few elementary mutational processes. We present a novel probabilistic method, EMu, to infer the mutational signatures of these processes from a collection of sequenced tumors. EMu naturally incorporates the tumor-specific opportunity for different mutation types according to sequence composition. Applying EMu to breast cancer data, we derive detailed maps of the activity of each process, both genome-wide and within specific local regions of the genome. Our work provides new opportunities to study the mutational processes underlying cancer development. EMu is available at http://www.sanger.ac.uk/resources/software/emu/.},
author = {Fischer, Andrej and Illingworth, Christopher JR and Campbell, Peter J and Mustonen, Ville},
doi = {10.1186/gb-2013-14-4-r39},
file = {::;::},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Cancer,NGS,Software},
mendeley-tags = {Cancer,NGS,Software},
month = {apr},
number = {4},
pages = {R39},
pmid = {23628380},
title = {{EMu: probabilistic inference of mutational processes and their localization in the cancer genome.}},
url = {http://genomebiology.com/2013/14/4/R39 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3663107&tool=pmcentrez&rendertype=abstract},
volume = {14},
year = {2013}
}
@article{Aksoy2013,
abstract = {MOTIVATION: The interaction between drugs and their targets, often proteins, and between antibodies and their targets is important for planning and analyzing investigational and therapeutic interventions in many biological systems. Although drug-target and antibody-target data sets are available in separate databases, they are not publically available in an integrated bioinformatics resource. As medical therapeutics, especially in cancer, increasingly uses targeted drugs and measures their effects on biomolecular profiles, there is an unmet need for a user-friendly toolset that allows researchers to comprehensively and conveniently access and query information about drugs, antibodies and their targets. SUMMARY: The PiHelper framework integrates human drug-target and antibody-target associations from publically available resources to help meet the needs of researchers in systems pharmacology, perturbation biology and proteomics. PiHelper has utilities to i) import drug- and antibody-target information; ii) search the associations either programmatically or through a web user interface (UI); iii) visualize the data interactively in a network; iv) export relationships for use in publications or other analysis tools. AVAILABILITY: PiHelper is free software under the GNU Lesser General Public License (LGPL) v3.0. Source code and documentation are at http://bit.ly/pihelper. We plan to coordinate contributions from the community by managing future releases. CONTACT: pihelper@cbio.mskcc.org.},
author = {Aksoy, B{\"{u}}lent Arman and Gao, Jianjiong and Dresdner, Gideon and Wang, Weiqing and Root, Alex and Jing, Xiaohong and Cerami, Ethan and Sander, Chris},
doi = {10.1093/bioinformatics/btt345},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {jun},
pages = {2--3},
pmid = {23766416},
title = {{PiHelper: An Open Source Framework for Drug-Target and Antibody-Target Data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23766416},
year = {2013}
}
@article{Newman2015,
abstract = {We introduce CIBERSORT, a method for characterizing cell composition of complex tissues from their gene expression profiles. When applied to enumeration of hematopoietic subsets in RNA mixtures from fresh, frozen and fixed tissues, including solid tumors, CIBERSORT outperformed other methods with respect to noise, unknown mixture content and closely related cell types. CIBERSORT should enable large-scale analysis of RNA mixtures for cellular biomarkers and therapeutic targets (http://cibersort.stanford.edu/).},
author = {Newman, Aaron M and Liu, Chih Long and Green, Michael R and Gentles, Andrew J and Feng, Weiguo and Xu, Yue and Hoang, Chuong D and Diehn, Maximilian and Alizadeh, Ash A},
doi = {10.1038/nmeth.3337},
issn = {1548-7091},
journal = {Nature Methods},
keywords = {Cancer Bioinformatics,Method,RNA-Seq,Stanford},
mendeley-tags = {Cancer Bioinformatics,Method,RNA-Seq,Stanford},
month = {mar},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Meth},
title = {{Robust enumeration of cell subsets from tissue expression profiles}},
url = {http://dx.doi.org/10.1038/nmeth.3337},
volume = {advance on},
year = {2015}
}
@article{Saunders2012,
abstract = {MOTIVATION: Whole genome and exome sequencing of matched tumor-normal sample pairs is becoming routine in cancer research. The consequent increased demand for somatic variant analysis of paired samples requires methods specialized to model this problem so as to sensitively call variants at any practical level of tumor impurity. RESULTS: We describe Strelka, a method for somatic SNV and small indel detection from sequencing data of matched tumor-normal samples. The method uses a novel Bayesian approach which represents continuous allele frequencies for both tumor and normal samples, while leveraging the expected genotype structure of the normal. This is achieved by representing the normal sample as a mixture of germline variation with noise, and representing the tumor sample as a mixture of the normal sample with somatic variation. A natural consequence of the model structure is that sensitivity can be maintained at high tumor impurity without requiring purity estimates. We demonstrate that the method has superior accuracy and sensitivity on impure samples compared with approaches based on either diploid genotype likelihoods or general allele-frequency tests. AVAILABILITY: The Strelka workflow source code is available at ftp://strelka@ftp.illumina.com/. CONTACT: csaunders@illumina.com},
author = {Saunders, Christopher T and Wong, Wendy S W and Swamy, Sajani and Becq, Jennifer and Murray, Lisa J and Cheetham, R Keira},
doi = {10.1093/bioinformatics/bts271},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Bayes Theorem,Cancer,Computational Biology,Computational Biology: methods,Exome,Gene Frequency,Genetic,Genetic Variation,Genome,Humans,INDEL Mutation,Models,NGS,Neoplasms,Neoplasms: genetics,SNP,Sequence Alignment,Software},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {jul},
number = {14},
pages = {1811--7},
pmid = {22581179},
title = {{Strelka: accurate somatic small-variant calling from sequenced tumor-normal sample pairs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22581179},
volume = {28},
year = {2012}
}
@article{Su2012,
abstract = {UNLABELLED: We developed a novel algorithm, PurityEst, to infer the tumor purity level from the allelic differential representation of heterozygous loci with somatic mutations in a human tumor sample with a matched normal tissue using next-generation sequencing data. We applied our tool to a whole cancer genome sequencing datasets and demonstrated the accuracy of PurityEst compared with DNA copy number-based estimation. AVAILABILITY: PurityEst has been implemented in PERL and is available at http://odin.mdacc.tmc.edu/$\sim$xsu1/PurityEst.html.},
author = {Su, Xiaoping and Zhang, Li and Zhang, Jianping and Meric-Bernstam, Funda and Weinstein, John N},
doi = {10.1093/bioinformatics/bts365},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Alleles,Base Sequence,DNA,DNA: methods,Genetic,Heterozygote,Humans,Models,Neoplasms,Neoplasms: genetics,Neoplasms: pathology,Sequence Analysis},
month = {sep},
number = {17},
pages = {2265--6},
pmid = {22743227},
title = {{PurityEst: estimating purity of human tumor samples using next-generation sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22743227 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3426843&tool=pmcentrez&rendertype=abstract},
volume = {28},
year = {2012}
}
@article{Koboldt2012,
abstract = {Cancer is a disease driven by genetic variation and mutation. Exome sequencing can be utilized for discovering these variants and mutations across hundreds of tumors. Here we present an analysis tool, VarScan 2, for the detection of somatic mutations and copy number alterations (CNAs) in exome data from tumor-normal pairs. Unlike most current approaches, our algorithm reads data from both samples simultaneously; a heuristic and statistical algorithm detects sequence variants and classifies them by somatic status (germline, somatic, or LOH); while a comparison of normalized read depth delineates relative copy number changes. We apply these methods to the analysis of exome sequence data from 151 high-grade ovarian tumors characterized as part of the Cancer Genome Atlas (TCGA). We validated some 7790 somatic coding mutations, achieving 93% sensitivity and 85% precision for single nucleotide variant (SNV) detection. Exome-based CNA analysis identified 29 large-scale alterations and 619 focal events per tumor on average. As in our previous analysis of these data, we observed frequent amplification of oncogenes (e.g., CCNE1, MYC) and deletion of tumor suppressors (NF1, PTEN, and CDKN2A). We searched for additional recurrent focal CNAs using the correlation matrix diagonal segmentation (CMDS) algorithm, which identified 424 significant events affecting 582 genes. Taken together, our results demonstrate the robust performance of VarScan 2 for somatic mutation and CNA detection and shed new light on the landscape of genetic alterations in ovarian cancer.},
author = {Koboldt, Daniel C and Zhang, Qunyuan and Larson, David E and Shen, Dong and McLellan, Michael D and Lin, Ling and Miller, Christopher A and Mardis, Elaine R and Ding, Li and Wilson, Richard K},
doi = {10.1101/gr.129684.111},
file = {::},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,Cancer Bioinformatics,Computational Biology,DNA Copy Number Variations,Exome,Female,Genomics,Genomics: methods,Genotype,Germ-Line Mutation,Humans,Internet,Mutation,NGS,Neoplasms,Neoplasms: genetics,Ovarian Neoplasms,Ovarian Neoplasms: genetics,Polymorphism,Reproducibility of Results,SNP,SNV,Sensitivity and Specificity,Single Nucleotide,Software},
mendeley-tags = {Cancer Bioinformatics,NGS,SNP,SNV,Software},
month = {mar},
number = {3},
pages = {568--76},
pmid = {22300766},
title = {{VarScan 2: somatic mutation and copy number alteration discovery in cancer by exome sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3290792&tool=pmcentrez&rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Nilsen2012,
abstract = {BACKGROUND: Cancer progression is associated with genomic instability and an accumulation of gains and losses of DNA. The growing variety of tools for measuring genomic copy numbers, including various types of array-CGH, SNP arrays and high-throughput sequencing, calls for a coherent framework offering unified and consistent handling of single- and multi-track segmentation problems. In addition, there is a demand for highly computationally efficient segmentation algorithms, due to the emergence of very high density scans of copy number. RESULTS: A comprehensive Bioconductor package for copy number analysis is presented. The package offers a unified framework for single sample, multi-sample and multi-track segmentation and is based on statistically sound penalized least squares principles. Conditional on the number of breakpoints, the estimates are optimal in the least squares sense. A novel and computationally highly efficient algorithm is proposed that utilizes vector-based operations in R. Three case studies are presented. CONCLUSIONS: The R package copynumber is a software suite for segmentation of single- and multi-track copy number data using algorithms based on coherent least squares principles.},
author = {Nilsen, Gro and Liest{\o}l, Knut and {Van Loo}, Peter and {Moen Vollan}, Hans Kristian and Eide, Marianne B and Rueda, Oscar M and Chin, Suet-Feung and Russell, Roslin and Baumbusch, Lars O and Caldas, Carlos and B{\o}rresen-Dale, Anne-Lise and Lingjaerde, Ole Christian},
doi = {10.1186/1471-2164-13-591},
file = {::},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,CNV,DNA,DNA Copy Number Variations,DNA: genetics,Follicular,Follicular: genetics,Gene Dosage,Genome,Genomic Instability,Human,Humans,Lymphoma,Neoplasms,Neoplasms: genetics,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Polymorphism,R,Single Nucleotide,Software},
mendeley-tags = {CNV,R,Software},
month = {jan},
pages = {591},
pmid = {23442169},
title = {{Copynumber: Efficient algorithms for single- and multi-track copy number segmentation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3582591&tool=pmcentrez&rendertype=abstract},
volume = {13},
year = {2012}
}
@article{Fang2014c,
abstract = {We present the 'dnet' package and apply it to the 'TCGA' mutation and clinical data of >3,000 patients. We uncover the existence of an underlying gene network that at least partially controls cancer 'survivalness', with mutations that are significantly correlated with patient survival, yet independent of tumour origin and type. The survivalness network has natural community structure corresponding to tumour hallmarks, and contains genes that are potentially druggable in the clinic. This network has evolutionary roots in Deuterostomia identifying PTK2 and VAV1 as under-valued relative to more studied genes from that era. The 'dnet' R package is available at http://cran.r-project.org/package=dnet.},
author = {Fang, Hai and Gough, Julian},
doi = {10.1186/s13073-014-0064-8},
file = {::},
issn = {1756-994X},
journal = {Genome Medicine},
number = {8},
pages = {64},
title = {{The 'dnet' approach promotes emerging research on cancer patient survival}},
url = {http://genomemedicine.com/content/6/8/64},
volume = {6},
year = {2014}
}
@article{Wang2012d,
abstract = {We report on the development of an unsupervised algorithm for the genome-wide discovery and analysis of chromatin signatures. Our Chromatin-profile Alignment followed by Tree-clustering algorithm (ChAT) employs dynamic programming of combinatorial histone modification profiles to identify locally similar chromatin sub-regions and provides complementary utility with respect to existing methods. We applied ChAT to genomic maps of 39 histone modifications in human CD4(+) T cells to identify both known and novel chromatin signatures. ChAT was able to detect chromatin signatures previously associated with transcription start sites and enhancers as well as novel signatures associated with a variety of regulatory elements. Promoter-associated signatures discovered with ChAT indicate that complex chromatin signatures, made up of numerous co-located histone modifications, facilitate cell-type specific gene expression. The discovery of novel L1 retrotransposon-associated bivalent chromatin signatures suggests that these elements influence the mono-allelic expression of human genes by shaping the chromatin environment of imprinted genomic regions. Analysis of long gene-associated chromatin signatures point to a role for the H4K20me1 and H3K79me3 histone modifications in transcriptional pause release. The novel chromatin signatures and functional associations uncovered by ChAT underscore the ability of the algorithm to yield novel insight on chromatin-based regulatory mechanisms.},
author = {Wang, Jianrong and Lunyak, Victoria V and Jordan, I King},
doi = {10.1093/nar/gks848},
file = {::},
issn = {1362-4962},
journal = {Nucleic acids research},
month = {sep},
pages = {1--15},
pmid = {22989711},
title = {{Chromatin signature discovery via histone modification profile alignments.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22989711},
year = {2012}
}
@article{Tamborero2013,
abstract = {A well-established approach for detecting genes involved in tumorigenesis due to copy number alterations (CNAs) is to assess the recurrence of the alteration across multiple samples. Expression data can be used to filter this list of candidates by assessing whether the gene expression significantly differs between tumors depending on the copy number status. A drawback of this approach is that it may fail to detect low-recurrent drivers. Furthermore, this analysis does not provide information about expression changes for each gene as compared to the whole data set and does not take into consideration the expression of normal samples. Here we describe a novel method (Oncodrive-CIS) aimed at ranking genes according to the expression impact caused by the CNAs. The rationale of Oncodrive-CIS is based on the hypothesis that genes involved in cancer due to copy number changes are more biased towards misregulation than are bystanders. Moreover, to gain insight into the expression changes caused by gene dosage, the expression of samples with CNAs is compared to that of tumor samples with diploid genotype and also to that of normal samples. Oncodrive-CIS demonstrated better performance in detecting putative associations between copy-number and expression in simulated data sets as compared to other methods aimed to this purpose, and picked up genes likely to be related with tumorigenesis when applied to real cancer samples. In summary, Oncodrive-CIS provides a statistical framework to evaluate the in cis effect of CNAs that may be useful to elucidate the role of these aberrations in driving oncogenesis. An implementation of this method and the corresponding user guide are freely available at http://bg.upf.edu/oncodrivecis.},
author = {Tamborero, David and Lopez-Bigas, Nuria and Gonzalez-Perez, Abel},
doi = {10.1371/journal.pone.0055489},
editor = {Lee, Ju-Seog},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Bioinformatics,CNV,Cancer,Expression,Gene Dosage,Gene Expression,Genetic Techniques,Method},
mendeley-tags = {Bioinformatics,CNV,Cancer,Expression,Method},
month = {jan},
number = {2},
pages = {e55489},
pmid = {23408991},
publisher = {Public Library of Science},
title = {{Oncodrive-CIS: a method to reveal likely driver genes based on the impact of their copy number changes on expression.}},
url = {http://dx.plos.org/10.1371/journal.pone.0055489 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3568145&tool=pmcentrez&rendertype=abstract},
volume = {8},
year = {2013}
}
@article{Kim2013,
abstract = {TopHat is a popular spliced aligner for RNA-sequence (RNA-seq) experiments. In this paper, we describe TopHat2, which incorporates many significant enhancements to TopHat. TopHat2 can align reads of various lengths produced by the latest sequencing technologies, while allowing for variable-length indels with respect to the reference genome. In addition to de novo spliced alignment, TopHat2 can align reads across fusion breaks, which can occur after genomic translocations. TopHat2 combines the ability to identify novel splice sites with direct mapping to known transcripts, producing sensitive and accurate alignments, even for highly repetitive genomes or in the presence of pseudogenes. TopHat2 is available at http://ccb.jhu.edu/software/tophat.},
author = {Kim, Daehwan and Pertea, Geo and Trapnell, Cole and Pimentel, Harold and Kelley, Ryan and Salzberg, Steven L},
doi = {10.1186/gb-2013-14-4-r36},
file = {::;::},
issn = {1465-6914},
journal = {Genome biology},
keywords = {NGS,RNA-Seq,Software,TopHat},
mendeley-tags = {NGS,RNA-Seq,Software,TopHat},
month = {apr},
number = {4},
pages = {R36},
pmid = {23618408},
title = {{TopHat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions.}},
url = {http://genomebiology.com/2013/14/4/R36 http://www.ncbi.nlm.nih.gov/pubmed/23618408},
volume = {14},
year = {2013}
}
@article{Qiao2014,
abstract = {Many tumors are composed of genetically divergent cell subpopulations. We report SubcloneSeeker, a package capable of exhaustive identification of subclone structures and evolutionary histories with bulk somatic variant allele frequency measurements from tumor biopsies. We present a statistical framework to elucidate whether specific sets of mutations are present within the same subclones, and the order in which they occur. We demonstrate how subclone reconstruction provides crucial information about tumorigenesis and relapse mechanisms; guides functional study by variant prioritization, and has the potential as a rational basis for informed therapeutic strategies for the patient. SubcloneSeeker is available at: https://github.com/yiq/SubcloneSeeker.},
author = {Qiao, Yi and Quinlan, Aaron R and Jazaeri, Amir A and Verhaak, Roeland and Wheeler, David A and Marth, Gabor T},
file = {::},
issn = {1465-6906},
journal = {Genome Biology},
keywords = {Cancer,Software,Tumor Heterogeneity},
language = {en},
mendeley-tags = {Cancer,Software,Tumor Heterogeneity},
month = {aug},
number = {8},
pages = {443},
publisher = {BioMed Central Ltd},
title = {{SubcloneSeeker: a computational framework for reconstructing tumor clone structure for cancer variant interpretation and prioritization}},
url = {http://genomebiology.com/2014/15/8/443/abstract},
volume = {15},
year = {2014}
}
@article{Roberts2013,
abstract = {MOTIVATION: With the advent of relatively affordable high-throughput technologies, DNA sequencing of cancers is now common practice in cancer research projects and will be increasingly used in clinical practice to inform diagnosis and treatment. Somatic (cancer-only) single nucleotide variants (SNVs) are the simplest class of mutation, yet their identification in DNA sequencing data is confounded by germline polymorphisms, tumour heterogeneity and sequencing and analysis errors. Four recently published algorithms for the detection of somatic SNV sites in matched cancer-normal sequencing datasets are VarScan, SomaticSniper, JointSNVMix and Strelka. In this analysis, we apply these four SNV calling algorithms to cancer-normal Illumina exome sequencing of a chronic myeloid leukaemia (CML) patient. The candidate SNV sites returned by each algorithm are filtered to remove likely false positives, then characterized and compared to investigate the strengths and weaknesses of each SNV calling algorithm.

RESULTS: Comparing the candidate SNV sets returned by VarScan, SomaticSniper, JointSNVMix2 and Strelka revealed substantial differences with respect to the number and character of sites returned; the somatic probability scores assigned to the same sites; their susceptibility to various sources of noise; and their sensitivities to low-allelic-fraction candidates.

AVAILABILITY: Data accession number SRA081939, code at http://code.google.com/p/snv-caller-review/

CONTACT: david.adelson@adelaide.edu.au

SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Roberts, Nicola D and Kortschak, R Daniel and Parker, Wendy T and Schreiber, Andreas W and Branford, Susan and Scott, Hamish S and Glonek, Garique and Adelson, David L},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Cancer,NGS,Review,SNV,Software},
mendeley-tags = {Cancer,NGS,Review,SNV,Software},
month = {sep},
number = {18},
pages = {2223--30},
title = {{A comparative analysis of algorithms for somatic SNV detection in cancer.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3753564&tool=pmcentrez&rendertype=abstract},
volume = {29},
year = {2013}
}
@article{Ng2012,
abstract = {MOTIVATION: A current challenge in understanding cancer processes is to pinpoint which mutations influence the onset and progression of disease. Toward this goal, we describe a method called PARADIGM-SHIFT that can predict whether a mutational event is neutral, gain-or loss-of-function in a tumor sample. The method uses a belief-propagation algorithm to infer gene activity from gene expression and copy number data in the context of a set of pathway interactions. RESULTS: The method was found to be both sensitive and specific on a set of positive and negative controls for multiple cancers for which pathway information was available. Application to the Cancer Genome Atlas glioblastoma, ovarian and lung squamous cancer datasets revealed several novel mutations with predicted high impact including several genes mutated at low frequency suggesting the approach will be complementary to current approaches that rely on the prevalence of events to reach statistical significance. AVAILABILITY: All source code is available at the github repository http:github.org/paradigmshift. CONTACT: jstuart@soe.ucsc.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Ng, Sam and Collisson, Eric A and Sokolov, Artem and Goldstein, Theodore and Gonzalez-Perez, Abel and Lopez-Bigas, Nuria and Benz, Christopher and Haussler, David and Stuart, Joshua M},
doi = {10.1093/bioinformatics/bts402},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Cancer,Gene Expression,Genes,Humans,Mutation,NF-E2-Related Factor 2,NF-E2-Related Factor 2: genetics,Neoplasm,Neoplasms,Neoplasms: genetics,Pathway,Retinoblastoma Protein,Retinoblastoma Protein: genetics,Software,p53},
mendeley-tags = {Cancer,Pathway,Software},
month = {sep},
number = {18},
pages = {i640--i646},
pmid = {22962493},
title = {{PARADIGM-SHIFT predicts the function of mutations in multiple cancers using pathway impact analysis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3436829&tool=pmcentrez&rendertype=abstract},
volume = {28},
year = {2012}
}
@article{Lonigro2011,
abstract = {The research community at large is expending considerable resources to sequence the coding region of the genomes of tumors and other human diseases using targeted exome capture (i.e., “whole exome sequencing”). The primary goal of targeted exome sequencing is to identify nonsynonymous mutations that potentially have functional consequences. Here, we demonstrate that whole-exome sequencing data can also be analyzed for comprehensively monitoring somatic copy number alterations (CNAs) by benchmarking the technique against conventional array CGH. A series of 17 matched tumor and normal tissues from patients with metastatic castrate-resistant prostate cancer was used for this assessment. We show that targeted exome sequencing reliably identifies CNAs that are common in advanced prostate cancer, such as androgen receptor (AR) gain and PTEN loss. Taken together, these data suggest that targeted exome sequencing data can be effectively leveraged for the detection of somatic CNAs in cancer.},
author = {Lonigro, Robert J. and Grasso, Catherine S. and Robinson, Dan R. and Jing, Xiaojun and Wu, Yi-Mi and Cao, Xuhong and Quist, Michael J. and Tomlins, Scott A. and Pienta, Kenneth J. and Chinnaiyan, Arul M.},
issn = {14765586},
journal = {Neoplasia},
keywords = {CNV,Cancer,NGS data analysis,Targeted Sequencing},
mendeley-tags = {CNV,Cancer,NGS data analysis,Targeted Sequencing},
month = {nov},
number = {11},
pages = {1019--IN21},
title = {{Detection of Somatic Copy Number Alterations in Cancer Using Targeted Exome Capture Sequencing}},
url = {http://www.sciencedirect.com/science/article/pii/S1476558611800886},
volume = {13},
year = {2011}
}
@article{Fernandez-Cuesta2015,
abstract = {Genomic translocation events frequently underlie cancer development through generation of gene fusions with oncogenic properties. Identification of such fusion transcripts by transcriptome sequencing might help to discover new potential therapeutic targets. We developed TRUP (Tumor-specimen suited RNA-seq Unified Pipeline) (https://github.com/ruping/TRUP webcite), a computational approach that combines split-read and read-pair analysis with de novo assembly for the identification of chimeric transcripts in cancer specimens. We apply TRUP to RNA-seq data of different tumor types, and find it to be more sensitive than alternative tools in detecting chimeric transcripts, such as secondary rearrangements in EML4-ALK-positive lung tumors, or recurrent inactivating rearrangements affecting RASSF8.},
author = {Fernandez-Cuesta, Lynnette and Sun, Ruping and Menon, Roopika and George, Julie and Lorenz, Susanne and Meza-Zepeda, Leonardo A and Peifer, Martin and Plenker, Dennis and Heuckmann, Johannes M and Leenders, Frauke and Zander, Thomas and Dahmen, Ilona and Koker, Mirjam and Sch{\"{o}}ttle, Jakob and Ullrich, Roland T and Altm{\"{u}}ller, Janine and Becker, Christian and N{\"{u}}rnberg, Peter and Seidel, Henrik and B{\"{o}}hm, Diana and G{\"{o}}ke, Friederike and Ans{\'{e}}n, Sascha and Russell, Prudence A and Wright, Gavin M and Wainer, Zoe and Solomon, Benjamin and Petersen, Iver and Clement, Joachim H and S{\"{a}}nger, J{\"{o}}rg and Brustugun, Odd-Terje and Helland, {\AA}slaug and Solberg, Steinar and Lund-Iversen, Marius and Buettner, Reinhard and Wolf, J{\"{u}}rgen and Brambilla, Elisabeth and Vingron, Martin and Perner, Sven and Haas, Stefan A and Thomas, Roman K},
doi = {10.1186/s13059-014-0558-0},
issn = {1465-6906},
journal = {Genome Biology},
keywords = {Cancer,Fusions,Lung,SV},
mendeley-tags = {Cancer,Fusions,Lung,SV},
month = {jan},
number = {1},
pages = {7},
title = {{Identification of novel fusion genes in lung cancer using breakpoint assembly of transcriptome sequencing data}},
url = {http://genomebiology.com/2015/16/1/7},
volume = {16},
year = {2015}
}
@article{Danciu2014,
abstract = {The last decade has seen an exponential growth in the quantity of clinical data collected nationwide, triggering an increase in opportunities to reuse the data for biomedical research. The Vanderbilt research data warehouse framework consists of identified and de-identified clinical data repositories, fee-for-service custom services, and tools built atop the data layer to assist researchers across the enterprise. Providing resources dedicated to research initiatives benefits not only the research community, but also clinicians, patients and institutional leadership. This work provides a summary of our approach in the secondary use of clinical data for research domain, including a description of key components and a list of lessons learned, designed to assist others assembling similar services and infrastructure.},
author = {Danciu, Ioana and Cowan, James D. and Basford, Melissa and Wang, Xiaoming and Saip, Alexander and Osgood, Susan and Shirey-Rice, Jana and Kirby, Jacqueline and Harris, Paul A.},
doi = {10.1016/j.jbi.2014.02.003},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
month = {feb},
title = {{Secondary Use of Clinical Data: the Vanderbilt Approach}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046414000392},
year = {2014}
}
@article{Wang2013a,
abstract = {Gene fusions are important genomic events in human cancer because their fusion gene products can drive the development of cancer and thus are potential prognostic tools or therapeutic targets in anti-cancer treatment. Major advancements have been made in computational approaches for fusion gene discovery over the past 3 years due to improvements and widespread applications of high-throughput next generation sequencing (NGS) technologies. To identify fusions from NGS data, existing methods typically leverage the strengths of both sequencing technologies and computational strategies. In this article, we review the NGS and computational features of existing methods for fusion gene detection and suggest directions for future development.},
author = {Wang, Qingguo and Xia, Junfeng and Jia, Peilin and Pao, William and Zhao, Zhongming},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {Fusion,NGS,Review,SV},
mendeley-tags = {Fusion,NGS,Review,SV},
month = {jul},
number = {4},
pages = {506--19},
title = {{Application of next generation sequencing to human gene fusion detection: computational tools, features and perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22877769},
volume = {14},
year = {2013}
}
@article{Hansen2013,
abstract = {MOTIVATION: Extensive DNA sequencing of tumor and matched normal samples using exome and whole-genome sequencing technologies has enabled the discovery of recurrent genetic alterations in cancer cells, but variability in stromal contamination and subclonal heterogeneity still present a severe challenge to available detection algorithms.

RESULTS: Here, we describe publicly available software, Shimmer, which accurately detects somatic single-nucleotide variants using statistical hypothesis testing with multiple testing correction. This program produces somatic single-nucleotide variant predictions with significantly higher sensitivity and accuracy than other available software when run on highly contaminated or heterogeneous samples, and it gives comparable sensitivity and accuracy when run on samples of high purity.

AVAILABILITY: http://www.github.com/nhansen/Shimmer},
author = {Hansen, Nancy F and Gartner, Jared J and Mei, Lan and Samuels, Yardena and Mullikin, James C},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Cell Line, Tumor,DNA Mutational Analysis,DNA Mutational Analysis: methods,Exome,Genetic Variation,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Humans,Mutation,Neoplasms,Neoplasms: genetics,Software},
month = {jun},
number = {12},
pages = {1498--503},
title = {{Shimmer: detection of genetic alterations in tumors using next-generation sequence data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23620360},
volume = {29},
year = {2013}
}
@article{Wang2011c,
abstract = {We developed 'clipping reveals structure' (CREST), an algorithm that uses next-generation sequencing reads with partial alignments to a reference genome to directly map structural variations at the nucleotide level of resolution. Application of CREST to whole-genome sequencing data from five pediatric T-lineage acute lymphoblastic leukemias (T-ALLs) and a human melanoma cell line, COLO-829, identified 160 somatic structural variations. Experimental validation exceeded 80%, demonstrating that CREST had a high predictive accuracy.},
author = {Wang, Jianmin and Mullighan, Charles G and Easton, John and Roberts, Stefan and Heatley, Sue L and Ma, Jing and Rusch, Michael C and Chen, Ken and Harris, Christopher C and Ding, Li and Holmfeldt, Linda and Payne-Turner, Debbie and Fan, Xian and Wei, Lei and Zhao, David and Obenauer, John C and Naeve, Clayton and Mardis, Elaine R and Wilson, Richard K and Downing, James R and Zhang, Jinghui},
doi = {10.1038/nmeth.1628},
file = {::},
issn = {1548-7105},
journal = {Nature methods},
keywords = {Algorithms,Animals,Base Pair Mismatch,Cancer,DNA,DNA: methods,Genome,Genome: genetics,Humans,NGS,Neoplasm,Neoplasm: genetics,Neoplasms,Neoplasms: genetics,Polymorphism,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Single Nucleotide,Single Nucleotide: genetics,Software,Structural Variation},
mendeley-tags = {Cancer,NGS,Software,Structural Variation},
month = {aug},
number = {8},
pages = {652--4},
title = {{CREST maps somatic structural variation in cancer genomes with base-pair resolution.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3527068&tool=pmcentrez&rendertype=abstract},
volume = {8},
year = {2011}
}
@article{Christoforides2013a,
abstract = {BACKGROUND: The field of cancer genomics has rapidly adopted next-generation sequencing (NGS) in order to study and characterize malignant tumors with unprecedented resolution. In particular for cancer, one is often trying to identify somatic mutations--changes specific to a tumor and not within an individual's germline. However, false positive and false negative detections often result from lack of sufficient variant evidence, contamination of the biopsy by stromal tissue, sequencing errors, and the erroneous classification of germline variation as tumor-specific.

RESULTS: We have developed a generalized Bayesian analysis framework for matched tumor/normal samples with the purpose of identifying tumor-specific alterations such as single nucleotide mutations, small insertions/deletions, and structural variation. We describe our methodology, and discuss its application to other types of paired-tissue analysis such as the detection of loss of heterozygosity as well as allelic imbalance. We also demonstrate the high level of sensitivity and specificity in discovering simulated somatic mutations, for various combinations of a) genomic coverage and b) emulated heterogeneity.

CONCLUSION: We present a Java-based implementation of our methods named Seurat, which is made available for free academic use. We have demonstrated and reported on the discovery of different types of somatic change by applying Seurat to an experimentally-derived cancer dataset using our methods; and have discussed considerations and practices regarding the accurate detection of somatic events in cancer genomes. Seurat is available at https://sites.google.com/site/seuratsomatic.},
author = {Christoforides, Alexis and Carpten, John D and Weiss, Glen J and Demeure, Michael J and {Von Hoff}, Daniel D and Craig, David W},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Bayes Theorem,Genomics,Genomics: methods,Humans,Models, Biological,Mutation,Neoplasms,Neoplasms: genetics},
month = {jan},
pages = {302},
title = {{Identification of somatic mutations in cancer through Bayesian-based analysis of sequenced genome pairs.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3751438&tool=pmcentrez&rendertype=abstract},
volume = {14},
year = {2013}
}
@article{VanLoo2010,
abstract = {We present an allele-specific copy number analysis of the in vivo breast cancer genome. We describe a unique bioinformatics approach, ASCAT (allele-specific copy number analysis of tumors), to accurately dissect the allele-specific copy number of solid tumors, simultaneously estimating and adjusting for both tumor ploidy and nonaberrant cell admixture. This allows calculation of "ASCAT profiles" (genome-wide allele-specific copy-number profiles) from which gains, losses, copy number-neutral events, and loss of heterozygosity (LOH) can accurately be determined. In an early-stage breast carcinoma series, we observe aneuploidy (>2.7n) in 45% of the cases and an average nonaberrant cell admixture of 49%. By aggregation of ASCAT profiles across our series, we obtain genomic frequency distributions of gains and losses, as well as genome-wide views of LOH and copy number-neutral events in breast cancer. In addition, the ASCAT profiles reveal differences in aberrant tumor cell fraction, ploidy, gains, losses, LOH, and copy number-neutral events between the five previously identified molecular breast cancer subtypes. Basal-like breast carcinomas have a significantly higher frequency of LOH compared with other subtypes, and their ASCAT profiles show large-scale loss of genomic material during tumor development, followed by a whole-genome duplication, resulting in near-triploid genomes. Finally, from the ASCAT profiles, we construct a genome-wide map of allelic skewness in breast cancer, indicating loci where one allele is preferentially lost, whereas the other allele is preferentially gained. We hypothesize that these alternative alleles have a different influence on breast carcinoma development.},
author = {{Van Loo}, Peter and Nordgard, Silje H and Lingj{\ae}rde, Ole Christian and Russnes, Hege G and Rye, Inga H and Sun, Wei and Weigman, Victor J and Marynen, Peter and Zetterberg, Anders and Naume, Bj{\o}rn and Perou, Charles M and B{\o}rresen-Dale, Anne-Lise and Kristensen, Vessela N},
doi = {10.1073/pnas.1009843107},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Alleles,Ascat,Breast Neoplasms,Breast Neoplasms: genetics,CNV,Cancer,Carcinoma,Carcinoma: genetics,Female,Gene Dosage,Genes,Genome,Human,Humans,NGS,Neoplasm,Ploidies,Software},
mendeley-tags = {Ascat,CNV,Cancer,NGS,Software},
month = {sep},
number = {39},
pages = {16910--5},
title = {{Allele-specific copy number analysis of tumors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2947907&tool=pmcentrez&rendertype=abstract},
volume = {107},
year = {2010}
}
@article{Shiraishi2013,
abstract = {Recent advances in high-throughput sequencing technologies have enabled a comprehensive dissection of the cancer genome clarifying a large number of somatic mutations in a wide variety of cancer types. A number of methods have been proposed for mutation calling based on a large amount of sequencing data, which is accomplished in most cases by statistically evaluating the difference in the observed allele frequencies of possible single nucleotide variants between tumours and paired normal samples. However, an accurate detection of mutations remains a challenge under low sequencing depths or tumour contents. To overcome this problem, we propose a novel method, Empirical Bayesian mutation Calling (https://github.com/friend1ws/EBCall), for detecting somatic mutations. Unlike previous methods, the proposed method discriminates somatic mutations from sequencing errors based on an empirical Bayesian framework, where the model parameters are estimated using sequencing data from multiple non-paired normal samples. Using 13 whole-exome sequencing data with 87.5-206.3 mean sequencing depths, we demonstrate that our method not only outperforms several existing methods in the calling of mutations with moderate allele frequencies but also enables accurate calling of mutations with low allele frequencies (≤10%) harboured within a minor tumour subpopulation, thus allowing for the deciphering of fine substructures within a tumour specimen.},
author = {Shiraishi, Yuichi and Sato, Yusuke and Chiba, Kenichi and Okuno, Yusuke and Nagata, Yasunobu and Yoshida, Kenichi and Shiba, Norio and Hayashi, Yasuhide and Kume, Haruki and Homma, Yukio and Sanada, Masashi and Ogawa, Seishi and Miyano, Satoru},
doi = {10.1093/nar/gkt126},
file = {::},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Bayes Theorem,Cancer,DNA Mutational Analysis,DNA Mutational Analysis: methods,Gene Frequency,Genomics,High-Throughput Nucleotide Sequencing,Humans,NGS,Neoplasms,Neoplasms: genetics,SNP,Software},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {apr},
number = {7},
pages = {e89},
title = {{An empirical Bayesian framework for somatic mutation detection from cancer genome sequencing data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3627598&tool=pmcentrez&rendertype=abstract},
volume = {41},
year = {2013}
}
@article{Griffith2017,
author = {Griffith, Malachi and Spies, Nicholas C and Krysiak, Kilannin and McMichael, Joshua F and Coffman, Adam C and Danos, Arpad M and Ainscough, Benjamin J and Ramirez, Cody A and Rieke, Damian T and Kujan, Lynzey and Barnell, Erica K and Wagner, Alex H and Skidmore, Zachary L and Wollam, Amber and Liu, Connor J and Jones, Martin R and Bilski, Rachel L and Lesurf, Robert and Feng, Yan-Yang and Shah, Nakul M and Bonakdar, Melika and Trani, Lee and Matlock, Matthew and Ramu, Avinash and Campbell, Katie M and Spies, Gregory C and Graubert, Aaron P and Gangavarapu, Karthik and Eldred, James M and Larson, David E and Walker, Jason R and Good, Benjamin M and Wu, Chunlei and Su, Andrew I and Dienstmann, Rodrigo and Margolin, Adam A and Tamborero, David and Lopez-Bigas, Nuria and Jones, Steven J M and Bose, Ron and Spencer, David H and Wartman, Lukas D and Wilson, Richard K and Mardis, Elaine R and Griffith, Obi L},
doi = {10.1038/ng.3774},
issn = {1061-4036},
journal = {Nature Genetics},
keywords = {Bioinformatics,oncogenomics},
mendeley-tags = {Bioinformatics,oncogenomics},
month = {jan},
number = {2},
pages = {170--174},
publisher = {Nature Research},
title = {{CIViC is a community knowledgebase for expert crowdsourcing the clinical interpretation of variants in cancer}},
url = {http://www.nature.com/doifinder/10.1038/ng.3774},
volume = {49},
year = {2017}
}
@article{Oesper2013,
abstract = {Tumor samples are typically heterogeneous, containing admixture by normal, non-cancerous cells and one or more subpopulations of cancerous cells. Whole-genome sequencing of a tumor sample yields reads from this mixture, but does not directly reveal the cell of origin for each read. We introduce THetA (Tumor Heterogeneity Analysis), an algorithm that infers the most likely collection of genomes and their proportions in a sample, for the case where copy number aberrations distinguish subpopulations. THetA successfully estimates normal admixture and recovers clonal and subclonal copy number aberrations in real and simulated sequencing data. THetA is available at http://compbio.cs.brown.edu/software/.},
author = {Oesper, Layla and Mahmoody, Ahmad and Raphael, Benjamin J},
doi = {10.1186/gb-2013-14-7-r80},
file = {::},
issn = {1465-6914},
journal = {Genome biology},
keywords = {algorithms,cancer genomics,dna sequencing,intra-tumor heterogeneity,tumor evolution},
month = {jul},
number = {7},
pages = {R80},
pmid = {23895164},
publisher = {BioMed Central Ltd},
title = {{THetA: inferring intra-tumor heterogeneity from high-throughput DNA sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23895164},
volume = {14},
year = {2013}
}
@article{Chen2013b,
abstract = {MOTIVATION: The accurate detection of copy number alterations (CNAs) in human genomes is important for understanding susceptibility to cancer and mechanisms of tumor progression. CNA detection in tumors from single nucleotide polymorphism (SNP) genotyping arrays is a challenging problem due to phenomena such as aneuploidy, stromal contamination, genomic waves and intra-tumor heterogeneity, issues that leading methods do not optimally address. RESULTS: Here we introduce methods and software (PennCNV-tumor) for fast and accurate CNA detection using signal intensity data from SNP genotyping arrays. We estimate stromal contamination by applying a maximum likelihood approach over multiple discrete genomic intervals. By conditioning on signal intensity across the genome, our method accounts for both aneuploidy and genomic waves. Finally, our method uses a hidden Markov model to integrate multiple sources of information, including total and allele-specific signal intensity at each SNP, as well as physical maps to make posterior inferences of CNAs. Using real data from cancer cell-lines and patient tumors, we demonstrate substantial improvements in accuracy and computational efficiency compared with existing methods. AVAILABILITY: Source code, documentation and example datasets are freely available at http://sourceforge.net/projects/penncnv-2. CONTACT: gary.k.chen@usc.edu or kaichop@gmail.com SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Chen, Gary K and Chang, Xiao and Curtis, Christina and Wang, Kai},
doi = {10.1093/bioinformatics/btt521},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {dec},
number = {23},
pages = {2964--70},
pmid = {24021380},
title = {{Precise inference of copy number alterations in tumor samples from SNP arrays.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24021380},
volume = {29},
year = {2013}
}
@article{Goya2010,
abstract = {MOTIVATION: Next-generation sequencing (NGS) has enabled whole genome and transcriptome single nucleotide variant (SNV) discovery in cancer. NGS produces millions of short sequence reads that, once aligned to a reference genome sequence, can be interpreted for the presence of SNVs. Although tools exist for SNV discovery from NGS data, none are specifically suited to work with data from tumors, where altered ploidy and tumor cellularity impact the statistical expectations of SNV discovery. RESULTS: We developed three implementations of a probabilistic Binomial mixture model, called SNVMix, designed to infer SNVs from NGS data from tumors to address this problem. The first models allelic counts as observations and infers SNVs and model parameters using an expectation maximization (EM) algorithm and is therefore capable of adjusting to deviation of allelic frequencies inherent in genomically unstable tumor genomes. The second models nucleotide and mapping qualities of the reads by probabilistically weighting the contribution of a read/nucleotide to the inference of a SNV based on the confidence we have in the base call and the read alignment. The third combines filtering out low-quality data in addition to probabilistic weighting of the qualities. We quantitatively evaluated these approaches on 16 ovarian cancer RNASeq datasets with matched genotyping arrays and a human breast cancer genome sequenced to >40x (haploid) coverage with ground truth data and show systematically that the SNVMix models outperform competing approaches. AVAILABILITY: Software and data are available at http://compbio.bccrc.ca CONTACT: sshah@bccrc.ca SUPPLEMANTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Goya, Rodrigo and Sun, Mark G F and Morin, Ryan D and Leung, Gillian and Ha, Gavin and Wiegand, Kimberley C and Senz, Janine and Crisan, Anamaria and Marra, Marco A and Hirst, Martin and Huntsman, David and Murphy, Kevin P and Aparicio, Sam and Shah, Sohrab P},
doi = {10.1093/bioinformatics/btq040},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Base Sequence,DNA,DNA: methods,Databases,Gene Expression Profiling,Genetic,Genetic Variation,Genome,Human,Humans,Molecular Sequence Data,NGS data analysis,Neoplasms,Neoplasms: genetics,Sequence Alignment,Sequence Analysis,Software},
mendeley-tags = {NGS data analysis},
month = {mar},
number = {6},
pages = {730--6},
pmid = {20130035},
title = {{SNVMix: predicting single nucleotide variants from next-generation sequencing of tumors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2832826&tool=pmcentrez&rendertype=abstract},
volume = {26},
year = {2010}
}
@article{Bao2014,
abstract = {MOTIVATION: Detection and quantification of the absolute DNA copy number alterations in tumor cells is challenging because the DNA specimen is extracted from a mixture of tumor and normal stromal cells. Estimates of tumor purity and ploidy are necessary to correctly infer copy number, and ploidy may itself be a prognostic factor in cancer progression. As deep sequencing of the exome or genome has become routine for characterization of tumor samples, in this work, we aim to develop a simple and robust algorithm to infer purity, ploidy and absolute copy numbers in whole numbers for tumor cells from sequencing data.

RESULTS: A simulation study shows that estimates have reasonable accuracy, and that the algorithm is robust against the presence of segmentation errors and subclonal populations. We validated our algorithm against a panel of cell lines with experimentally determined ploidy. We also compared our algorithm with the well-established single-nucleotide polymorphism array-based method called ABSOLUTE on three sets of tumors of different types. Our method had good performance on these four benchmark datasets for both purity and ploidy estimates, and may offer a simple solution to copy number alteration quantification for cancer sequencing projects.Availability and implementation: The R package absCNseq is available from http://biostats.mcc.ucsd.edu/files/absCNseq_1.0.tar.gz.

CONTACT: kmesser@ucsd.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Bao, Lei and Pu, Minya and Messer, Karen},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {CNV,Cancer Bioinformatics,NGS,Ploidy,Software,Tumor purity},
mendeley-tags = {CNV,Cancer Bioinformatics,NGS,Ploidy,Software,Tumor purity},
month = {jan},
number = {8},
pages = {1056--1063},
title = {{AbsCN-seq: a statistical method to estimate tumor purity, ploidy and absolute copy numbers from next-generation sequencing data.}},
url = {http://bioinformatics.oxfordjournals.org/content/30/8/1056},
volume = {30},
year = {2014}
}
@article{Stead2013,
abstract = {Current methods for resolving genetically distinct subclones in tumor samples require somatic mutations to be clustered by allelic frequencies, which are determined by applying a variant calling program to next-generation sequencing data. Such programs were developed to accurately distinguish true polymorphisms and somatic mutations from the artifactual nonreference alleles introduced during library preparation and sequencing. However, numerous variant callers exist with no clear indication of the best performer for subclonal analysis, in which the accuracy of the assigned variant frequency is as important as correctly indicating whether the variant is present or not. Furthermore, sequencing depth (the number of times that a genomic position is sequenced) affects the ability to detect low-allelic fraction variants and accurately assign their allele frequencies. We created two synthetic sequencing datasets, and sequenced real KRAS amplicons, with variants spiked in at specific ratios, to assess which caller performs best in terms of both variant detection and assignment of allelic frequencies. We also assessed the sequencing depths required to detect low-allelic fraction variants. We found that VarScan2 performed best overall with sequencing depths of 100×, 250×, 500×, and 1,000× required to accurately identify variants present at 10%, 5%, 2.5%, and 1%, respectively.},
author = {Stead, Lucy F and Sutton, Kate M and Taylor, Graham R and Quirke, Philip and Rabbitts, Pamela},
doi = {10.1002/humu.22365},
file = {::},
issn = {1098-1004},
journal = {Human mutation},
keywords = {Cancer,NGS,SNP,Software},
mendeley-tags = {Cancer,NGS,SNP,Software},
month = {oct},
number = {10},
pages = {1432--8},
pmid = {23766071},
title = {{Accurately identifying low-allelic fraction variants in single samples with next-generation sequencing: applications in tumor subclone resolution.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23766071},
volume = {34},
year = {2013}
}
@article{Fang2013e,
abstract = {Biologists are increasingly confronted with the challenge of quickly understanding genome-wide biological data, which usually involve a large number of genomic coordinates (e.g. genes) but a much smaller number of samples. To meet the need for data of this shape, we present an open-source package called 'supraHex' for training, analysing and visualising omics data. This package devises a supra-hexagonal map to self-organise the input data, offers scalable functionalities for post-analysing the map, and more importantly, allows for overlaying additional data for multilayer omics data comparisons. Via applying to DNA replication timing data of mouse embryogenesis, we demonstrate that supraHex is capable of simultaneously carrying out gene clustering and sample correlation, providing intuitive visualisation at each step of the analysis. By overlaying CpG and expression data onto the trained replication-timing map, we also show that supraHex is able to intuitively capture an inherent relationship between late replication, low CpG density promoters and low expression levels. As part of the Bioconductor project, supraHex makes accessible to a wide community in a simple way, what would otherwise be a complex framework for the ultrafast understanding of any tabular omics data, both scientifically and artistically. This package can run on Windows, Mac and Linux, and is freely available together with many tutorials on featuring real examples at http://supfam.org/supraHex.},
author = {Fang, Hai and Gough, Julian},
doi = {10.1016/j.bbrc.2013.11.103},
file = {::;::},
issn = {1090-2104},
journal = {Biochemical and biophysical research communications},
keywords = {Bioinformatics},
mendeley-tags = {Bioinformatics},
month = {dec},
pmid = {24309102},
publisher = {Elsevier Inc.},
title = {{supraHex: An R/Bioconductor package for tabular omics data analysis using a supra-hexagonal map.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0006291X13020056 http://www.ncbi.nlm.nih.gov/pubmed/24309102},
year = {2013}
}
@article{Dees2012,
abstract = {Massively parallel sequencing technology and the associated rapidly decreasing sequencing costs have enabled systemic analyses of somatic mutations in large cohorts of cancer cases. Here we introduce a comprehensive mutational analysis pipeline that uses standardized sequence-based inputs along with multiple types of clinical data to establish correlations among mutation sites, affected genes and pathways, and to ultimately separate the commonly abundant passenger mutations from the truly significant events. In other words, we aim to determine the Mutational Significance in Cancer (MuSiC) for these large data sets. The integration of analytical operations in the MuSiC framework is widely applicable to a broad set of tumor types and offers the benefits of automation as well as standardization. Herein, we describe the computational structure and statistical underpinnings of the MuSiC pipeline and demonstrate its performance using 316 ovarian cancer samples from the TCGA ovarian cancer project. MuSiC correctly confirms many expected results, and identifies several potentially novel avenues for discovery.},
author = {Dees, Nathan D and Zhang, Qunyuan and Kandoth, Cyriac and Wendl, Michael C and Schierding, William and Koboldt, Daniel C and Mooney, Thomas B and Callaway, Matthew B and Dooling, David and Mardis, Elaine R and Wilson, Richard K and Ding, Li},
doi = {10.1101/gr.134635.111},
file = {::},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,BRCA1 Protein,BRCA1 Protein: genetics,Cancer Bioinformatics,Computational Biology,Computational Biology: methods,DNA Mutational Analysis,DNA Mutational Analysis: methods,DNA Mutational Analysis: standards,Female,Genes,Humans,Molecular Sequence Annotation,Molecular Sequence Annotation: methods,Mutation,Neoplasm,Ovarian Neoplasms,Ovarian Neoplasms: genetics,Reproducibility of Results,Software},
mendeley-tags = {Cancer Bioinformatics},
month = {aug},
number = {8},
pages = {1589--98},
pmid = {22759861},
title = {{MuSiC: identifying mutational significance in cancer genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3409272&tool=pmcentrez&rendertype=abstract},
volume = {22},
year = {2012}
}
@article{Carter2012,
author = {Carter, Scott L and Cibulskis, Kristian and Helman, Elena and McKenna, Aaron and Shen, Hui and Zack, Travis and Laird, Peter W and Onofrio, Robert C and Winckler, Wendy and Weir, Barbara A and Beroukhim, Rameen and Pellman, David and Levine, Douglas A and Lander, Eric S and Meyerson, Matthew and Getz, Gad},
doi = {10.1038/nbt.2203},
file = {::},
issn = {10870156},
journal = {Nature Biotechnology},
number = {October 2011},
title = {{Absolute quantification of somatic DNA alterations in human cancer}},
url = {http://www.nature.com/doifinder/10.1038/nbt.2203},
year = {2012}
}
@article{Drier2013,
abstract = {Whole-genome sequencing using massively parallel sequencing technologies enables accurate detection of somatic rearrangements in cancer. Pinpointing large numbers of rearrangement breakpoints to base-pair resolution allows analysis of rearrangement microhomology and genomic location for every sample. Here we analyze 95 tumor genome sequences from breast, head and neck, colorectal, and prostate carcinomas, and from melanoma, multiple myeloma, and chronic lymphocytic leukemia. We discover three genomic factors that are significantly correlated with the distribution of rearrangements: replication time, transcription rate, and GC content. The correlation is complex, and different patterns are observed between tumor types, within tumor types, and even between different types of rearrangements. Mutations in the APC gene correlate with and, hence, potentially contribute to DNA breakage in late-replicating, low %GC, untranscribed regions of the genome. We show that somatic rearrangements display less microhomology than germline rearrangements, and that breakpoint loci are correlated with local hypermutability with a particular enrichment for transversions.},
author = {Drier, Yotam and Lawrence, Michael S and Carter, Scott L and Stewart, Chip and Gabriel, Stacey B and Lander, Eric S and Meyerson, Matthew and Beroukhim, Rameen and Getz, Gad},
doi = {10.1101/gr.141382.112},
file = {::},
issn = {1549-5469},
journal = {Genome research},
keywords = {Base Composition,BreakPointer,Chromosome Breakage,Chromosome Breakpoints,Computational Biology,Computational Biology: methods,DNA Replication,Genetic,Humans,Mutation,Mutation Rate,NGS,Neoplasms,Neoplasms: genetics,Recombination,SV,Software,Transcription},
mendeley-tags = {BreakPointer,NGS,SV,Software},
month = {feb},
number = {2},
pages = {228--35},
pmid = {23124520},
title = {{Somatic rearrangements across cancer reveal classes of samples with distinct patterns of DNA breakage and rearrangement-induced hypermutability.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3561864&tool=pmcentrez&rendertype=abstract},
volume = {23},
year = {2013}
}
@article{Gu2013,
abstract = {BACKGROUND: DNA methylation of promoter CpG islands is associated with gene suppression, and its unique genome-wide profiles have been linked to tumor progression. Coupled with high-throughput sequencing technologies, it can now efficiently determine genome-wide methylation profiles in cancer cells. Also, experimental and computational technologies make it possible to find the functional relationship between cancer-specific methylation patterns and their clinicopathological parameters. METHODOLOGY/PRINCIPAL FINDINGS: Cancer methylome system (CMS) is a web-based database application designed for the visualization, comparison and statistical analysis of human cancer-specific DNA methylation. Methylation intensities were obtained from MBDCap-sequencing, pre-processed and stored in the database. 191 patient samples (169 tumor and 22 normal specimen) and 41 breast cancer cell-lines are deposited in the database, comprising about 6.6 billion uniquely mapped sequence reads. This provides comprehensive and genome-wide epigenetic portraits of human breast cancer and endometrial cancer to date. Two views are proposed for users to better understand methylation structure at the genomic level or systemic methylation alteration at the gene level. In addition, a variety of annotation tracks are provided to cover genomic information. CMS includes important analytic functions for interpretation of methylation data, such as the detection of differentially methylated regions, statistical calculation of global methylation intensities, multiple gene sets of biologically significant categories, interactivity with UCSC via custom-track data. We also present examples of discoveries utilizing the framework. CONCLUSIONS/SIGNIFICANCE: CMS provides visualization and analytic functions for cancer methylome datasets. A comprehensive collection of datasets, a variety of embedded analytic functions and extensive applications with biological and translational significance make this system powerful and unique in cancer methylation research. CMS is freely accessible at: http://cbbiweb.uthscsa.edu/KMethylomes/.},
author = {Gu, Fei and Doderer, Mark S. and Huang, Yi-Wen and Roa, Juan C. and Goodfellow, Paul J. and Kizer, E. Lynette and Huang, Tim H. M. and Chen, Yidong},
doi = {10.1371/journal.pone.0060980},
editor = {Chuang, Eric Y.},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Breast Neoplasms,Breast Neoplasms: genetics,Cell Line,DNA Methylation,Database,Databases,Endometrial Neoplasms,Endometrial Neoplasms: genetics,Female,Genes,Genetic,Genome,Human,Humans,Information Storage and Retrieval,Internet,Methylome,Molecular Sequence Annotation,Multigene Family,NGS,Neoplasm,Promoter Regions,Software,Tumor,User-Computer Interface},
mendeley-tags = {Database,Methylome,NGS,Software},
month = {jan},
number = {4},
pages = {e60980},
pmid = {23630576},
title = {{CMS: a web-based system for visualization and analysis of genome-wide methylation data of human cancers.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3632540&tool=pmcentrez&rendertype=abstract},
volume = {8},
year = {2013}
}
@article{Yuan2012,
abstract = {Somatic Copy Number Alterations (CNAs) in human genomes are present in almost all human cancers. Systematic efforts to characterize such structural variants must effectively distinguish significant consensus events from random background aberrations. Here we introduce Significant Aberration in Cancer (SAIC), a new method for characterizing and assessing the statistical significance of recurrent CNA units. Three main features of SAIC include: (1) exploiting the intrinsic correlation among consecutive probes to assign a score to each CNA unit instead of single probes; (2) performing permutations on CNA units that preserve correlations inherent in the copy number data; and (3) iteratively detecting Significant Copy Number Aberrations (SCAs) and estimating an unbiased null distribution by applying an SCA-exclusive permutation scheme.},
author = {Yuan, Xiguo and Yu, Guoqiang and Hou, Xuchu and Shih, Ie-Ming and Clarke, Robert and Zhang, Junying and Hoffman, Eric P and Wang, Roger R and Zhang, Zhen and Wang, Yue},
doi = {10.1186/1471-2164-13-342},
file = {::},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,CNV,Cancer: NGS,Computer Simulation,DNA Copy Number Variations,DNA Copy Number Variations: genetics,Databases,Female,Genetic,Genome,Human,Human: genetics,Humans,Male,Models,Neoplasms,Neoplasms: genetics,Oncogenes,Oncogenes: genetics,Software},
mendeley-tags = {CNV,Cancer: NGS,Software},
month = {jan},
pages = {342},
title = {{Genome-wide identification of significant aberrations in cancer genome.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3428679&tool=pmcentrez&rendertype=abstract},
volume = {13},
year = {2012}
}
@article{Stehr2011,
abstract = {BACKGROUND: Current large-scale cancer sequencing projects have identified large numbers of somatic mutations covering an increasing number of different cancer tissues and patients. However, the characterization of these mutations at the structural and functional level remains a challenge. RESULTS: We present results from an analysis of the structural impact of frequent missense cancer mutations using an automated method. We find that inactivation of tumor suppressors in cancer correlates frequently with destabilizing mutations preferably in the core of the protein, while enhanced activity of oncogenes is often linked to specific mutations at functional sites. Furthermore, our results show that this alteration of oncogenic activity is often associated with mutations at ATP or GTP binding sites. CONCLUSIONS: With our findings we can confirm and statistically validate the hypotheses for the gain-of-function and loss-of-function mechanisms of oncogenes and tumor suppressors, respectively. We show that the distinct mutational patterns can potentially be used to pre-classify newly identified cancer-associated genes with yet unknown function.},
author = {Stehr, Henning and Jang, Seon-Hi J and Duarte, Jos{\'{e}} M and Wierling, Christoph and Lehrach, Hans and Lappe, Michael and Lange, Bodo M H},
doi = {10.1186/1476-4598-10-54},
file = {::},
issn = {1476-4598},
journal = {Molecular cancer},
keywords = {Databases,Genetic,Humans,Missense,Missense: genetics,Models,Molecular,Molecular Sequence Annotation,Molecular Structure,Mutation,Neoplasms,Neoplasms: genetics,Neoplasms: pathology,Oncogene Proteins,Oncogene Proteins: chemistry,Oncogene Proteins: genetics,Polymorphism,Protein Stability,Single Nucleotide,Single Nucleotide: genetics,Tumor Suppressor Proteins,Tumor Suppressor Proteins: chemistry,Tumor Suppressor Proteins: genetics},
month = {jan},
number = {1},
pages = {54},
pmid = {21575214},
title = {{The structural impact of cancer-associated missense mutations in oncogenes and tumor suppressors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21575214 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3123651&tool=pmcentrez&rendertype=abstract},
volume = {10},
year = {2011}
}
@article{Kumar2014,
abstract = {OBJECTIVE
The aim of this study is to conceptualize a novel approach, which facilitates us to design prototype interfaces for healthcare software. 

METHODS
Concepts and techniques from various disciplines were used to conceptualize an interface design approach named MORTARS (Map Original Rhetorical To Adapted Rhetorical Situation). The concepts and techniques included in this approach are (1) rhetorical situation – a concept of philosophy provided by Lloyd Bitzer (1968); (2) move analysis – an applied linguistic technique provided by Swales and Bhatia (1990); (3) interface design guidelines – a cognitive and computer science concept provided by Johnson (2010); (4) usability evaluation instrument – an interface evaluation questionnaire provided by Lund (2001); (5) user modeling via stereotyping – a cognitive and computer science concept provided by Elaine Rich (1979). A prototype interface for outpatient clinic software was designed to introduce the underlying concepts of MORTARS. The prototype interface was evaluated by thirty-two medical informaticians. 

RESULTS
The medical informaticians found the designed prototype interface to be useful (73.3%), easy to use (71.9%), easy to learn (93.1%), and satisfactory (53.2%). 

CONCLUSIONS
MORTARS approach was found to be effective in designing the prototype user interface for the outpatient clinic software. This approach might be further used to design interfaces for various software pertaining to healthcare and other domains.},
author = {Kumar, Ajit and Maskara, Reena and Maskara, Sanjeev and Chiang, I-Jen},
doi = {10.1016/j.jbi.2014.02.007},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
month = {feb},
title = {{Conceptualization and Application of an Approach for Designing Healthcare Software Interfaces}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046414000434},
year = {2014}
}
@article{Gonzalez-Perez2013,
abstract = {The IntOGen-mutations platform (http://www.intogen.org/mutations/) summarizes somatic mutations, genes and pathways involved in tumorigenesis. It identifies and visualizes cancer drivers, analyzing 4,623 exomes from 13 cancer sites. It provides support to cancer researchers, aids the identification of drivers across tumor cohorts and helps rank mutations for better clinical decision-making.},
author = {Gonzalez-Perez, Abel and Perez-Llamas, Christian and Deu-Pons, Jordi and Tamborero, David and Schroeder, Michael P and Jene-Sanz, Alba and Santos, Alberto and Lopez-Bigas, Nuria},
file = {::},
issn = {1548-7105},
journal = {Nature methods},
keywords = {Cancer Genomics,Pan-Cancer},
mendeley-tags = {Cancer Genomics,Pan-Cancer},
month = {sep},
title = {{IntOGen-mutations identifies cancer drivers across tumor types.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24037244},
year = {2013}
}
@article{Shugay2013,
abstract = {MOTIVATION: Gene fusions resulting from chromosomal aberrations are an important cause of cancer. The complexity of genomic changes in certain cancer types has hampered the identification of gene fusions by molecular cytogenetic methods, especially in carcinomas. This is changing with the advent of next-generation sequencing, which is detecting a substantial number of new fusion transcripts in individual cancer genomes. However, this poses the challenge of identifying those fusions with greater oncogenic potential amid a background of 'passenger' fusion sequences. RESULTS: In the present work, we have used some recently identified genomic hallmarks of oncogenic fusion genes to develop a pipeline for the classification of fusion sequences, namely, Oncofuse. The pipeline predicts the oncogenic potential of novel fusion genes, calculating the probability that a fusion sequence behaves as 'driver' of the oncogenic process based on features present in known oncogenic fusions. Cross-validation and extensive validation tests on independent datasets suggest a robust behavior with good precision and recall rates. We believe that Oncofuse could become a useful tool to guide experimental validation studies of novel fusion sequences found during next-generation sequencing analysis of cancer transcriptomes. Availability and implementation: Oncofuse is a naive Bayes Network Classifier trained and tested using Weka machine learning package. The pipeline is executed by running a Java/Groovy script, available for download at www.unav.es/genetica/oncofuse.html.},
author = {Shugay, Mikhail and {Ortiz de Mend{\'{i}}bil}, I{\~{n}}igo and Vizmanos, Jos{\'{e}} L and Novo, Francisco J},
doi = {10.1093/bioinformatics/btt445},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {oct},
number = {20},
pages = {2539--46},
pmid = {23956304},
title = {{Oncofuse: a computational framework for the prediction of the oncogenic potential of gene fusions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23956304},
volume = {29},
year = {2013}
}
@article{Vaske2010,
abstract = {MOTIVATION: High-throughput data is providing a comprehensive view of the molecular changes in cancer tissues. New technologies allow for the simultaneous genome-wide assay of the state of genome copy number variation, gene expression, DNA methylation and epigenetics of tumor samples and cancer cell lines. Analyses of current data sets find that genetic alterations between patients can differ but often involve common pathways. It is therefore critical to identify relevant pathways involved in cancer progression and detect how they are altered in different patients. RESULTS: We present a novel method for inferring patient-specific genetic activities incorporating curated pathway interactions among genes. A gene is modeled by a factor graph as a set of interconnected variables encoding the expression and known activity of a gene and its products, allowing the incorporation of many types of omic data as evidence. The method predicts the degree to which a pathway's activities (e.g. internal gene states, interactions or high-level 'outputs') are altered in the patient using probabilistic inference. Compared with a competing pathway activity inference approach called SPIA, our method identifies altered activities in cancer-related pathways with fewer false-positives in both a glioblastoma multiform (GBM) and a breast cancer dataset. PARADIGM identified consistent pathway-level activities for subsets of the GBM patients that are overlooked when genes are considered in isolation. Further, grouping GBM patients based on their significant pathway perturbations divides them into clinically-relevant subgroups having significantly different survival outcomes. These findings suggest that therapeutics might be chosen that target genes at critical points in the commonly perturbed pathway(s) of a group of patients. AVAILABILITY: Source code available at http://sbenz.github.com/Paradigm,. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Vaske, Charles J and Benz, Stephen C and Sanborn, J Zachary and Earl, Dent and Szeto, Christopher and Zhu, Jingchun and Haussler, David and Stuart, Joshua M},
doi = {10.1093/bioinformatics/btq182},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Breast Neoplasms,Breast Neoplasms: genetics,Cancer,DNA Copy Number Variations,Female,Gene Expression Profiling,Gene Expression Profiling: methods,Genomics,Genomics: methods,Glioblastoma,Glioblastoma: genetics,Humans,Neoplasms,Neoplasms: genetics,Pathway,Software},
mendeley-tags = {Cancer,Pathway,Software},
month = {jun},
number = {12},
pages = {i237--45},
pmid = {20529912},
title = {{Inference of patient-specific pathway activities from multi-dimensional cancer genomics data using PARADIGM.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2881367&tool=pmcentrez&rendertype=abstract},
volume = {26},
year = {2010}
}
@article{Cibulskis2013a,
abstract = {Detection of somatic point substitutions is a key step in characterizing the cancer genome. However, existing methods typically miss low-allelic-fraction mutations that occur in only a subset of the sequenced cells owing to either tumor heterogeneity or contamination by normal cells. Here we present MuTect, a method that applies a Bayesian classifier to detect somatic mutations with very low allele fractions, requiring only a few supporting reads, followed by carefully tuned filters that ensure high specificity. We also describe benchmarking approaches that use real, rather than simulated, sequencing data to evaluate the sensitivity and specificity as a function of sequencing depth, base quality and allelic fraction. Compared with other methods, MuTect has higher sensitivity with similar specificity, especially for mutations with allelic fractions as low as 0.1 and below, making MuTect particularly useful for studying cancer subclones and their evolution in standard exome and genome sequencing data.},
author = {Cibulskis, Kristian and Lawrence, Michael S and Carter, Scott L and Sivachenko, Andrey and Jaffe, David and Sougnez, Carrie and Gabriel, Stacey and Meyerson, Matthew and Lander, Eric S and Getz, Gad},
doi = {10.1038/nbt.2514},
file = {::},
issn = {1546-1696},
journal = {Nature biotechnology},
keywords = {Bayes Theorem,Cancer,Cancer Genomics,DNA,GATK,Genome,Heterozygote,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Human,Humans,MuTect,Neoplasm,Neoplasm: analysis,Neoplasm: genetics,Neoplasms,Neoplasms: genetics,Point Mutation,Reproducibility of Results,SNP calling,SNV,Sensitivity and Specificity,Software},
mendeley-tags = {Cancer,Cancer Genomics,GATK,MuTect,SNP calling,SNV,Software},
month = {mar},
number = {3},
pages = {213--9},
pmid = {23396013},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples.}},
url = {http://dx.doi.org/10.1038/nbt.2514 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3833702&tool=pmcentrez&rendertype=abstract http://www.ncbi.nlm.nih.gov/pubmed/23396013},
volume = {31},
year = {2013}
}
@article{Aksoy2014a,
abstract = {MOTIVATION: Somatic homozygous deletions of chromosomal regions in cancer, while not necessarily oncogenic, may lead to therapeutic vulnerabilities specific to cancer cells compared with normal cells. A recently reported example is the loss of one of the two isoenzymes in glioblastoma cancer cells such that the use of a specific inhibitor selectively inhibited growth of the cancer cells, which had become fully dependent on the second isoenzyme. We have now made use of the unprecedented conjunction of large-scale cancer genomics profiling of tumor samples in The Cancer Genome Atlas (TCGA) and of tumor-derived cell lines in the Cancer Cell Line Encyclopedia, as well as the availability of integrated pathway information systems, such as Pathway Commons, to systematically search for a comprehensive set of such epistatic vulnerabilities.

RESULTS: Based on homozygous deletions affecting metabolic enzymes in 16 TCGA cancer studies and 972 cancer cell lines, we identified 4104 candidate metabolic vulnerabilities present in 1019 tumor samples and 482 cell lines. Up to 44% of these vulnerabilities can be targeted with at least one Food and Drug Administration-approved drug. We suggest focused experiments to test these vulnerabilities and clinical trials based on personalized genomic profiles of those that pass preclinical filters. We conclude that genomic profiling will in the future provide a promising basis for network pharmacology of epistatic vulnerabilities as a promising therapeutic strategy.

AVAILABILITY AND IMPLEMENTATION: A web-based tool for exploring all vulnerabilities and their details is available at http://cbio.mskcc.org/cancergenomics/statius/ along with supplemental data files.},
author = {Aksoy, B{\"{u}}lent Arman and Demir, Emek and Babur, {\"{O}}zg{\"{u}}n and Wang, Weiqing and Jing, Xiaohong and Schultz, Nikolaus and Sander, Chris},
doi = {10.1093/bioinformatics/btu164},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {jul},
number = {14},
pages = {2051--9},
pmid = {24665131},
title = {{Prediction of individualized therapeutic vulnerabilities in cancer from genomic profiles.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4080742&tool=pmcentrez&rendertype=abstract},
volume = {30},
year = {2014}
}
@article{4358917,
author = {Qu, Changtao and Zimmermann, F and Kumpf, K and Kamuzinzi, R and Ledent, V and Herzog, R},
doi = {10.1109/TITB.2007.907987},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Web services;biology computing;data mining;grid co},
number = {2},
pages = {182--190},
title = {{Semantics-Enabled Service Discovery Framework in the SIMDAT Pharma Grid}},
volume = {12},
year = {2008}
}
@inproceedings{4426890,
author = {Choi, YoungJin and Jeong, Karpjoo and Kim, Dongkwang and Lee, Jonghyun and Lim, Sang Boem and Jung, Seunho and Heo, Daeyoung and Hwang, Suntae and Byeon, Ok-Hwan},
booktitle = {e-Science and Grid Computing, IEEE International Conference on},
doi = {10.1109/E-SCIENCE.2007.38},
keywords = {biology computing;grid computing;groupware;molecul},
pages = {213--220},
title = {{Glyco-MGrid: A Collaborative Molecular Simulation Grid for e-Glycomics}},
year = {2007}
}
@inproceedings{1199353,
author = {Kurata, K and Breton, V},
booktitle = {Cluster Computing and the Grid, 2003. Proceedings. CCGrid 2003. 3rd IEEE/ACM International Symposium on},
doi = {10.1109/CCGRID.2003.1199353},
keywords = {distributed databases;genetic engineering;grid com},
month = {may},
pages = {62--69},
title = {{A method to find unique sequences on distributed genomic databases}},
year = {2003}
}
@inproceedings{6266328,
author = {Mirto, M and Passante, M and Aloisio, G},
booktitle = {Computer-Based Medical Systems (CBMS), 2012 25th International Symposium on},
doi = {10.1109/CBMS.2012.6266328},
issn = {1063-7125},
keywords = {bioinformatics;data visualisation;genomics;grid co},
pages = {1--6},
title = {{The ProGenGrid virtual laboratory for bioinformatics}},
year = {2012}
}
@article{Database,
author = {Database, What I S A},
file = {::},
pages = {1--35},
title = {{Introduction to Biological Databases}}
}
@inproceedings{4414488,
author = {Liu, Pengfei and Dong, Shoubin and Cao, Yicheng and Du, Zhengping and Mao, Zhike},
booktitle = {Asia-Pacific Service Computing Conference, The 2nd IEEE},
doi = {10.1109/APSCC.2007.79},
keywords = {distributed databases;grid computing;software arch},
pages = {406--413},
title = {{A Complex Virtual Screening Computing Platform Based on SOA}},
year = {2007}
}
@article{Liu2009,
author = {Liu, Yuelan and Wang, Jianhua and Liu, Yuefan and Tan, Zhenwu},
doi = {10.1109/NDT.2009.5272808},
file = {::},
isbn = {978-1-4244-4614-8},
journal = {2009 First International Conference on Networked Digital Technologies},
keywords = {-- web,according to their respective,bioinformatics,data integration,heterogeneous database,services,targets,they collect and},
month = {jul},
pages = {292--296},
publisher = {Ieee},
title = {{Research on data integration of bioinformatics database based on web services}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5272808},
year = {2009}
}
@inproceedings{41684,
address = {Prague, Czech Republic},
author = {Schwarzkopf, Malte and Konwinski, Andy and Abd-El-Malek, Michael and Wilkes, John},
booktitle = {SIGOPS European Conference on Computer Systems (EuroSys)},
pages = {351--364},
title = {{Omega: flexible, scalable schedulers for large compute clusters}},
url = {http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf},
year = {2013}
}
@inproceedings{6723630,
author = {Ding, Xiang and Wang, Xiaoqing and Dou, Aixia and Yuan, Xiaoxiang and Wang, Long},
booktitle = {Geoscience and Remote Sensing Symposium (IGARSS), 2013 IEEE International},
doi = {10.1109/IGARSS.2013.6723630},
issn = {2153-6996},
keywords = {earthquakes;geophysical techniques;China North-Sou},
pages = {3686--3689},
title = {{The development of Great Earthquake Risk Assessment System based on high resolution grid data}},
year = {2013}
}
@inproceedings{4520187,
author = {Ragan, M},
booktitle = {Parallel Architectures, Algorithms, and Networks, 2008. I-SPAN 2008. International Symposium on},
doi = {10.1109/I-SPAN.2008.53},
issn = {1087-4089},
keywords = {Web services;biology computing;cellular biophysics},
month = {may},
pages = {10},
title = {{The Visible Cell #153; Project: Changing the Way We Think about Cells}},
year = {2008}
}
@article{Kr2003,
author = {Kr, Peer},
file = {::},
keywords = {computational biology,data analysis,data modelling,molecular biology databases},
pages = {7--42},
title = {{A Computational Biology Database Digest : Data , Data Analysis , and Data Management}},
volume = {13},
year = {2003}
}
@inproceedings{1385879,
author = {Sheldon, F T and Batsell, S G and Prowell, S J and Langston, M A},
booktitle = {System Sciences, 2005. HICSS '05. Proceedings of the 38th Annual Hawaii International Conference on},
doi = {10.1109/HICSS.2005.495},
issn = {1530-1605},
keywords = {Autonomic Computing Paradigm;Cyber-Security;Infras},
pages = {310a--310a},
title = {{Position Statement: Methodology to Support Dependable Survivable Cyber-Secure Infrastructures}},
year = {2005}
}
@inproceedings{6001941,
author = {Han, Xiangdi and Zhou, Yi and Wang, Shixin and Wang, Litao and Hou, Yanfang},
booktitle = {Multimedia Technology (ICMT), 2011 International Conference on},
doi = {10.1109/ICMT.2011.6001941},
keywords = {ecology;economic indicators;environmental science},
pages = {739--742},
title = {{Spatialization approach to 1km grid GDP based on remote sensing}},
year = {2011}
}
@inproceedings{6154909,
author = {Dobre, C and Cristea, V},
booktitle = {P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC), 2011 International Conference on},
doi = {10.1109/3PGCIC.2011.70},
keywords = {data analysis;discrete event simulation;informatio},
month = {oct},
pages = {372--377},
title = {{Simulation Analysis of CMS Data Replication and Production Activities}},
year = {2011}
}
@inproceedings{6008868,
author = {Sul, Seung-Jin and Tovchigrechko, A},
booktitle = {Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW), 2011 IEEE International Symposium on},
doi = {10.1109/IPDPS.2011.180},
issn = {1530-2075},
keywords = {C++ language;bioinformatics;learning (artificial i},
month = {may},
pages = {481--489},
title = {{Parallelizing BLAST and SOM Algorithms with MapReduce-MPI Library}},
year = {2011}
}
@inproceedings{6685667,
abstract = {It is a tendency for enterprises to deploy their applications on Infrastructure as a Service (IaaS) platforms. Many latest IaaS service providers offer Virtual Machine (VM) instances with various capacities and prices by the minute. In this paper, based on the observation that the workload of nowaday applications fluctuates frequently, we propose a cost-aware automatic VM scaling method of fine granularity to satisfy the Service Level Agreement (SLA) and minimize the rent of VMs down to the minute. In the environment with sporadic and sharp swings of workload, our approach acts quickly to get suitable VM scaling scheme to stabilize the response time, reduce the SLA violations and save the rent of VM usage.},
author = {Zhao, He and Peng, Chenglei and Yu, Yao and Zhou, Yu and Wang, Ziqiang and Du, Sidan},
booktitle = {Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC), 2013 International Conference on},
doi = {10.1109/CyberC.2013.26},
keywords = {cloud computing;contracts;virtual machines;IaaS se},
month = {oct},
pages = {109--116},
title = {{Cost-Aware Automatic Virtual Machine Scaling in Fine Granularity for Cloud Applications}},
year = {2013}
}
@inproceedings{4725900,
author = {Pan, Q H and Hadzic, F and Dillon, Tharam S},
booktitle = {Semantics, Knowledge and Grid, 2008. SKG '08. Fourth International Conference on},
doi = {10.1109/SKG.2008.57},
keywords = {XML;bank data processing;data mining;knowledge man},
pages = {87--94},
title = {{Conjoint Data Mining of Structured and Semi-structured Data}},
year = {2008}
}
@inproceedings{1300502,
author = {Kuo, Yu-Lun and Yang, Chao-Tung and Lai, Chuan-Lin and Tseng, Tsai-Ming},
booktitle = {Parallel Architectures, Algorithms and Networks, 2004. Proceedings. 7th International Symposium on},
doi = {10.1109/ISPAN.2004.1300502},
issn = {1087-4089},
keywords = {Internet;biology computing;distributed databases;g},
month = {may},
pages = {339--344},
title = {{Construct a grid computing environment for bioinformatics}},
year = {2004}
}
@inproceedings{6123522,
abstract = {Scientific computing requires simulation and visualization involving large data sets among collaborating teams. Cloud platforms offer a promising solution via ScaaS. We report on the architecture, implementation and User Experience (UX) evaluation of one such SCaaS platform implementing TOUGH2V2.0, a numerical simulator for sub-surface fluid and heat flow, offered as a service. Results from example simulations, with virtualization of workloads in a multi-tenant, Virtual Machine (VM)-based cloud platform, are presented. These include fluid production from a geothermal reservoir, diffusive and advective spreading of contaminants, radial flow from a CO2 injection well and gas diffusion of a chemical through porous media. Prepackaged VM pools deployed autonomically ensure that sessions are provisioned elastically on demand. Users can access data-intensive visualizations via a web-browser. Authentication, user state and sessions are managed securely via an Access Gateway, to autonomically redirect and manage the workflows when multiple concurrent users are accessing their own sessions. Usability in the cloud and the traditional desktop are comparatively assessed, using several UX metrics. Simulated network conditions of different quality were imposed using a WAN emulator. Usability was found to be good for all the simulations under even moderately degraded network quality, as long as latency was not well above 100 ms. Hosting of a complex scientific computing application on an actual, global Enterprise cloud platform (as opposed to earlier remoting platforms) and its usability assessment, both presented for the first time, are the essential contributions of this work.},
author = {Saripalli, P and Oldenburg, C and Walters, B and Radheshyam, N},
booktitle = {Utility and Cloud Computing (UCC), 2011 Fourth IEEE International Conference on},
doi = {10.1109/UCC.2011.58},
keywords = {authorisation;cloud computing;computational fluid},
pages = {345--354},
title = {{Implementation and Usability Evaluation of a Cloud Platform for Scientific Computing as a Service (SCaaS)}},
year = {2011}
}
@inproceedings{4375754,
author = {Afgan, E and Bangalore, P},
booktitle = {Bioinformatics and Bioengineering, 2007. BIBE 2007. Proceedings of the 7th IEEE International Conference on},
doi = {10.1109/BIBE.2007.4375754},
keywords = {biology computing;genetics;grid computing;molecula},
month = {oct},
pages = {1394--1398},
title = {{Performance Characterization of BLAST for the Grid}},
year = {2007}
}
@inproceedings{5137641,
author = {Yang, Mei and Wang, Shixin and Zhou, Yi and Wang, Litao and Zeng, Chuiqing},
booktitle = {Urban Remote Sensing Event, 2009 Joint},
doi = {10.1109/URS.2009.5137641},
keywords = {demography;economics;environmental management;geog},
month = {may},
pages = {1--6},
title = {{Population spatialization in Gansu Province based on RS and GIS}},
year = {2009}
}
@article{4267683,
author = {Gong, Xiujun and Nakamura, K and Yu, Hua and Yura, Kei and Go, Nobuhiro},
doi = {10.1109/TITB.2006.888700},
file = {::},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Attitudes,Factual;Health Knowledge,Practice;Information Dissemination;Information St,biology computing;biomedical engineering;data mini},
number = {4},
pages = {428--434},
title = {{BAAQ: An Infrastructure for Application Integration and Knowledge Discovery in Bioinformatics}},
volume = {11},
year = {2007}
}
@inproceedings{5716071,
author = {Moustafa, A and Bhattacharya, D and Allen, A E},
booktitle = {Biomedical Engineering Conference (CIBEC), 2010 5th Cairo International},
doi = {10.1109/CIBEC.2010.5716071},
issn = {2156-6097},
keywords = {bioinformatics;biological techniques;cellular biop},
pages = {103--107},
title = {{iTree: A high-throughput phylogenomic pipeline}},
year = {2010}
}
@article{Systems2007,
author = {Systems, Modern Database},
file = {::},
title = {{Scientific databases Biological data management}},
year = {2007}
}
@inproceedings{4561985,
author = {Mirto, M and Fiore, S and Cafaro, M and Passante, M and Aloisio, G},
booktitle = {Computer-Based Medical Systems, 2008. CBMS '08. 21st IEEE International Symposium on},
doi = {10.1109/CBMS.2008.93},
issn = {1063-7125},
keywords = {biology computing;grid computing;relational databa},
pages = {191--196},
title = {{A Grid-Based Bioinformatics Wrapper for Biological Databases}},
year = {2008}
}
@inproceedings{1520959,
author = {Wozniak, J M and Brenner, P and Thain, D and Striegel, A and Izaguirre, J A},
booktitle = {High Performance Distributed Computing, 2005. HPDC-14. Proceedings. 14th IEEE International Symposium on},
doi = {10.1109/HPDC.2005.1520959},
issn = {1082-8907},
keywords = {biology computing;digital simulation;grid computin},
pages = {191--200},
title = {{Generosity and gluttony in GEMS: grid enabled molecular simulations}},
year = {2005}
}
@inproceedings{5420181,
author = {Liu, Xiyu and Tang, Hao},
booktitle = {Pervasive Computing (JCPC), 2009 Joint Conferences on},
doi = {10.1109/JCPC.2009.5420181},
keywords = {biocomputing;computational complexity;pattern clus},
pages = {249--252},
title = {{A migrating DNA computing technique for grid clustering}},
year = {2009}
}
@article{Yu2007,
author = {Yu, C. and Wilson, P.},
doi = {10.1109/HPCMP-UGC.2007.5},
file = {::},
isbn = {978-0-7695-3088-5},
journal = {2007 DoD High Performance Computing Modernization Program Users Group Conference},
month = {jun},
pages = {417--420},
publisher = {Ieee},
title = {{A Tool for Creating and Parallelizing Bioinformatics Pipelines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4438020},
volume = {2},
year = {2007}
}
@article{1644730,
author = {Chen, S and Zhang, X and Rundensteiner, E A},
doi = {10.1109/TKDE.2006.117},
issn = {1041-4347},
journal = {Knowledge and Data Engineering, IEEE Transactions on},
keywords = {concurrency control;data integrity;distributed dat},
number = {8},
pages = {1068--1081},
title = {{A compensation-based approach for view maintenance in distributed environments}},
volume = {18},
year = {2006}
}
@inproceedings{4221142,
author = {Mirto, M and Cafaro, M and Fiore, S and Aloisio, G},
booktitle = {Advanced Information Networking and Applications Workshops, 2007, AINAW '07. 21st International Conference on},
doi = {10.1109/AINAW.2007.26},
keywords = {Web services;biology computing;grid computing;rela},
month = {may},
pages = {713--718},
title = {{A Grid System for the Ingestion of Biological Data into a Relational DBMS}},
volume = {1},
year = {2007}
}
@inproceedings{4736761,
abstract = {This paper proposes and evaluates an approach to the parallelization, deployment and management of bioinformatics applications that integrates several emerging technologies for distributed computing. The proposed approach uses the MapReduce paradigm to parallelize tools and manage their execution, machine virtualization to encapsulate their execution environments and commonly used data sets into flexibly deployable virtual machines, and network virtualization to connect resources behind firewalls/NATs while preserving the necessary performance and the communication environment. An implementation of this approach is described and used to demonstrate and evaluate the proposed approach. The implementation integrates Hadoop, Virtual Workspaces, and ViNe as the MapReduce, virtual machine and virtual network technologies, respectively, to deploy the commonly used bioinformatics tool NCBI BLAST on a WAN-based test bed consisting of clusters at two distinct locations, the University of Florida and the University of Chicago. This WAN-based implementation, called CloudBLAST, was evaluated against both non-virtualized and LAN-based implementations in order to assess the overheads of machine and network virtualization, which were shown to be insignificant. To compare the proposed approach against an MPI-based solution, CloudBLAST performance was experimentally contrasted against the publicly available mpiBLAST on the same WAN-based test bed. Both versions demonstrated performance gains as the number of available processors increased, with CloudBLAST delivering speedups of 57 against 52.4 of MPI version, when 64 processors on 2 sites were used. The results encourage the use of the proposed approach for the execution of large-scale bioinformatics applications on emerging distributed environments that provide access to computing resources as a service.},
author = {Matsunaga, A and Tsugawa, M and Fortes, J},
booktitle = {eScience, 2008. eScience '08. IEEE Fourth International Conference on},
doi = {10.1109/eScience.2008.62},
file = {::},
keywords = {Cloud,application program interfaces,bioinformatics,mess},
mendeley-tags = {Cloud,bioinformatics},
pages = {222--229},
title = {{CloudBLAST: Combining MapReduce and Virtualization on Distributed Resources for Bioinformatics Applications}},
year = {2008}
}
@inproceedings{6270584,
abstract = {Cloud computing is increasingly becoming a popular solution to massive data analysis in bioinformatics. In order to enable scientists to harness the computing power provided by Cloud platforms, we designed Green Pipe, a scalable computational workflow system, which runs jobs as MapReduce tasks on virtual Hadoop clusters. This paper introduces a power-aware scheduling algorithm in the workflow engine to optimize workflow execution in terms of running time and energy consumption. Experimental results demonstrate the performance improvement in Green Pipe.},
author = {Mao, Yaokuan and Wu, Wenjun and Zhang, Hui and Luo, Liang},
booktitle = {Parallel and Distributed Processing Symposium Workshops PhD Forum (IPDPSW), 2012 IEEE 26th International},
doi = {10.1109/IPDPSW.2012.273},
keywords = {bioinformatics;cloud computing;data analysis;data},
month = {may},
pages = {2211--2219},
title = {{GreenPipe: A Hadoop Based Workflow System on Energy-efficient Clouds}},
year = {2012}
}
@inproceedings{5621639,
author = {Miladinova, S and Stips, A},
booktitle = {Baltic International Symposium (BALTIC), 2010 IEEE/OES US/EU},
doi = {10.1109/BALTIC.2010.5621639},
issn = {2150-6027},
keywords = {bathymetry;mixing;oceanographic regions;oceanograp},
pages = {1--7},
title = {{Modelling of Baltic Sea inflow events and deep water currents}},
year = {2010}
}
@article{journals/bioinformatics/Schatz09,
author = {Schatz, Michael C},
file = {::},
journal = {Bioinformatics},
keywords = {Cloud,bioinformatics,dblp},
mendeley-tags = {Cloud,bioinformatics},
number = {11},
pages = {1363--1369},
title = {{CloudBurst: highly sensitive read mapping with MapReduce.}},
url = {http://dblp.uni-trier.de/db/journals/bioinformatics/bioinformatics25.html#Schatz09},
volume = {25},
year = {2009}
}
@inproceedings{6399736,
abstract = {Because of the ever-increasing application of next-generation sequencing (NGS) in research, and the expectation of faster experiment turn-around, it is becoming unfeasible and unscalable for analysis to be done exclusively by existing trained bioinformaticians. Instead, researchers and bench biologists are performing at least parts of most analyses. In order for this to be realized, two conditions must be satisfied: (1) well designed and accessible tools need to be made available, and (2) researchers and biologists need to be trained to use such tools in order to confidently handle high volumes of NGS data. Bio-Linux is a fully featured, powerful, configurable and easy to maintain bioinformatics workstation and helps on both counts by offering well over one hundred bioinformatics tools packaged into a single distribution, easily accessible and readily usable. Bio-Linux is also accessible in the form of virtual images or on the cloud, thus providing researchers with immediate access to scalable compute infrastructure required to run the analysis. Furthermore this paper discusses how bioinformatics training on Bio-Linux is helping to bridge the data production and analysis gap.},
author = {Booth, Timothy and Bicak, Mesude and Gweon, Hyun Soon and Field, Dawn and Afgan, Enis},
booktitle = {2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE)},
doi = {10.1109/BIBE.2012.6399736},
isbn = {978-1-4673-4358-9},
keywords = {Bio-Linux,Bioinformatics,Cloud computing,Genomics,Linux,NGS,Universal Serial Bus,analysis gap,bioinformatics tools,bioinformatics training,bioinformatics workstation,data analysis,data production,next-generation sequencing,sequences,training,virtual images},
month = {nov},
pages = {578--582},
publisher = {IEEE},
title = {{Bio-Linux as a tool for bioinformatics training}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6399736},
year = {2012}
}
@inproceedings{1357998,
author = {Curcin, Vasa and Ghanem, M and Guo, Yike and Rowe, A and He, Wayne and Pei, Hao and Qiang, Lu and Li, Yuanyuan},
booktitle = {Services Computing, 2004. (SCC 2004). Proceedings. 2004 IEEE International Conference on},
doi = {10.1109/SCC.2004.1357998},
keywords = {Internet;biology computing;data integrity;grid com},
month = {sep},
pages = {123--131},
title = {{IT service infrastructure for integrative systems biology}},
year = {2004}
}
@inproceedings{6428800,
abstract = {With the rapid growth of emerging applications like social network analysis, semantic Web analysis and bioinformatics network analysis, a variety of data to be processed continues to witness a quick increase. Effective management and analysis of large-scale data poses an interesting but critical challenge. Recently, big data has attracted a lot of attention from academia, industry as well as government. This paper introduces several big data processing technics from system and application aspects. First, from the view of cloud data management and big data processing mechanisms, we present the key issues of big data processing, including cloud computing platform, cloud architecture, cloud database and data storage scheme. Following the Map Reduce parallel processing framework, we then introduce Map Reduce optimization strategies and applications reported in the literature. Finally, we discuss the open issues and challenges, and deeply explore the research directions in the future on big data processing in cloud computing environments.},
author = {Ji, Changqing and Li, Yu and Qiu, Wenming and Awada, Uchechukwu and Li, Keqiu},
booktitle = {2012 12th International Symposium on Pervasive Systems, Algorithms and Networks},
doi = {10.1109/I-SPAN.2012.9},
isbn = {978-1-4673-5064-8},
issn = {1087-4089},
keywords = {Big Data,Cloud Computing,Computer architecture,Data Management,Data handling,Data models,Data storage systems,Distributed Computing,Distributed databases,Information management,MapReduce optimization strategies,MapReduce parallel processing,application aspects,big data processing mechanisms,cloud architecture,cloud computing environments,cloud data management,cloud database,data storage scheme,large-scale data analysis,large-scale data management,network theory (graphs),open issues,optimisation,parallel processing,social sciences,system aspects},
month = {dec},
pages = {17--23},
publisher = {IEEE},
title = {{Big Data Processing in Cloud Computing Environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6428800},
year = {2012}
}
@inproceedings{5693909,
abstract = {Volcanoes are a principal factor of hazard across the Pacific Rim, with their focus of interest mostly divided into pyroclastic flows and ash deposition. The latter has significantly more impact due to its widespread geographical reach and prolonged effects in human activities and health. TEPHRA is a volcanic ash dispersion model based on a simple version of the advection-diffusion Suzuki model, which has been revisited and modified for the Iraz'u volcano in Costa Rica. A full parameter exploration is necessary in this particular case (albeit not sufficient) due to scarce observational data. We present in this paper the model, its assumptions and limitations as well as application lifecycle with resulting ash distribution graphics. The computational experimental settings are described, in particular the use of Nimrod/G with respect to non-homogeneous parameter sweeps and its impact on execution time. We also analyze the implementation of a new parameter discard mechanism common to e-Science experiments where sequential generation of new parameter sets has to be complemented with an early verification in order to avoid allocation of CPU time to non-valid scenarios. Finally four sample 100K-scenario runs are analyzed for both traditional HPC clustering and Cloud computing resources in the Amazon EC2 Cloud.},
author = {Núñez, S and Bethwaite, B and Brenes, J and Barrantes, G and Castro, J and Malavassi, E and Abramson, D},
booktitle = {e-Science (e-Science), 2010 IEEE Sixth International Conference on},
doi = {10.1109/eScience.2010.27},
keywords = {ash;cloud computing;diffusion;geophysics computing},
pages = {129--136},
title = {{NG-TEPHRA: A Massively Parallel, Nimrod/G-enabled Volcanic Simulation in the Grid and the Cloud}},
year = {2010}
}
@inproceedings{5736131,
author = {Ailin, Chen and ZeMei, Dai and Jie, Ding and Haifeng, Huang and Yan, Wang and Ming-yi, He},
booktitle = {Electricity Distribution (CICED), 2010 China International Conference on},
keywords = {IEC standards;network servers;substation automatio},
month = {sep},
pages = {1--5},
title = {{Application of IEC 61850 proxy in seamless communication between digital substation and control centre}},
year = {2010}
}
@inproceedings{4031094,
author = {Kus, W},
booktitle = {e-Science and Grid Computing, 2006. e-Science '06. Second IEEE International Conference on},
doi = {10.1109/E-SCIENCE.2006.261054},
keywords = {Biological cells;Databases;Evolutionary computatio},
pages = {121},
title = {{Evolutionary Optimization of Forging Anvils Using Grid Based on Alchemi Framework}},
year = {2006}
}
@inproceedings{4662800,
author = {Levesque, M J and Ichikawa, K and Date, S and Haga, J H},
booktitle = {Grid Computing, 2008 9th IEEE/ACM International Conference on},
doi = {10.1109/GRID.2008.4662800},
keywords = {database management systems;drug delivery systems;},
month = {sep},
pages = {201--208},
title = {{Bringing flexibility to virtual screening for enzymatic inhibitors on the grid}},
year = {2008}
}
@article{6204095,
author = {Biserica, M and Besanger, Y and Caire, Raphael and Chilard, O and Deschamps, P},
doi = {10.1109/TSG.2012.2193673},
issn = {1949-3053},
journal = {Smart Grid, IEEE Transactions on},
keywords = {distributed power generation;power system control;},
month = {sep},
number = {3},
pages = {1137--1144},
title = {{Neural Networks to Improve Distribution State Estimation #x2014;Volt Var Control Performances}},
volume = {3},
year = {2012}
}
@inproceedings{4133153,
author = {Ghaibeh, A A and Sasaki, M and Chuman, H},
booktitle = {Computational Intelligence and Bioinformatics and Computational Biology, 2006. CIBCB '06. 2006 IEEE Symposium on},
doi = {10.1109/CIBCB.2006.331011},
keywords = {chemical engineering computing;computational geome},
month = {sep},
pages = {1--6},
title = {{Using Voronoi Grid and SVM Linear Regression in Drug Discovery}},
year = {2006}
}
@inproceedings{6098717,
abstract = {The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. We present an improved MapReduce-parallel implementation by splitting both of input query sequence files and sequence databases for search, called bCloudBLAST, showing very good scaling and speedup behavior on large sequence database. bCloudBLAST is written in Java, executable on UNIX/Linux, Windows and NacOS systems. (Free) MapReduce and Hadoop libraries can be found at http://hadoop.apache.org/. For download: http://www.darwintree.cn/tools.htm.},
author = {Meng, Zhen and Li, Jianhui and Zhou, Yunchun and Liu, Qi and Liu, Yong and Cao, Wei},
booktitle = {Biomedical Engineering and Informatics (BMEI), 2011 4th International Conference on},
doi = {10.1109/BMEI.2011.6098717},
keywords = {DNA;Java;Linux;bioinformatics;molecular biophysics},
month = {oct},
pages = {2072--2076},
title = {{bCloudBLAST: An efficient mapreduce program for bioinformatics applications}},
volume = {4},
year = {2011}
}
@inproceedings{6063037,
author = {Siddesh, G M and Srinivasa, K G},
booktitle = {High Performance Computing and Communications (HPCC), 2011 IEEE 13th International Conference on},
doi = {10.1109/HPCC.2011.77},
keywords = {Internet;bioinformatics;grid computing;resource al},
month = {sep},
pages = {544--549},
title = {{An Adaptive Scheduler Framework for Complex Workflow Jobs on Grid Systems}},
year = {2011}
}
@article{5611496,
abstract = {Executing large number of independent jobs or jobs comprising of large number of tasks that perform minimal intertask communication is a common requirement in many domains. Various technologies ranging from classic job schedulers to the latest cloud technologies such as MapReduce can be used to execute these "many-tasks” in parallel. In this paper, we present our experience in applying two cloud technologies Apache Hadoop and Microsoft DryadLINQ to two bioinformatics applications with the above characteristics. The applications are a pairwise Alu sequence alignment application and an Expressed Sequence Tag (EST) sequence assembly program. First, we compare the performance of these cloud technologies using the above applications and also compare them with traditional MPI implementation in one application. Next, we analyze the effect of inhomogeneous data on the scheduling mechanisms of the cloud technologies. Finally, we present a comparison of performance of the cloud technologies under virtual and nonvirtual hardware platforms.},
author = {Ekanayake, J and Gunarathne, T and Qiu, J},
doi = {10.1109/TPDS.2010.178},
file = {::},
issn = {1045-9219},
journal = {Parallel and Distributed Systems, IEEE Transactions on},
keywords = {Cloud,bioinformatics,cloud computing,message passing,sch},
mendeley-tags = {Cloud,bioinformatics},
number = {6},
pages = {998--1011},
title = {{Cloud Technologies for Bioinformatics Applications}},
volume = {22},
year = {2011}
}
@article{Babu1986,
author = {Babu, M Madan},
file = {::},
title = {{Biological Databases and Protein Sequence Analysis}},
year = {1986}
}
@inproceedings{1286595,
author = {Cannataro, M and Comito, C and Guzzo, A and Veltri, P},
booktitle = {Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004. International Conference on},
doi = {10.1109/ITCC.2004.1286595},
keywords = {biology;distributed databases;grid computing;meta},
pages = {90--94 Vol.2},
title = {{Integrating ontology and workflow in PROTEUS, a grid-based problem solving environment for bioinformatics}},
volume = {2},
year = {2004}
}
@inproceedings{1647659,
author = {Aloisio, G and Cafaro, M and Fiore, S and Mirto, M},
booktitle = {Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on},
doi = {10.1109/CBMS.2006.28},
issn = {1063-7125},
keywords = {Internet;biology computing;database management sys},
pages = {739--744},
title = {{A Split amp;amp; Merge Data Management Architecture for a Grid Environment}},
year = {2006}
}
@article{Zomaya2005,
author = {Zomaya, a.},
doi = {10.1109/ICITA.2005.167},
file = {::},
isbn = {0-7695-2316-1},
journal = {Third International Conference on Information Technology and Applications (ICITA'05)},
pages = {3--3},
publisher = {Ieee},
title = {{Keynote 1: Grid Computing: Opportunities for Bioinformatics Research}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1488761},
volume = {1},
year = {2005}
}
@inproceedings{4215439,
author = {Kasam, V and Salzemann, J and Jacq, N and Mass, A and Breton, V},
booktitle = {Cluster Computing and the Grid, 2007. CCGRID 2007. Seventh IEEE International Symposium on},
doi = {10.1109/CCGRID.2007.66},
keywords = {diseases;grid computing;medical computing;molecula},
month = {may},
pages = {691--700},
title = {{Large Scale Deployment of Molecular Docking Application on Computational Grid infrastructures for Combating Malaria}},
year = {2007}
}
@article{Karol2001,
author = {Karol, Michael and Dwan, Christopher and Freeman, John and Weissman, Jon and Livny, Miron and Retzel, Emest},
file = {::},
isbn = {0769512968},
pages = {3--4},
title = {{Applying Grid Technologies to Bioinformatics}},
year = {2001}
}
@inproceedings{5333882,
author = {Koumakis, L and Moustakis, V and Tsiknakis, M and Kafetzopoulos, D and Potamias, G},
booktitle = {Engineering in Medicine and Biology Society, 2009. EMBC 2009. Annual International Conference of the IEEE},
doi = {10.1109/IEMBS.2009.5333882},
issn = {1557-170X},
keywords = {Nucleic Acid;Female;Genetic Association Studies;H,Single Nucleotide;Systems Integration,Web services;biology computing;genomics;breast can},
month = {sep},
pages = {6958--6962},
title = {{Supporting genotype-to-phenotype association studies with grid-enabled knowledge discovery workflows}},
year = {2009}
}
@inproceedings{6184986,
abstract = {The sizes of databases have seen exponential growth in the past, and such growth is expected to accelerate in the future, with the steady drop in storage cost accompanied by a rapid increase in storage capacity. Many years ago, a terabyte database was considered to be large, but nowadays they are sometimes regarded as small, and the daily volumes of data being added to some databases are measured in terabytes. In the future, petabyte and exabyte databases will be common. With such volumes of data, it is evident that the sequential processing paradigm will be unable to cope, for example, even assuming a data rate of 1 terabyte per second, reading through a petabyte database will take over 10 days. To effectively manage such volumes of data, it is necessary to allocate multiple resources to it, very often massively so. The processing of databases of such astronomical proportions requires an understanding of how high-performance systems and parallelism work. Besides the massive volume of data in the database to be processed, some data has been distributed across the globe in a Grid environment. These massive data centres are also a part of the emergence of Cloud computing, where data access has shifted from local machines to powerful servers hosting web applications and services, making data access across the Internet using standard web browsers pervasive. This adds another dimension to such systems. This talk, based on our recent published book [1], discusses fundamental understanding of parallelism in data-intensive applications, and demonstrates how to develop faster capabilities to support them. This includes the importance of indexing in parallel systems [2-4], specialized algorithms to support various query processing [5-9], as well as objectoriented scheme [10-12]. Parallelism in databases has been around since the early 1980s, when many researchers in this area aspired to build large special-purpose database machines -- databases employing dedicated specialize- parallel hardware. Some projects were born, including Bubba, Gamma, etc. These came and went. However, commercial DBMS vendors quickly realized the importance of supporting high performance for large databases, and many of them have incorporated parallelism and grid features into their products. Their commitment to high-performance systems and parallelism, as well as grid configurations, shows the importance and inevitability of parallelism. There have been an increase number of researches in high performance parallel database processing in the last five years (2008-12). Data partitioning is still the fundamental issue in high performance database processing [13, 14]. The data itself is getting more complex, including XML-based data [15, 16], bio-informatics data [17, 18], and data streams [19, 20]. These new data types require new approaches to parallel processing. In addition, database transactions [21, 22] are still a major focus in many high performance database systems, such as grid transactions. We also see an increasing growth of new application domains, broadly categorized as data-intensive applications, including data warehousing and online analytic processing (OLAP) [23-25]. Therefore, it is critical to understand the underlying principle of data parallelism, before specialized and new application domains can be properly addressed.},
author = {Taniar, D},
booktitle = {Advanced Information Networking and Applications (AINA), 2012 IEEE 26th International Conference on},
doi = {10.1109/AINA.2012.140},
issn = {1550-445X},
keywords = {Web services;XML;cloud computing;data mining;data},
pages = {5--6},
title = {{High Performance Database Processing}},
year = {2012}
}
@article{6701655720110801,
abstract = {The article focuses on the use of cloud computing services of Amazon Web Services LLC (AWS) in biomedical research. It states that AWS provides computing environment in storage, random acess memory (RAM), and networking for biomedical research. Moreover, information regarding cloud computing security, scalable computing development, and a case study on genome mapping is also discussed.},
author = {Fusaro, Vincent A and Patil, Prasad and Gafni, Erik and Wall, Dennis P and Tonellato, Peter J},
file = {::},
journal = {PLoS Computational Biology},
keywords = {--,Cloud,amazon,area,bioinformatics,biology,cloud,computer,computing,gene,llc,mapping,medicine,networks,research,services,storage,web},
mendeley-tags = {Cloud,bioinformatics},
number = {8},
pages = {1--6},
title = {{Biomedical Cloud Computing With Amazon Web Services.}},
url = {http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=67016557&site=ehost-live},
volume = {7},
year = {2011}
}
@inproceedings{5558018,
abstract = {The Biological applications such as Gene and Protein analysis integrate and analyze biological data for the research in many bioinformatics and other bio related fields. Such applications are used under many large scale scientific applications and help in computing, integrating data, execute the analysis, automate the process by using information retrieved by different tasks and computational procedures to assist the scientists in scientific discovery and data distribution. Grid based and/or web based scientific workflow tools are used for bioinformatics related complex research to make scientists' and researchers' work easier. On average, scientists spend about 80% of their time assembling data to prepare for analysis. This is due largely in part to the fact that many of these resources required for data processing must be gathered from an external source. The best of these resources, however, are scattered across the globe. They are hosted at universities, institutes, and laboratories throughout the world. To bring all of these resources together by hiding system, network, and application level heterogeneity issues are challenging.},
author = {Balasooriya, J},
booktitle = {Cloud Computing (CLOUD), 2010 IEEE 3rd International Conference on},
doi = {10.1109/CLOUD.2010.80},
keywords = {Internet;bioinformatics;grid computing;Web based s},
pages = {526--527},
title = {{Cloud Computing Infrastructure for Biological Echo-Systems}},
year = {2010}
}
@inproceedings{5380170,
author = {Piao, Minghe and Lim, Sang Boem and Lee, Jong Hyun},
booktitle = {Computer and Electrical Engineering, 2009. ICCEE '09. Second International Conference on},
doi = {10.1109/ICCEE.2009.50},
keywords = {grid computing;portals;scheduling;virtual reality;},
pages = {615--619},
title = {{GREE-VS: A Grid-based Remote Execution Environment System for Virtual Screening}},
volume = {1},
year = {2009}
}
@inproceedings{1392652,
author = {Hunter, A and Schibeci, D and Hiew, H L and Bellgard, M},
booktitle = {Cluster Computing, 2004 IEEE International Conference on},
doi = {10.1109/CLUSTR.2004.1392652},
issn = {1552-5244},
keywords = {Internet;Java;Perl;biology computing;distributed o},
month = {sep},
pages = {486--},
title = {{GRID-enabled bioinformatics applications for comparative genomic analysis at the CBBC}},
year = {2004}
}
@article{Readings2004,
author = {Readings, Further},
file = {::},
keywords = {Bioinformatica,introduccion},
mendeley-tags = {Bioinformatica,introduccion},
number = {February},
title = {{Accomplishments and Challenges in Bioinformatics The Details :}},
year = {2004}
}
@article{IAES2012486,
abstract = {This paper outlines the key characteristics that cloud computing technologies possess and illustrates the cloud computing stack containing the three essential services (SaaS, PaaS and IaaS) that have come to define the technology and its delivery model. The underlying virtualization technologies that make cloud computing possible are also identified and explained. The various challenges that face cloud computing technologies today are investigated and discussed. The future of cloud computing technologies along with its various applications and trends are also explored, giving a brief outlook of where and how the technology will progress into the future.},
author = {Carlin, Sean and Curran, Kevin},
file = {::},
journal = {International Journal of Cloud Computing and Services Science (IJ-CLOSER)},
keywords = {Cloud,bioinformatics},
mendeley-tags = {Cloud,bioinformatics},
number = {2},
pages = {59--65},
title = {{Cloud Computing Technologies}},
url = {http://iaesjournal.com/online/index.php/IJ-CLOSER/article/view/486},
volume = {1},
year = {2012}
}
@inproceedings{1630940,
author = {Blanchet, C and Combet, C and Deleage, G},
booktitle = {Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
doi = {10.1109/CCGRID.2006.1630940},
keywords = {biology computing;distributed databases;genetics;g},
month = {may},
pages = {8 pp.--48},
title = {{Integrating bioinformatics resources on the EGEE grid platform}},
volume = {2},
year = {2006}
}
@inproceedings{6488081,
abstract = {Cloud computing, providing access to more than a hundred thousand computers at a time to process, store or share information has stretched the horizon of possibilities in many areas of science and business. The pay as you use nature of cloud computing services such as EC2 provided by Amazon wherein you pay only for the servers, server or part thereof that you use also makes it economically attractive. This paper is targeted at researchers in the field of genetics and biotechnology who wish to understand how cloud computing can contribute to their area of research. Bio-informatics tools are extensively used in biological research today. These tools can be utilized more efficiently and in a possibly more cost and time effective manner using cloud technology. High-throughput genomics leads to reams of data that cannot be processed by local research facility computers at the speed it is generated. This bottleneck has been overcome by the use of cloud computing to store and to process data in real time. Large datasets and applications for image analysis, data mining, protein folding, and gene sequencing can also be shared for collaborative research between facilities using clouds. This is a simpler approach than transferring such data. Clouds also provide applications such as Apache's Hadoop and Google's MapReduce for parallel computing (particularly useful for making services such as BLAST easier to deploy in a shorter time). Cloud computing with characteristics like multi-tenacity, scalability, low cost, virtualization, agility and empowerment of end users has made it possible to share the results of studies at much finer granularity, along with sharing scientific investigations at the level of individual data points. Cloud technology can contribute significantly in areas such as biodiversity informatics and the study of human genetic variation and disease. We discuss these possibilities, the challenges in realizing the potential for application of cloud technology - n biological sciences, and the work being done to overcome these challenges.},
author = {Menon, K and Anala, K and Trupti, S D G and Sood, N},
booktitle = {Cloud Computing Technologies, Applications and Management (ICCCTAM), 2012 International Conference on},
doi = {10.1109/ICCCTAM.2012.6488081},
keywords = {bioinformatics;biotechnology;cloud computing;data},
pages = {102--107},
title = {{Cloud computing: Applications in biological research and future prospects}},
year = {2012}
}
@inproceedings{1199350,
author = {Hastings, S and Kurc, T and Langella, S and Catalyurek, U and Pan, T and Saltz, J},
booktitle = {Cluster Computing and the Grid, 2003. Proceedings. CCGrid 2003. 3rd IEEE/ACM International Symposium on},
doi = {10.1109/CCGRID.2003.1199350},
keywords = {data visualisation;grid computing;image registrati},
month = {may},
pages = {36--43},
title = {{Image processing for the grid: a toolkit for building grid-enabled image processing applications}},
year = {2003}
}
@inproceedings{6253584,
author = {Vijayakumar, S and Bhargavi, A and Praseeda, U and Ahamed, S A},
booktitle = {Cloud Computing (CLOUD), 2012 IEEE 5th International Conference on},
doi = {10.1109/CLOUD.2012.34},
issn = {2159-6182},
keywords = {DNA;RNA;bioinformatics;cloud computing;grid comput},
pages = {819--827},
title = {{Optimizing Sequence Alignment in Cloud Using Hadoop and MPP Database}},
year = {2012}
}
@inproceedings{6269726,
author = {Karuna, K and Mangala, N and Janaki, C and Shashi, S and Subrata, C},
booktitle = {Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE), 2012 IEEE 21st International Workshop on},
doi = {10.1109/WETICE.2012.51},
issn = {1524-4547},
keywords = {bioinformatics;grid computing;medical computing;mi},
pages = {194--196},
title = {{Galaxy Workflow Integration on Garuda Grid}},
year = {2012}
}
@inproceedings{5687653,
abstract = {Life sciences make heavily use of the web for proving data and analysis tools. There are more than one thousand molecular biology databases characterized by their complexity and higher level of interdependence. Cloud computing has been successfully applied in many fields where managing the data is the major hurdle. It offers an abstraction layer that enables an integrated access to processing, storing and virtualization. In this paper we review the current organization of molecular biology databases and the main technologies for cloud computing. Finally we discuss the main opportunities and challenges of applying bioinformatics to cloud computing.},
author = {Arrais, J P and Oliveira, J L},
booktitle = {Information Technology and Applications in Biomedicine (ITAB), 2010 10th IEEE International Conference on},
doi = {10.1109/ITAB.2010.5687653},
keywords = {bioinformatics;cloud computing;database management},
month = {nov},
pages = {1--4},
title = {{On the exploitation of cloud computing in bioinformatics}},
year = {2010}
}
@inproceedings{Lagar-Cavilla:2009:SRV:1519065.1519067,
address = {New York, NY, USA},
author = {Lagar-Cavilla, Horacio Andr{\'{e}}s and Whitney, Joseph Andrew and Scannell, Adin Matthew and Patchin, Philip and Rumble, Stephen M and de Lara, Eyal and Brudno, Michael and Satyanarayanan, Mahadev},
booktitle = {Proceedings of the 4th ACM European Conference on Computer Systems},
doi = {10.1145/1519065.1519067},
file = {::},
isbn = {978-1-60558-482-9},
keywords = {Cloud,bioinformatics,cloud computing,virtualization},
mendeley-tags = {Cloud,bioinformatics},
pages = {1--12},
publisher = {ACM},
series = {EuroSys '09},
title = {{SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing}},
url = {http://doi.acm.org/10.1145/1519065.1519067},
year = {2009}
}
@inproceedings{5958990,
abstract = {Distributed system resources have become prevalent in ICT departments to lessen the burden of huge expenses incurred by very expensive storage computer systems. Add to this the continuous introduction and ever-growing evolution of simple to complex applications, the demand to access huge quantities of data, intensive computations, powerful simulations, maintaining and offering system resources and middleware infrastructure services the need to do all of this at an affordable and reasonable price is crucial. Distributed grid and cloud computing resources are currently considered to be one of the best technology options to provide this. They have many similar features and functions, and both of them are classed as distributed systems. They are capable of offering unaffordable resources and services at a reasonable price in a mass marketplace. The big question is: what is a reasonable price? How is pricing modeled and on what kind of economic principles is it based? Much of the issues surrounding these questions are very complex in themselves. This paper provides a comparative review of grid and cloud computing economic and pricing models from which appropriate tariffs and charging models can be chosen to meet particular business objectives. The actual choice depends on many other factors like enterprise regulations, tax laws, service level agreements and return on investments, are very important but outside the scope of this paper. In this paper we give the basic core principles and a comparative review of the latest and most appropriate economic and pricing models applicable to grid and cloud computing in order to propose better models for the future.},
author = {Samimi, P and Patel, A},
booktitle = {Computers Informatics (ISCI), 2011 IEEE Symposium on},
doi = {10.1109/ISCI.2011.5958990},
keywords = {Biological system modeling,Cloud computing,Computational modeling,Datos,Economics,Grid computing,ICT departments,Pricing,Software,charging models,distributed cloud computing resources,distributed grid computing resources,distributed processing,distributed system resources,economic models,enterprise regulations,importante,middleware,middleware infrastructure services,pricing models,return on investments,service level agreement (SLA),service level agreements,socio-economic effects,storage computer systems,tax laws},
mendeley-tags = {Datos,importante},
pages = {634--639},
title = {{Review of pricing models for grid amp; cloud computing}},
year = {2011}
}
@article{Liu2010a,
author = {Liu, Yuelan and Liu, Xiaoming and Yang, Lu},
doi = {10.1109/ICIME.2010.5477628},
file = {::},
isbn = {978-1-4244-5263-7},
journal = {2010 2nd IEEE International Conference on Information Management and Engineering},
keywords = {- middleware,bioinformatics,ontology,xml},
pages = {272--275},
publisher = {Ieee},
title = {{Analysis and design of heterogeneous bioinformatics database integration system based on middleware}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5477628},
year = {2010}
}
@article{5638146,
author = {Chatziioannou, A A and Kanaris, I and Doukas, C and Moulos, P and Kolisis, F N and Maglogiannis, I},
doi = {10.1109/TITB.2010.2092784},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Genetic;Gene Expression Profiling;Humans;Internet,biological techniques;biology computing;data analy},
number = {1},
pages = {83--92},
title = {{GRISSOM Platform: Enabling Distributed Processing and Management of Biological Data Through Fusion of Grid and Web Technologies}},
volume = {15},
year = {2011}
}
@article{6362453,
author = {Li, X and Yang, D and Liu, X and Wu, X.-M.},
doi = {10.1109/MCAS.2012.2221521},
issn = {1531-636X},
journal = {Circuits and Systems Magazine, IEEE},
keywords = {Internet;complex networks;electrocardiography;larg},
number = {4},
pages = {33--46},
title = {{Bridging Time Series Dynamics and Complex Network Theory with Application to Electrocardiogram Analysis}},
volume = {12},
year = {2012}
}
@inproceedings{6061080,
abstract = {The scheduling of workflow applications involves the mapping of individual workflow tasks to computational resources, based on a range of functional and non-functional quality of service requirements. Workflow applications such as scientific workflows often require extensive computational processing and generate significant amounts of experimental data. The emergence of cloud computing has introduced a utility-type market model, where computational resources of varying capacities can be procured on demand, in a pay-per-use fashion. In workflow based applications dependencies exist amongst tasks which requires the generation of schedules in accordance with defined precedence constraints. These constraints pose a difficult planning problem, where tasks must be scheduled for execution only once all their parent tasks have completed. In general the two most important objectives of workflow schedulers are the minimisation of both cost and make span. The cost of workflow execution consists of both computational costs incurred from processing individual tasks, and data transmission costs. With scientific workflows potentially large amounts of data must be transferred between compute and storage sites. This paper proposes a novel cloud workflow scheduling approach which employs a Markov Decision Process to optimally guide the workflow execution process depending on environmental state. In addition the system employs a genetic algorithm to evolve workflow schedules. The overall architecture is presented, and initial results indicate the potential of this approach for developing viable workflow schedules on the Cloud.},
author = {Barrett, E and Howley, E and Duggan, J},
booktitle = {Web Services (ECOWS), 2011 Ninth IEEE European Conference on},
doi = {10.1109/ECOWS.2011.27},
keywords = {Markov processes;cloud computing;genetic algorithm},
month = {sep},
pages = {83--90},
title = {{A Learning Architecture for Scheduling Workflow Applications in the Cloud}},
year = {2011}
}
@article{Eltabakh,
author = {Eltabakh, Mohamed Y and Lafayette, West},
file = {::},
title = {{bdbms – A Database Management System for Biological Data}}
}
@inproceedings{6189638,
abstract = {Biodiversity is commonly used to measure the degree of variation of life forms and health conditions within an area or ecosystem. However, reckless human behaviors and dramatic climate changes inevitably cause species extinction. It is therefore important and urgent to establish genetic databases worldwide for preserving precious genome sequences of diversified organisms and perhaps restoration for endangered species. To integrate such abundant genetic data, it requires a robust, stable, and secure mechanism for reliable data storage and retrieval. Here, we have constructed a Taiwan biodiversity genetic database within a cloud environment. The database contains Taiwan related gene sequences from NCBI and local researchers, and it is daily updated for data synchronization. We also collected genome data of various model species from Ensembl for cross-species comparison and functional annotation. To realize the proposed database for such extremely large amount of genome data, cloud computing techniques indeed provide an adequate solution and a mature environment for the exponentially increasing genetic sequences. Besides, important issues for constructing a biodiversity genome database such as data integrity, confidentiality efficiency, availability, and reliability are concerned and discussed in this paper.},
author = {Lin, Han-Yu and Chen, Chien-Ming and Chen, Yu-Lun and Chang, Ya-Hui and Pai, Tun-Wen},
booktitle = {Biometrics and Security Technologies (ISBAST), 2012 International Symposium on},
doi = {10.1109/ISBAST.2012.10},
keywords = {biology computing;cloud computing;data handling;da},
pages = {108--112},
title = {{Construction of a Taiwan Biodiversity Genetic Database in Cloud Environments}},
year = {2012}
}
@article{journals/bmcbi/WallKFPPT10,
author = {Wall, Dennis P and Kudtarkar, Parul and Fusaro, Vincent A and Pivovarov, Rimma and Patil, Prasad and Tonellato, Peter J},
file = {::},
journal = {BMC Bioinformatics},
keywords = {Cloud,bioinformatics,dblp},
mendeley-tags = {Cloud,bioinformatics},
pages = {259},
title = {{Cloud computing for comparative genomics.}},
url = {http://dblp.uni-trier.de/db/journals/bmcbi/bmcbi11.html#WallKFPPT10},
volume = {11},
year = {2010}
}
@inproceedings{6507500,
abstract = {Smith-Waterman (SW) is a popular application in Bioinformatics which calculates the best score/alignment between two genomic sequences. Even though SW provides the best result, it is not widely used in genome projects due to huge requirements in computing power and memory space. Recently, Cloud Computing has been receiving a lot of attention since it is able to provide utility computing in an elastic environment. The advantages of Cloud Computing can be obtained at zero cost since many of the Public Clouds provide free usage slots, allowing users to run their applications for free in Cloud environments. Also, many Clouds can be put together and seen as a unique environment, creating Federated Clouds. In this paper, we propose and evaluate an approach to implement the SW algorithm in Federated Clouds. A hierarchical Multi-Cloud architecture is proposed which is able to transparently connect and manage several Clouds. The results obtained with our architecture and our MapReduce SW implementation in five Public Clouds show that, only by using the free quota, we were able to run the SW application over a huge genomic database in time that is comparable with the one obtained in multicore clusters, showing the appropriateness of our approach.},
author = {Leite, A F and de Melo, A.C.M.A.},
booktitle = {High Performance Computing (HiPC), 2012 19th International Conference on},
doi = {10.1109/HiPC.2012.6507500},
keywords = {bioinformatics;cloud computing;genomics;MapReduce;},
pages = {1--9},
title = {{Executing a biological sequence comparison application on a federated cloud environment}},
year = {2012}
}
@inproceedings{5566469,
author = {Zhao, Wuqing and Xu, Xianbin and Wang, Zhuowei and Zhang, Yuping and He, Shuibing},
booktitle = {Internet Technology and Applications, 2010 International Conference on},
doi = {10.1109/ITAPP.2010.5566469},
keywords = {Internet;grid computing;replicated databases;Inter},
pages = {1--4},
title = {{A Dynamic Optimal Replication Strategy in Data Grid Environment}},
year = {2010}
}
@inproceedings{6391814,
author = {Koehler, M and Knight, R and Benkner, S and Kaniovskyi, Y and Wood, S},
booktitle = {Semantics, Knowledge and Grids (SKG), 2012 Eighth International Conference on},
doi = {10.1109/SKG.2012.51},
keywords = {biological techniques;cloud computing;data integra},
month = {oct},
pages = {80--87},
title = {{The VPH-Share Data Management Platform: Enabling Collaborative Data Management for the Virtual Physiological Human Community}},
year = {2012}
}
@inproceedings{1467745,
author = {Jithesh, P V and Kelly, N and Donachy, P and Harmer, T and Perrott, R and McCurley, M and Townsley, M and Johnston, J and McKee, S},
booktitle = {Computer-Based Medical Systems, 2005. Proceedings. 18th IEEE Symposium on},
doi = {10.1109/CBMS.2005.57},
issn = {1063-7125},
keywords = {biochemistry;biology computing;drugs;genetics;grid},
pages = {523--528},
title = {{GeneGrid: grid based solution for bioinformatics application integration and experiment execution}},
year = {2005}
}
@inproceedings{5981072,
author = {Liu, Jinsong and Wang, Wei and Xiang, Haibing},
booktitle = {Geoinformatics, 2011 19th International Conference on},
doi = {10.1109/GeoInformatics.2011.5981072},
issn = {2161-024X},
keywords = {cartography;database management systems;demography},
pages = {1--4},
title = {{The computational model of multi-scale population density}},
year = {2011}
}
@inproceedings{6327448,
author = {Santos, G and Pinto, T and Morais, H and Vale, Z and Praca, I},
booktitle = {Database and Expert Systems Applications (DEXA), 2012 23rd International Workshop on},
doi = {10.1109/DEXA.2012.78},
issn = {1529-4188},
keywords = {multi-agent systems;power engineering computing;po},
month = {sep},
pages = {331--335},
title = {{Multi-agent Simulation of Continental, Regional, and Micro Electricity Markets}},
year = {2012}
}
@inproceedings{6732620,
abstract = {Task scheduling in a federated cloud environment is a complex problem since there are several cloud providers presenting distinct memory and storage capacities that should be addressed. This article focus on the task scheduling problem in BioNimbuZ, a federated cloud infrastructure for executing bioinformatics applications, which was previously proposed by our group. We present a scheduling algorithm based on Load Balancing Ant Colony (LBACO), called ACOsched, to perform efficient distribution of tasks by finding the best cloud in the federation to execute these tasks. We developed experiments using real biological data, executing the Bowtie mapping tool on one instance of BioNimbuZ, composed by two cloud providers, Amazon EC2 and a bioinformatics laboratory at the University of Brasilia/Brazil. The obtained results show that ACOsched led to a significant improvement in the makespan time of Bowtie executing in BioNimbuZ, when compared to the simple round robin algorithm called DynamicAHP, previously developed in this federated cloud infrastrucutre.},
author = {de Oliveira, Gabriel S S and Ribeiro, Edward and Ferreira, Diogo A and Araujo, Aleteia P. F. and Holanda, Maristela T and Walter, Maria Emilia M. T.},
booktitle = {2013 IEEE International Conference on Bioinformatics and Biomedicine},
doi = {10.1109/BIBM.2013.6732620},
isbn = {978-1-4799-1309-1},
keywords = {ACOsched,Amazon EC2,BioNimbuZ,Bioinformatics,Bowtie mapping tool,Equations,Heuristic algorithms,LBACO,Mathematical model,Scheduling,Scheduling algorithms,Servers,ant colony optimisation,bioinformatics laboratory,biological data,cloud computing,federated cloud infrastructure,load balancing ant colony,makespan time,memory capacities,resource allocation,storage capacities,task scheduling},
month = {dec},
pages = {8--14},
publisher = {IEEE},
title = {{ACOsched: A scheduling algorithm in a federated cloud infrastructure for bioinformatics applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6732620},
year = {2013}
}
@article{journals/bmcbi/QiuEGCBLZWREHF10,
author = {Qiu, Judy and Ekanayake, Jaliya and Gunarathne, Thilina and Choi, Jong Youl and Bae, Seung-Hee and Li, Hui and Zhang, Bingjing and Wu, Tak-Lon and Ruan, Yang and Ekanayake, Saliya and Hughes, Adam and Fox, Geoffrey},
file = {::},
journal = {BMC Bioinformatics},
keywords = {Cloud,bioinformatics,dblp},
mendeley-tags = {Cloud,bioinformatics},
number = {S-12},
pages = {S3},
title = {{Hybrid cloud and cluster computing paradigms for life science applications.}},
url = {http://dblp.uni-trier.de/db/journals/bmcbi/bmcbi11S.html#QiuEGCBLZWREHF10},
volume = {11},
year = {2010}
}
@inproceedings{6409084,
author = {Edla, D R and Jana, P K},
booktitle = {Information and Communication Technologies (WICT), 2012 World Congress on},
doi = {10.1109/WICT.2012.6409084},
keywords = {computational complexity;data mining;pattern class},
month = {oct},
pages = {254--259},
title = {{A grid clustering algorithm using cluster boundaries}},
year = {2012}
}
@inproceedings{5567998,
author = {Zhang, Ping and Xie, Kunqing and Ma, Xiujun and Li, Xiong and Sun, Yixian},
booktitle = {Geoinformatics, 2010 18th International Conference on},
doi = {10.1109/GEOINFORMATICS.2010.5567998},
keywords = {data handling;grid computing;software agents;visua},
pages = {1--5},
title = {{A replication strategy based on swarm intelligence in spatial data grid}},
year = {2010}
}
@article{4358901,
author = {Ahmed, W M and Lenz, D and Liu, Jia and Robinson, J Paul and Ghafoor, A},
doi = {10.1109/TITB.2007.904153},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Automated;Programming Languages;Radiology Informa,Computer-Assisted;Information Dissemination;Infor,Theoretical;Pattern Recognition,XML;biological techniques;biology computing;grid c},
number = {2},
pages = {226--240},
title = {{XML-Based Data Model and Architecture for a Knowledge-Based Grid-Enabled Problem-Solving Environment for High-Throughput Biological Imaging}},
volume = {12},
year = {2008}
}
@inproceedings{1210001,
author = {Chen, Huajun and Wu, Zhaohui and Huang, Chang and Xu, Jiefeng},
booktitle = {Challenges of Large Applications in Distributed Environments, 2003. Proceedings of the International Workshop on},
doi = {10.1109/CLADE.2003.1210001},
keywords = {Biological materials;Computer architecture;Compute},
pages = {62--69},
title = {{Towards a grid-based architecture for traditional Chinese medicine}},
year = {2003}
}
@inproceedings{1558554,
author = {Aloisio, G and Cafaro, M and Epicoco, I and Fiore, S and Mirto, M},
booktitle = {Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on},
doi = {10.1109/CCGRID.2005.1558554},
keywords = {grid computing;medical information systems;ontolog},
month = {may},
pages = {196--203 Vol. 1},
title = {{A semantic grid-based data access and integration service for bioinformatics}},
volume = {1},
year = {2005}
}
@inproceedings{1630930,
author = {Konagaya, A},
booktitle = {Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
doi = {10.1109/CCGRID.2006.1630930},
keywords = {Internet;biology computing;grid computing;ontologi},
month = {may},
pages = {6 pp.--37},
title = {{OBIGrid: towards the 'Ba' for sharing resources, services and knowledge for bioinformatics}},
volume = {2},
year = {2006}
}
@inproceedings{1199420,
author = {Fuhrmann, T and Schafferhans, A and Etzold, T},
booktitle = {Cluster Computing and the Grid, 2003. Proceedings. CCGrid 2003. 3rd IEEE/ACM International Symposium on},
doi = {10.1109/CCGRID.2003.1199420},
keywords = {biology computing;query processing;replicated data},
month = {may},
pages = {601--605},
title = {{An overlay-network approach for distributed access to SRS}},
year = {2003}
}
@inproceedings{4031483,
author = {Qian-mu, Li and Man-wu, Xu and Hong, Zhang},
booktitle = {Grid and Cooperative Computing, 2006. GCC 2006. Fifth International Conference},
doi = {10.1109/GCC.2006.17},
keywords = {grid computing;program diagnostics;bionics;event d},
month = {oct},
pages = {369--373},
title = {{A Root-fault Detection System of Grid Based on Immunology}},
year = {2006}
}
@article{4220640,
author = {Salzemann, J and Jacq, N and Breton, V},
doi = {10.1109/TNB.2007.897468},
issn = {1536-1241},
journal = {NanoBioscience, IEEE Transactions on},
keywords = {Factual;France;Information Storage and Retrieval;,Internet;biology computing;grid computing;middlewa},
number = {2},
pages = {131--135},
title = {{Replication and Update of Molecular Biology Databases}},
volume = {6},
year = {2007}
}
@inproceedings{5759331,
author = {Hamou, R M and Lehireche, A and Lokbani, A C and Rahmani, M},
booktitle = {Cryptography and Network Security, Data Mining and Knowledge Discovery, E-Commerce Its Applications and Embedded Systems (CDEE), 2010 First ACIS International Symposium on},
doi = {10.1109/CDEE.2010.60},
keywords = {cellular automata;indexing;parallel processing;pat},
month = {oct},
pages = {271--277},
title = {{Text Clustering by 2D Cellular Automata Based on the N-Grams}},
year = {2010}
}
@article{journals/bioinformatics/BatemanW09,
abstract = {We are used to having huge datasets pouring out of high-throughput genome centres, but with the advent of ultra high-throughput sequencing, genotyping and other functional genomics in every laboratory we are facing a scary new era in petabyte scale data. For example, the 1000 genomes' projects will probably produce about 1 Tb of finished data. To process data, this project required about 100 Tbs of scratch disk. Working at this level, real technical limitations start to hamper progress. One has to consider storage, but not just having enough, but making sure its available to your compute (network), that you have sufficient I/O to do anything in real time. Software language and implementation become critical factors when dealing with terabytes of data. With such high-intensity computing, power (getting enough), cooling, etc. become real issues.},
author = {Bateman, Alex and Wood, Matt},
file = {::},
journal = {Bioinformatics},
keywords = {Cloud,bioinformatics,dblp},
mendeley-tags = {Cloud,bioinformatics},
number = {12},
pages = {1475},
title = {{Cloud computing.}},
url = {http://dblp.uni-trier.de/db/journals/bioinformatics/bioinformatics25.html#BatemanW09},
volume = {25},
year = {2009}
}
@inproceedings{4536484,
author = {Bertis, V and Bolze, R and Desprez, F and Reed, K},
booktitle = {Parallel and Distributed Processing, 2008. IPDPS 2008. IEEE International Symposium on},
doi = {10.1109/IPDPS.2008.4536484},
issn = {1530-2075},
keywords = {biology computing;grid computing;scientific inform},
pages = {1--8},
title = {{Large scale execution of a bioinformatic application on a volunteer grid}},
year = {2008}
}
@article{Wang2013,
author = {Wang, May Dongmei},
doi = {10.1109/RBME.2012.2228311},
file = {::},
issn = {1941-1189},
journal = {IEEE reviews in biomedical engineering},
keywords = {Computational Biology,High-Throughput Nucleotide Sequencing,Humans},
month = {jan},
pages = {3--8},
pmid = {23192635},
title = {{In the spotlight: Bioinformatics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23192635},
volume = {6},
year = {2013}
}
@article{Niemenmaa15032012,
abstract = {Summary: Hadoop-BAM is a novel library for the scalable manipulation of aligned next-generation sequencing data in the Hadoop distributed computing framework. It acts as an integration layer between analysis applications and BAM files that are processed using Hadoop. Hadoop-BAM solves the issues related to BAM data access by presenting a convenient API for implementing map and reduce functions that can directly operate on BAM records. It builds on top of the Picard SAM JDK, so tools that rely on the Picard API are expected to be easily convertible to support large-scale distributed processing. In this article we demonstrate the use of Hadoop-BAM by building a coverage summarizing tool for the Chipster genome browser. Our results show that Hadoop offers good scalability, and one should avoid moving data in and out of Hadoop between analysis steps.Availability: Available under the open-source MIT license at http://sourceforge.net/projects/hadoop-bam/Contact: matti.niemenmaa@aalto.fiSupplementary information: Supplementary material is available at Bioinformatics online.},
author = {Niemenmaa, Matti and Kallio, Aleksi and Schumacher, Andr{\'{e}} and Klemel{\"{a}}, Petri and Korpelainen, Eija and Heljanko, Keijo},
doi = {10.1093/bioinformatics/bts054},
file = {::},
journal = {Bioinformatics},
keywords = {Cloud,bioinformatics},
mendeley-tags = {Cloud,bioinformatics},
number = {6},
pages = {876--877},
title = {{Hadoop-BAM: directly manipulating next generation sequencing data in the cloud}},
url = {http://bioinformatics.oxfordjournals.org/content/28/6/876.abstract},
volume = {28},
year = {2012}
}
@article{4463822,
author = {Huang, C.-H. and Konagaya, A and Lanza, V and Sloot, P M A},
doi = {10.1109/TITB.2008.918585},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Bioinformatics;Biological system modeling;Biology,Biological,Factual;Internet;Models},
number = {2},
pages = {133--137},
title = {{Guest Editorial Introduction to the Special Section on BioGrid: Biomedical Computations on the Grid}},
volume = {12},
year = {2008}
}
@inproceedings{1503151,
author = {Yang, Chao-Tung and Hsiung, Yi-Chun and Kan, Heng-Chuan},
booktitle = {Information Technology: Research and Education, 2005. ITRE 2005. 3rd International Conference on},
doi = {10.1109/ITRE.2005.1503151},
keywords = {Java;XML;biology computing;database management sys},
pages = {398--402},
title = {{Implementation of a biology data translation system on grid environments}},
year = {2005}
}
@article{4358919,
author = {Singh, A and Chen, Chen and Liu, Weiguo and Mitchell, W and Schmidt, B},
doi = {10.1109/TITB.2007.908462},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {DNA;User-Computer Interface,DNA;biology computing;evolution (biological);genet,Genetic;Genomics;Information Dissemination;Inform},
number = {2},
pages = {218--225},
title = {{A Hybrid Computational Grid Architecture for Comparative Genomics}},
volume = {12},
year = {2008}
}
@article{Armbrust:2010:VCC:1721654.1721672,
address = {New York, NY, USA},
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
doi = {10.1145/1721654.1721672},
issn = {0001-0782},
journal = {Commun. ACM},
number = {4},
pages = {50--58},
publisher = {ACM},
title = {{A View of Cloud Computing}},
url = {http://doi.acm.org/10.1145/1721654.1721672},
volume = {53},
year = {2010}
}
@article{cloudspringer,
author = {Truong, Hong-Linh and Dustdar, Schahram},
doi = {10.1007/s00607-010-0120-1},
issn = {0010-485X},
journal = {Computing},
keywords = {Cloud computing; Computational science and enginee},
number = {1},
pages = {75--91},
publisher = {Springer Vienna},
title = {{Cloud computing for small research groups in computational science and engineering: current status and outlook}},
url = {http://dx.doi.org/10.1007/s00607-010-0120-1},
volume = {91},
year = {2011}
}
@inproceedings{4125806,
author = {Qin, Biao and Wang, Shan and Du, Xiaoyong},
booktitle = {Semantics, Knowledge and Grid, 2005. SKG '05. First International Conference on},
doi = {10.1109/SKG.2005.68},
keywords = {cache storage;distributed databases;peer-to-peer c},
month = {nov},
pages = {18},
title = {{Effective Maintenance of Materialized Views in Peer Data Management Systems}},
year = {2005}
}
@inproceedings{6603714,
abstract = {In a society characterized by aging population and economical crisis it is desirable to reduce the costs of public healthcare systems. It is increasingly necessary to streamline the health system resources leading to the development of new medical services such as telemedicine, monitoring of chronic patients, personalized health services, creating new services for dependants, etc. Those new application and services will significantly increasing the volume of health information to manage, including data from medical and biological sensors, contextual information, health records, reference information, etc., which in turn requires the availability of health applications anywhere, at any time and also access to medical information must be pervasive and mobile. In this paper we propose one potential solution for creating those new services based on cloud computing and vital signs sensors.},
author = {{Gachet Paez}, D and Aparicio, F and {De Buenaga}, M and Rubio, M},
booktitle = {Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS), 2013 Seventh International Conference on},
doi = {10.1109/IMIS.2013.81},
keywords = {ageing;assisted living;biosensors;cloud computing;},
pages = {451--455},
title = {{Highly Personalized Health Services Using Cloud and Sensors}},
year = {2013}
}
@inproceedings{5575891,
abstract = {Meta-scheduling is proven to be an essential tool for resource management in cross-domain environments like Grids. This paper takes one step further in combining meta-scheduling with application management tool to present a novel multi-domain cloud environment for biological research. With the scalable architecture and stable performance, Community Scheduler Framework (CSF) is a mature meta-scheduler for traditional Grids. However, as a scheduler, CSF does not support flexible application management. In this paper, we integrate CSF with another useful tool OPAL, which is a toolkit for wrapping scientific application as web services, to take advantage of both grid and cloud resources. We first talk about the integration of CSF4 with Opal to form a user-transparent and configurable scientific cloud platform. Next, some enhancements of CSF4 are presented to improve the usability of the OPAL-CSF system. In addition, we will describe a virtual screening web service in OPAL-CSF system. The virtual screening service compares multiple receptor/ligand protein pairs using Autodock. Performance analysis of this service shows that the integration of Opal and CSF can efficiently facilitate large-scale scientific applications.},
author = {Wei, Xiaohui and Xi, Lixian and Li, Hongliang and Wang, Haibin and Li, W and Ren, Jingyuan},
booktitle = {Frontier of Computer Science and Technology (FCST), 2010 Fifth International Conference on},
doi = {10.1109/FCST.2010.77},
keywords = {Internet;Web services;biology computing;grid compu},
pages = {414--419},
title = {{A Case of Meta-scheduling for Biological Researches in Cloud Environment}},
year = {2010}
}
@inproceedings{6629243,
abstract = {Scientific workflows have been used as a programming model to automate scientific tasks ranging from short pipelines to complex workflows that span across heterogeneous data and computing resources. While utilization of scientific workflow technologies varies slightly across different scientific disciplines, all informatics and computational science disciplines provide a common set of attributes to facilitate and accelerate workflow-driven research. Scientific workflows provide assembly of complex processing easily in local or distributed environments via rich and expressive programming models. Scientific workflows enable transparent access to diverse resources ranging from local clusters and traditional supercomputers to elastic and heterogeneous Cloud resources. Scientific workflows support incorporation of multiple software tools including domain specific tools for standard processing to custom generalized workflows and middleware tools that can be reused in various contexts. Scientific workflows often collect provenance information on workflow entities, e.g., workflow definitions, their executions and run time parameters, and, in turn, assure a level of reproducibility while enabling referencing and replicating results. While doing all these, scientific workflows often foster an open-source, open-access and standards-driven community development model based on sharing and collaborations. Cyberinfrastructure platforms and gateways commonly employ scientific workflows to bridge the gap between the infrastructure and users needs. While capturing and communicating the scientific process formally, workflows ensure flexibility, synergy between users, provide optimized usage of resources, increase reuse and ensure compliance with system specific data models and community-driven standards. Currently, scientific workflows are used widely in life sciences at different stages of end-to-end data lifecycle from generation to analysis and publication of biological data. The - ata handled by such workflows can be produced by sequencers, sensor networks, medical imaging instruments and other heterogeneous resources at significant rates at decreasing costs making the analysis and archival of such data a 'big data' challenge. Additionally, these new biological data resources are making new and exciting research in areas including metagenomics and personalized medicine possible. However, the analysis of big biological data is still very costly requiring new scalable computational models and programming paradigms to be applied to biological analysis. Although, some new paradigms exists for analysis of big data, application of these best practices to life sciences is still in its infancy. Scientific workflows can act as a scaffold and help speed this process up via combination of existing programming models and computational models with the challenges of biological problems as reusable blocks. In this talk, I will talk about such an approach that builds upon distributed data parallel patterns, e.g., MapReduce, and underlying execution engines, e.g., Hadoop, and matches the computational requirements of bioinformatics tools with such patterns and engines. The results of the presented approach is developed as a part of the bioKepler (bioKepler.org) module and can be downloaded to work within the release 2.4 of the Kepler scientific workflow system (kepler-project.org).},
author = {Altintas, I},
booktitle = {Computational Advances in Bio and Medical Sciences (ICCABS), 2013 IEEE 3rd International Conference on},
doi = {10.1109/ICCABS.2013.6629243},
keywords = {bioinformatics;cloud computing;distributed algorit},
pages = {1},
title = {{Workflow-driven programming paradigms for distributed analysis of biological big data}},
year = {2013}
}
@inproceedings{5490228,
author = {Han, Ju and Singh, S and Sun, Lan and Simmons, B and Auer, M and Parvin, B},
booktitle = {Biomedical Imaging: From Nano to Macro, 2010 IEEE International Symposium on},
doi = {10.1109/ISBI.2010.5490228},
issn = {1945-7928},
keywords = {Raman spectroscopy;biological techniques;biologica},
pages = {1273--1276},
title = {{Chemical profiling of the plant cellwall through Raman microspectroscopy}},
year = {2010}
}
@article{4014765,
author = {Hunter, P J and Li, W W and McCulloch, A D and Noble, D},
doi = {10.1109/MC.2006.392},
issn = {0018-9162},
journal = {Computer},
keywords = {XML;biology computing;cellular biophysics;database},
month = {nov},
number = {11},
pages = {48--54},
title = {{Multiscale modeling: physiome project standards, tools, and databases}},
volume = {39},
year = {2006}
}
@inproceedings{4402152,
author = {Yan, Shuying and Han, Yanbo and Wang, Jing and Liu, Chen},
booktitle = {e-Business Engineering, 2007. ICEBE 2007. IEEE International Conference on},
doi = {10.1109/ICEBE.2007.91},
keywords = {Web services;automatic validity checking;explorato},
month = {oct},
pages = {581--588},
title = {{"Service Hyperlink" for Exploratory Service Composition}},
year = {2007}
}
@inproceedings{5708501,
abstract = {The utility computing model introduced by cloud computing combined with the rich set of cloud infrastructure services offers a very viable alternative to traditional servers and computing clusters. MapReduce distributed data processing architecture has become the weapon of choice for data-intensive analyses in the clouds and in commodity clusters due to its excellent fault tolerance features, scalability and the ease of use. Currently, there are several options for using MapReduce in cloud environments, such as using MapReduce as a service, setting up one's own MapReduce cluster on cloud instances, or using specialized cloud MapReduce runtimes that take advantage of cloud infrastructure services. In this paper, we introduce Azure MapReduce, a novel MapReduce runtime built using the Microsoft Azure cloud infrastructure services. Azure MapReduce architecture successfully leverages the high latency, eventually consistent, yet highly scalable Azure infrastructure services to provide an efficient, on demand alternative to traditional MapReduce clusters. Further we evaluate the use and performance of MapReduce frameworks, including Azure MapReduce, in cloud environments for scientific applications using sequence assembly and sequence alignment as use cases.},
author = {Gunarathne, T and Wu, Tak-Lon and Qiu, J and Fox, G},
booktitle = {Cloud Computing Technology and Science (CloudCom), 2010 IEEE Second International Conference on},
doi = {10.1109/CloudCom.2010.107},
file = {::},
keywords = {Cloud,bioinformatics,cloud computing,distributed processing,fault toler},
mendeley-tags = {Cloud,bioinformatics},
month = {nov},
pages = {565--572},
title = {{MapReduce in the Clouds for Science}},
year = {2010}
}
@article{4358898,
author = {Tsiknakis, M and Brochhausen, M and Nabrzyski, J and Pucacki, J and Sfakianakis, S G and Potamias, G and Desmedt, C and Kafetzopoulos, D},
doi = {10.1109/TITB.2007.903519},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {cancer;data analysis;data visualisation;distribute},
number = {2},
pages = {205--217},
title = {{A Semantic Grid Infrastructure Enabling Integrated Access and Analysis of Multilevel Biomedical Data in Support of Postgenomic Clinical Trials on Cancer}},
volume = {12},
year = {2008}
}
@article{citeulike:8802720,
author = {Pennisi, Elizabeth},
doi = {10.1126/science.331.6018.666},
file = {::},
issn = {1095-9203},
journal = {Science},
keywords = {Cloud,bioinformatics,genomics,opinion},
mendeley-tags = {Cloud,bioinformatics},
number = {6018},
pages = {666--668},
pmid = {21310981},
publisher = {American Association for the Advancement of Science},
title = {{Will Computers Crash Genomics?}},
url = {http://dx.doi.org/10.1126/science.331.6018.666},
volume = {331},
year = {2011}
}
@inproceedings{6695840,
abstract = {Bioinformatics is in a dilemma that traditional analysis tools work hard on the large-scale data from the high-throughout sequencing. In recent years, the open source Apache Hadoop project, which adopts MapReduce framework and distributed file system, brings bioinformatics researchers opportunities to obtain a scalable, efficient and reliable computing performance on Linux clusters and Cloud Computing Service. In this paper, we present Hadoop-based applications employed in bioinformatics, covering next-generation sequencing and other biological domains. In addition, we discuss obstacles and future works about Hadoop in bioinformatics.},
author = {Li, Xubin and Jiang, Wenrui and Jiang, Yi and Zou, Quan},
booktitle = {2012 7th Open Cirrus Summit},
doi = {10.1109/OCS.2012.40},
isbn = {978-0-7695-4908-8},
keywords = {Assembly,Bioinformatics,Cloud,Cloud computing,Genomics,Graphics processing units,Hadoop,Linux,Linux clusters,MapReduce,MapReduce framework,Next-generation sequencing,Sequential analysis,biological domains,cloud computing service,distributed databases,distributed file system,high-throughout sequencing,large-scale data,open source Apache Hadoop project,parallel programming,public domain software},
month = {jun},
pages = {48--52},
publisher = {IEEE},
title = {{Hadoop Applications in Bioinformatics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6695840},
year = {2012}
}
@article{5611496,
abstract = {Executing large number of independent jobs or jobs comprising of large number of tasks that perform minimal intertask communication is a common requirement in many domains. Various technologies ranging from classic job schedulers to the latest cloud technologies such as MapReduce can be used to execute these "many-tasks” in parallel. In this paper, we present our experience in applying two cloud technologies Apache Hadoop and Microsoft DryadLINQ to two bioinformatics applications with the above characteristics. The applications are a pairwise Alu sequence alignment application and an Expressed Sequence Tag (EST) sequence assembly program. First, we compare the performance of these cloud technologies using the above applications and also compare them with traditional MPI implementation in one application. Next, we analyze the effect of inhomogeneous data on the scheduling mechanisms of the cloud technologies. Finally, we present a comparison of performance of the cloud technologies under virtual and nonvirtual hardware platforms.},
author = {Ekanayake, J and Gunarathne, T and Qiu, J},
doi = {10.1109/TPDS.2010.178},
file = {::},
issn = {1045-9219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Apache Hadoop,Bioinformatics,Clouds,Distributed programming,Instruction sets,MPI implementation,Matrix decomposition,Microsoft DryadLINQ,Pipelines,Programming,Runtime,cloud computing,cloud technologies,expressed sequence tag,intertask communication,job schedulers,message passing,parallel systems,performance,programming paradigms.,scheduling,sequence assembly program},
month = {jun},
number = {6},
pages = {998--1011},
title = {{Cloud Technologies for Bioinformatics Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5611496},
volume = {22},
year = {2011}
}
@inproceedings{1287345,
author = {Yang, Chao-Tung and Kuo, Yu-Lun and Lai, Chuan-Lin},
booktitle = {e-Technology, e-Commerce and e-Service, 2004. EEE '04. 2004 IEEE International Conference on},
doi = {10.1109/EEE.2004.1287345},
keywords = {biology computing;distributed databases;grid compu},
pages = {448--451},
title = {{Design and implementation of a computational grid for bioinformatics}},
year = {2004}
}
@inproceedings{1647660,
author = {McClatchey, R H and Manset, D and Solomonides, A E},
booktitle = {Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on},
doi = {10.1109/CBMS.2006.109},
issn = {1063-7125},
keywords = {biological organs;grid computing;mammography;medic},
pages = {745--750},
title = {{Lessons Learned from MammoGrid for Integrated Biomedical Solutions}},
year = {2006}
}
@inproceedings{4262683,
author = {Zosso, D and Arnold, K and Schwede, T and Podvinec, M},
booktitle = {Computer-Based Medical Systems, 2007. CBMS '07. Twentieth IEEE International Symposium on},
doi = {10.1109/CBMS.2007.103},
issn = {1063-7125},
keywords = {Internet;biochemistry;biology computing;data visua},
pages = {406--411},
title = {{SWISS-TANDEM: A Web-Based Workspace for MS/MS Protein Identification on PC Grids}},
year = {2007}
}
@inproceedings{1630937,
author = {Zhu, Ji and Guo, Ang and Lu, Zhonghua and Wu, Yongwei and Shen, Bin and Chi, Xuebin},
booktitle = {Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
doi = {10.1109/CCGRID.2006.1630937},
keywords = {biology computing;grid computing;middleware;CNGrid},
month = {may},
pages = {7 pp.--44},
title = {{Analysis of the bioinformatics grid technique applications in China}},
volume = {2},
year = {2006}
}
@article{Lawn201157,
author = {Lawn, Stephen D and Zumla, Alimuddin I},
doi = {http://dx.doi.org/10.1016/S0140-6736(10)62173-3},
file = {::},
issn = {0140-6736},
journal = {The Lancet},
keywords = {Cloud,bioinformatics},
mendeley-tags = {Cloud,bioinformatics},
number = {9785},
pages = {57--72},
title = {{Tuberculosis}},
url = {http://www.sciencedirect.com/science/article/pii/S0140673610621733},
volume = {378},
year = {2011}
}
@inproceedings{6495914,
abstract = {Scientific applications are increasingly using cloud resources for their data analysis workflows. However, managing data effectively and efficiently over these cloud resources is challenging due to the myriad storage choices with different performance-cost trade-offs, complex application choices, complexity associated with elasticity and, failure rates. The explosion in scientific data coupled with unique characteristics of cloud environments require a more flexible and robust distributed data management solution than the ones currently in existence. This paper describes the design and implementation of FRIEDA - a Flexible Robust Intelligent Elastic Data Management framework. FRIEDA coordinates data in a transient cloud environment taking into account specific application characteristics. Additionally, we describe a range of data management strategies and show the benefit of flexible data management schemes in cloud environments. We study two distinct scientific applications from bioinformatics and image analysis to understand the effectiveness of such a framework.},
author = {Ghoshal, Devarshi and Ramakrishnan, Lavanya},
booktitle = {2012 SC Companion: High Performance Computing, Networking Storage and Analysis},
doi = {10.1109/SC.Companion.2012.132},
isbn = {978-0-7695-4956-9},
keywords = {Cloud,Data Management,FRIEDA framework,Scheduling,bioinformatics,cloud computing,cloud environment,cloud resource,data analysis,data analysis workflow,distributed data management solution,flexible robust intelligent elastic data managemen,image analysis,specific application characteristics,storage choice},
month = {nov},
pages = {1096--1105},
publisher = {IEEE},
title = {{FRIEDA: Flexible Robust Intelligent Elastic Data Management in Cloud Environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6495914},
year = {2012}
}
@inproceedings{5708454,
abstract = {Systems biology is characterized by a large community of scientists who use a wide variety of fragmented and competing data sets and computational tools of all scales to support their research. In order to provide a more coherent computational environment for systems biology, we are working as part of the Department of Energy Systems Biology Knowledgebase (Kbase) project to define a federated cloud-based system architecture. The Kbase will eventually host massive amounts of biological data, provide high performance and scalable computational resources, and support a large user community with tools and services to enable them to utilize the Kbase resources. In this paper, we describe the results of our investigations into the design of a workflow infrastructure suitable for use in the Kbase. The approach utilizes standards-based workflow description and open source integration technologies, and incorporates a data aware workflow execution layer for exploiting data locality in the federated architecture. We describe a use case and the initial prototype implementation we have built that demonstrates the feasibility of our approach.},
author = {Gorton, I and Liu, Yan and Yin, Jian},
booktitle = {Cloud Computing Technology and Science (CloudCom), 2010 IEEE Second International Conference on},
doi = {10.1109/CloudCom.2010.79},
file = {::},
keywords = {Cloud,bioinformatics,biology computing,cloud computing,knowledge based},
mendeley-tags = {Cloud,bioinformatics},
month = {nov},
pages = {218--225},
title = {{Exploring Architecture Options for a Federated, Cloud-Based System Biology Knowledgebase}},
year = {2010}
}
@inproceedings{1620442,
author = {Boisson, J.-C. and Jourdan, L and Talbi, E and Rolando, C},
booktitle = {Advanced Information Networking and Applications, 2006. AINA 2006. 20th International Conference on},
doi = {10.1109/AINA.2006.48},
issn = {1550-445X},
keywords = {biological techniques;biology computing;evolutiona},
pages = {5 pp.--},
title = {{A preliminary work on evolutionary identification of protein variants and new proteins on grids}},
volume = {2},
year = {2006}
}
@inproceedings{6058962,
abstract = {Cloud computing faces two business-related issues. First, the management of clouds requires not only technical skills but also the understanding of the business side of these systems. Traditional distributed systems research did not require this understanding, simply assuming that all components involved are willing to cooperate according to the resource allocation algorithm prescribed. This cooperation assumption is not valid anymore if clouds are commercial. Secondly, the demand of cloud customers can cause demand spikes at any time, raising the questions on how to allocate the limited resources. Consequently, a real-time view of the provider's business with respect to revenue streams and costs becomes essential for cloud providers. Such a system helps them to respond in an economically efficient way. From the user perspective, the understanding of the different offerings of clouds is also becoming none-trivial, requiring a support in selecting the best-fitting cloud service. A solution to these two issues is the use of business economics. The objective of this work is to lay out a concept and design of a business support service platform, which is called Cloud Management Cockpit (CMC), which uses business economics for giving decision support to providers for managing clouds and for using clouds. The applications of the CMC are laid out in three scenarios. They highlight the usefulness of the CMC business support models and demonstrate how CMC enables platform interoperability, service composition, and reduces complexity of clouds.},
author = {Altmann, J and Hovestadt, M and Kao, Odej},
booktitle = {Networked Computing (INC), 2011 The 7th International Conference on},
keywords = {business data processing;cloud computing;decision},
month = {sep},
pages = {149--154},
title = {{Business support service platform for providers in open cloud computing markets}},
year = {2011}
}
@inproceedings{5634363,
author = {Li, Qin and Martin, K M},
booktitle = {Parallel and Distributed Processing with Applications (ISPA), 2010 International Symposium on},
doi = {10.1109/ISPA.2010.92},
keywords = {grid computing;grid computing;grid marketplace;mar},
month = {sep},
pages = {427--432},
title = {{A Viable Grid Marketplace}},
year = {2010}
}
@article{Rosenthal2010342,
author = {Rosenthal, Arnon and Mork, Peter and Li, Maya Hao and Stanford, Jean and Koester, David and Reynolds, Patti},
doi = {http://dx.doi.org/10.1016/j.jbi.2009.08.014},
file = {::},
issn = {1532-0464},
journal = {Journal of Biomedical Informatics},
keywords = {Bioinformatics,Cloud,Cloud computing,Cost-benefit analysis,Data sharing,Distributed computing,Security,bioinformatics},
mendeley-tags = {Cloud,bioinformatics},
number = {2},
pages = {342--353},
title = {{Cloud computing: A new business paradigm for biomedical information sharing}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046409001154},
volume = {43},
year = {2010}
}
@article{Konig,
author = {K{\"{o}}nig, Matthias},
file = {::},
title = {{Biological Databases}}
}
@inproceedings{1551016,
author = {Aberer, K and Alima, L O and Ghodsi, A and Girdzijauskas, S and Haridi, S and Hauswirth, M},
booktitle = {Peer-to-Peer Computing, 2005. P2P 2005. Fifth IEEE International Conference on},
doi = {10.1109/P2P.2005.38},
keywords = {application program interfaces;data structures;ope},
pages = {11--20},
title = {{The essence of P2P: a reference architecture for overlay networks}},
year = {2005}
}
@inproceedings{4375625,
author = {Tilson, J L and Rendon, G and Ger, Mao-Feng and Jakobsson, E},
booktitle = {Bioinformatics and Bioengineering, 2007. BIBE 2007. Proceedings of the 7th IEEE International Conference on},
doi = {10.1109/BIBE.2007.4375625},
keywords = {biology computing;genetics;grid computing;molecula},
month = {oct},
pages = {620--627},
title = {{MotifNetwork: A Grid-enabled Workflow for High-throughput Domain Analysis of Biological Sequences: Implications for annotation and study of phylogeny, protein interactions, and intraspecies variation}},
year = {2007}
}
@inproceedings{1286589,
author = {Wu, B and Dovey, M and Ng, M H and Tai, K and Murdock, S and Jeffreys, P and Cox, S and Essex, J and Sansom, M S P},
booktitle = {Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004. International Conference on},
doi = {10.1109/ITCC.2004.1286589},
keywords = {Internet;biology computing;database management sys},
pages = {50--54 Vol.2},
title = {{A Web/grid portal implementation of BioSimGrid: a biomolecular simulation database}},
volume = {2},
year = {2004}
}
@inproceedings{6695840,
abstract = {Bioinformatics is in a dilemma that traditional analysis tools work hard on the large-scale data from the high-throughout sequencing. In recent years, the open source Apache Hadoop project, which adopts MapReduce framework and distributed file system, brings bioinformatics researchers opportunities to obtain a scalable, efficient and reliable computing performance on Linux clusters and Cloud Computing Service. In this paper, we present Hadoop-based applications employed in bioinformatics, covering next-generation sequencing and other biological domains. In addition, we discuss obstacles and future works about Hadoop in bioinformatics.},
author = {Xubin, Li and Wenrui, Jiang and Yi, Jiang and Quan, Zou},
booktitle = {Open Cirrus Summit (OCS), 2012 Seventh},
doi = {10.1109/OCS.2012.40},
keywords = {Linux;bioinformatics;cloud computing;distributed d},
pages = {48--52},
title = {{Hadoop Applications in Bioinformatics}},
year = {2012}
}
@inproceedings{6009292,
author = {Taha, K and Elmasri, R},
booktitle = {Services Computing (SCC), 2011 IEEE International Conference on},
doi = {10.1109/SCC.2011.52},
keywords = {bioinformatics;grid computing;middleware;query pro},
pages = {440--447},
title = {{IRTG: A Grid Middleware for Bioinformatics}},
year = {2011}
}
@inproceedings{6747031,
author = {Wang, Xiao-lei and Li, Jiang-yu and Liu, Yang and Wang, Yu-feng and Zhao, Dong-sheng},
booktitle = {Biomedical Engineering and Informatics (BMEI), 2013 6th International Conference on},
doi = {10.1109/BMEI.2013.6747031},
keywords = {Bioinformatics;Galaxy;High-performance Computing;L},
pages = {712--716},
title = {{Building localized bioinformatics platform based on Galaxy and high performance computing cluster}},
year = {2013}
}
@article{CPE:CPE1780,
abstract = {Cloud computing offers exciting new approaches for scientific computing that leverage major commercial players' hardware and software investments in large-scale data centers. Loosely coupled problems are very important in many scientific fields, and with the ongoing move towards data-intensive computing, they are on the rise. There exist several different approaches to leveraging clouds and cloud-oriented data processing frameworks to perform pleasingly parallel (also called embarrassingly parallel) computations. In this paper, we present three pleasingly parallel biomedical applications: (i) assembly of genome fragments; (ii) sequence alignment and similarity search; and (iii) dimension reduction in the analysis of chemical structures, which are implemented utilizing a cloud infrastructure service-based utility computing models of Amazon Web Services (http://Amazon.com Inc., Seattle, WA, USA) and Microsoft Windows Azure (Microsoft Corp., Redmond, WA, USA) as well as utilizing MapReduce-based data processing frameworks Apache Hadoop (Apache Software Foundation, Los Angeles, CA, USA) and Microsoft DryadLINQ. We review and compare each of these frameworks, performing a comparative study among them based on performance, cost, and usability. High latency, eventually consistent cloud infrastructure service-based frameworks that rely on off-the-node cloud storage were able to exhibit performance efficiencies and scalability comparable to the MapReduce-based frameworks with local disk-based storage for the applications considered. In this paper, we also analyze variations in cost among the different platform choices (e.g., Elastic Compute Cloud instance types), highlighting the importance of selecting an appropriate platform based on the nature of the computation. Copyright {\textcopyright} 2011 John Wiley & Sons, Ltd.},
author = {Gunarathne, Thilina and Wu, Tak-Lon and Choi, Jong Youl and Bae, Seung-Hee and Qiu, Judy},
doi = {10.1002/cpe.1780},
file = {::},
issn = {1532-0634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Cloud,bioinformatics,cloud technology,map reduce},
mendeley-tags = {Cloud,bioinformatics},
number = {17},
pages = {2338--2354},
publisher = {John Wiley & Sons, Ltd},
title = {{Cloud computing paradigms for pleasingly parallel biomedical applications}},
url = {http://dx.doi.org/10.1002/cpe.1780},
volume = {23},
year = {2011}
}
@article{Gang2006,
author = {Gang, Chen and Wu, Yongwei and Zheng, Weimin},
doi = {10.1109/GCCW.2006.96},
file = {::},
isbn = {0-7695-2695-0},
journal = {2006 Fifth International Conference on Grid and Cooperative Computing Workshops},
pages = {381--388},
publisher = {Ieee},
title = {{UGE4B: An Universal Grid Environment for Bioinformatics Research}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4031579},
year = {2006}
}
@inproceedings{6578360,
author = {Abid, S and Fnaiech, F and {Ben Braiek}, E},
booktitle = {Electrical Engineering and Software Applications (ICEESA), 2013 International Conference on},
doi = {10.1109/ICEESA.2013.6578360},
keywords = {edge detection;image colour analysis;multilayer pe},
pages = {1--6},
title = {{A novel neural network approach for image edge detection}},
year = {2013}
}
@inproceedings{976649,
abstract = {A primary goal of the Texas Synergy I program was to evaluate the utility of high temporal resolution data acquired at coarse spatial resolution for assessment of regional conditions and early detection of environmental changes. Three sample applications are shown to emphasize the use of EOS Core System (ECS) data for these purposes. (1) A multi-year Normalized Difference Vegetation Index (NDVI) composite of cloud-free sequences over Texas was developed by Center for Space Research (CSR) to demonstrate the severity of the year 2000 drought. The NDVI time series products illustrate precipitation trends qualitatively and highlight the drought's impact on vegetation across the state of Texas. (2) Similarly, the daily Sea-viewing Wide Field-of-view Sensor (SeaWiFS) chlorophyll data product derived by CSR using NASA's SeaWiFS Data Analysis System (SeaDAS) software provided useful information on the extent of a red tide event that developed along the entire Texas coast in the fall of 2000. Several samples of Moderate Resolution Imaging Spectro radiometer (MODIS) data, with their improved spatial resolution and spectral calibration, indicated a potential for low cost monitoring of biological signatures in the ocean. (3) Finally, severe weather in West Texas, exacerbated by the drought conditions of spring 2000, ignited a 47000 acre wildfire in the Glass Mountains. CSR downloaded and processed ECS data imaged during and after the fire event. Several 2D and 3D data products demonstrated the integration of 30-m digital elevation models and GIS data layers with Landsat 7 Enhanced Thematic Mapper Plus (ETM+) imagery. Hot spots were evident in MODIS 3.7-micron imagery while the evolving burnscar was clearly visible in 250-m pixel resolution MODIS imagery},
author = {Tapley, B D and Crawford, M M and Howard, T and Hutchison, K D and Smith, S and Wells, G L},
booktitle = {Geoscience and Remote Sensing Symposium, 2001. IGARSS '01. IEEE 2001 International},
doi = {10.1109/IGARSS.2001.976649},
keywords = {hydrology;remote sensing;terrain mapping;ECS data;},
pages = {824--826 vol.2},
title = {{Application of EOS Core System data and data products for monitoring and mitigating natural disasters}},
volume = {2},
year = {2001}
}
@inproceedings{4272527,
author = {Fan, Hao and Li, Lin},
booktitle = {Bioinformatics and Biomedical Engineering, 2007. ICBBE 2007. The 1st International Conference on},
doi = {10.1109/ICBBE.2007.43},
keywords = {biochemistry;biology computing;data analysis;grid},
pages = {153--156},
title = {{Study on Metadata Applications for Proteomics Data Integration}},
year = {2007}
}
@inproceedings{1630934,
author = {Sulakhe, D and Rodriguez, A and Wilde, M and Foster, I and Maltsev, N},
booktitle = {Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
doi = {10.1109/CCGRID.2006.1630934},
keywords = {biology computing;database management systems;gene},
month = {may},
pages = {7 pp.--41},
title = {{Using multiple grid resources for bioinformatics applications in GADU}},
volume = {2},
year = {2006}
}
@inproceedings{1639529,
author = {Boukerche, A and Sousa, M S and de Melo, A.C.M.A.},
booktitle = {Parallel and Distributed Processing Symposium, 2006. IPDPS 2006. 20th International},
doi = {10.1109/IPDPS.2006.1639529},
keywords = {DNA;biology computing;genetics;grid computing;BLAS},
pages = {8 pp.--},
title = {{A multiple task allocation framework for biological sequence comparison in a grid environment}},
year = {2006}
}
@inproceedings{6530576,
abstract = {Biomolecular network is an important research subject of the bioinformatics. A crucial step in biological studies is to understand the biomolecular networks. Visualization is helpful for complex biomolecular network analysis because the network scale is rapidly expanding along with the growth of biological data sets. A novel biomolecular network visualization tool, BNVC (Biomolecular Network Visualization based on Cytoscape Web) has been developed on CPSE-Bio (Cloud based Problem Solving Environment for Bioinformatics) in this paper. The CPSE-Bio is a Problem-Solving-Environment for bioinformatics based on cloud computing platform. The BNVC, as a web-based application, is designed to visualize and analyze complex biomolecular networks, particularly compare two similar networks, which is the first web-based tool to analysis alignment status of complex biomolecular networks based on Cytoscape Web.},
author = {Jiang, Xie and Ronggui, Yi and Huiran, Zhang and Wu, Zhang and Kawata, S},
booktitle = {Computing and Convergence Technology (ICCCT), 2012 7th International Conference on},
keywords = {bioinformatics;cloud computing;data visualisation;},
pages = {1488--1492},
title = {{Biomolecular network visualization based on CPSE-Bio}},
year = {2012}
}
@inproceedings{4090196,
author = {Gardner, M K and Feng, Wu-chun and Archuleta, J and Lin, Heshan and Ma, Xiaosong},
booktitle = {SC 2006 Conference, Proceedings of the ACM/IEEE},
doi = {10.1109/SC.2006.46},
keywords = {database management systems;genetics;grid computin},
month = {nov},
pages = {22},
title = {{Parallel Genomic Sequence-Searching on an Ad-Hoc Grid: Experiences, Lessons Learned, and Implications}},
year = {2006}
}
@inproceedings{6345730,
author = {Scaglione, A},
booktitle = {Power and Energy Society General Meeting, 2012 IEEE},
doi = {10.1109/PESGM.2012.6345730},
issn = {1944-9925},
keywords = {cellular radio;computer networks;educational cours},
pages = {1},
title = {{A course on smartgrid networks}},
year = {2012}
}
@inproceedings{1227289,
author = {Bourne, P},
booktitle = {Bioinformatics Conference, 2003. CSB 2003. Proceedings of the 2003 IEEE},
doi = {10.1109/CSB.2003.1227289},
keywords = {biology computing;genetics;molecular biophysics;op},
pages = {13--},
title = {{High performance computational biology-past progress and future promise}},
year = {2003}
}
@inproceedings{4019635,
author = {Zhang, Xiaodan and Wu, D D and Zhou, Xiaohua and Hu, Xiaohua},
booktitle = {BioInformatics and BioEngineering, 2006. BIBE 2006. Sixth IEEE Symposium on},
doi = {10.1109/BIBE.2006.253310},
keywords = {biological techniques;biology computing;data minin},
month = {oct},
pages = {12--19},
title = {{A Language Modeling Text Mining Approach to the Annotation of Protein Community}},
year = {2006}
}
@inproceedings{5636125,
abstract = {Cloud computing is a new form of technology, which infrastructure, developing platform, software, and storage can be delivered as a service in a pay as you use cost model. However, for critical business application and more sensitive information, cloud providers must be selected based on high level of trustworthiness. In this paper, we present a trust model to evaluate cloud services in order to help cloud users select the most reliable resources. We integrate our previous work “conceptual SLA framework for cloud computing” with the proposed trust management model to present a new solution of defining the reliable criteria for the selection process of cloud providers.},
author = {Alhamad, M and Dillon, T and Chang, E},
booktitle = {Network-Based Information Systems (NBiS), 2010 13th International Conference on},
doi = {10.1109/NBiS.2010.67},
issn = {2157-0418},
keywords = {Internet;security of data;SLA based trust model;cl},
month = {sep},
pages = {321--324},
title = {{SLA-Based Trust Model for Cloud Computing}},
year = {2010}
}
@inproceedings{5736040,
author = {Lu, Liu and Ke, Zhou},
booktitle = {Electricity Distribution (CICED), 2010 China International Conference on},
keywords = {Monte Carlo methods;graphical user interfaces;load},
month = {sep},
pages = {1--6},
title = {{The research and development of comprehensive evaluation and management system for harmonic and negative-sequence}},
year = {2010}
}
@inproceedings{1558590,
author = {Sulakhe, D and Rodriguez, A and D'Souza, M and Wilde, M and Nefedova, V and Foster, I and Maltsev, N},
booktitle = {Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International Symposium on},
doi = {10.1109/CCGRID.2005.1558590},
keywords = {biology computing;data analysis;genetic engineerin},
month = {may},
pages = {455--462 Vol. 1},
title = {{GNARE: an environment for grid-based high-throughput genome analysis}},
volume = {1},
year = {2005}
}
@article{1501739,
author = {Wang, D and Carr, E and Gross, L J and Berry, M W},
doi = {10.1109/MCSE.2005.104},
issn = {1521-9615},
journal = {Computing in Science Engineering},
keywords = {data visualisation;ecology;environmental science c},
month = {sep},
number = {5},
pages = {44--52},
title = {{Toward ecosystem modeling on computing grids}},
volume = {7},
year = {2005}
}
@inproceedings{6104641,
abstract = {Bioinformatics is a developing interdisciplinary science which combines information technologies into biological researches. The techniques from this emerging field have shown great potential in many business areas including drug design, agriculture, and so on. Meanwhile, this new computational field has also been one of the largest consumers of computational power, as the analyses in bioinformatics are often extremely computationally or data intensive. Although there are already several projects which have done tentative exploration on deploying bioinformatics applications to cloud environments, the deployment is ad-hoc and restricted to a single private cloud environment. Moreover, the complexity of various demands of bench biologists and bioinformaticians also brings new challenges to bioinformatics cloud development. In this paper, we first identify the key participants and their interactions in a public bioinformatics cloud environment, where bioinformatic analyses are consumed as services on top of a cloud infrastructure. After that, we propose a research framework to discuss the domain-specific technical challenges in delivering such a solution. Finally, we summarize the existing related research efforts based on our framework and introduce our ongoing Web Lab project.},
author = {Sun, Xi and Fan, Liya and Yan, Linlin and Kong, Lei and Ding, Yang and Guo, ChangJie and Sun, Wei},
booktitle = {2011 IEEE 8th International Conference on e-Business Engineering},
doi = {10.1109/ICEBE.2011.42},
isbn = {978-1-4577-1404-7},
keywords = {Conferences,Decision support systems,Helium,bioinformatics,bioinformatics services,cloud computing,cloud infrastructure,domain-specific technical challenges,public cloud environment,research framework},
month = {oct},
pages = {352--357},
publisher = {IEEE},
title = {{Deliver Bioinformatics Services in Public Cloud: Challenges and Research Framework}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6104641},
year = {2011}
}
@article{citeulike:7369275,
abstract = {To keep pace with accelerating sequencing machines, genomics researchers clean house and move toward the cloud.},
author = {Baker, Monya},
doi = {10.1038/nmeth0710-495},
file = {::},
issn = {1548-7091},
journal = {Nature Methods},
keywords = {Cloud,bigdata,bioinformatics},
mendeley-tags = {Cloud,bioinformatics},
number = {7},
pages = {495--499},
publisher = {Nature Publishing Group},
title = {{Next-generation sequencing: adjusting to data overload}},
url = {http://dx.doi.org/10.1038/nmeth0710-495},
volume = {7},
year = {2010}
}
@inproceedings{4031576,
author = {Cho, Kyu Cheol and Ma, Yong Beom and Lee, Jong Sik},
booktitle = {Grid and Cooperative Computing Workshops, 2006. GCCW '06. Fifth International Conference on},
doi = {10.1109/GCCW.2006.35},
keywords = {ART neural nets;biology computing;grid computing;p},
month = {oct},
pages = {367--370},
title = {{Design of Computational Grid-based Intelligence ART1 Classification System for Bioinformatics Applications}},
year = {2006}
}
@inproceedings{4100463,
author = {Blanchet, C and Mollon, R and Thain, D and Deleage, G},
booktitle = {Grid Computing, 7th IEEE/ACM International Conference on},
doi = {10.1109/ICGRID.2006.311006},
keywords = {biology computing;data analysis;file organisation;},
month = {sep},
pages = {120--127},
title = {{Grid Deployment of Legacy Bioinformatics Applications with Transparent Data Access}},
year = {2006}
}
@inproceedings{5169410,
author = {Kim, Min-Sung and Hwang, Taeho and Han, Youngwoong and Yi, Gwan-Su},
booktitle = {Computer Science and Information Technology - Spring Conference, 2009. IACSITSC '09. International Association of},
doi = {10.1109/IACSIT-SC.2009.71},
keywords = {grid computing;middleware;SAMS;Semantic Applicatio},
pages = {533--537},
title = {{Application Oriented Grid Job Management System for Sequence Alignment}},
year = {2009}
}
@inproceedings{1630822,
author = {Abramson, D and Lynch, A and Takemiya, H and Tanimura, Y and Date, S and Nakamura, H and Jeong, Karpjoo and Hwang, Suntae and Zhu, Ji and Lu, Zhong-hua and Amoreira, C and Baldridge, K and Lee, Hurng-Chun and Wang, Chi-Wei and Shih, Horng-Liang and Molina, T and Li, W W and Arzberger, P W},
booktitle = {Cluster Computing and the Grid, 2006. CCGRID 06. Sixth IEEE International Symposium on},
doi = {10.1109/CCGRID.2006.30},
keywords = {PRAGMA;applications;deployment;grid;lessons;strate},
month = {may},
pages = {241--248},
title = {{Deploying Scientific Applications to the PRAGMA Grid Testbed: Strategies and Lessons}},
volume = {1},
year = {2006}
}
@inproceedings{4159726,
author = {Hamilton, C H and Rau-Chaplin, A},
booktitle = {Complex, Intelligent and Software Intensive Systems, 2007. CISIS 2007. First International Conference on},
doi = {10.1109/CISIS.2007.16},
keywords = {computational complexity;data handling;grid comput},
pages = {139--146},
title = {{Compact Hilbert Indices for Multi-Dimensional Data}},
year = {2007}
}
@inproceedings{4463144,
author = {Giannakeas, N and Fotiadis, D I and Politou, A S},
booktitle = {Engineering in Medicine and Biology Society, 2006. EMBS '06. 28th Annual International Conference of the IEEE},
doi = {10.1109/IEMBS.2006.260782},
issn = {1557-170X},
keywords = {biological techniques;biology computing;computatio},
pages = {5876--5879},
title = {{An Automated Method for Gridding in Microarray Images}},
year = {2006}
}
@article{Sadedin2012,
abstract = {SUMMARY: Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specializes in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross-platform, making it very easy to adopt and deploy into existing environments. Availability and implementation: Bpipe is freely available from http://bpipe.org under a BSD License.},
author = {Sadedin, Simon P and Pope, Bernard and Oshlack, Alicia},
doi = {10.1093/bioinformatics/bts167},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Computational Biology,Computational Biology: instrumentation,Computational Biology: methods,Programming Languages,Software,Workflow},
month = {jun},
number = {11},
pages = {1525--6},
pmid = {22500002},
title = {{Bpipe: a tool for running and managing bioinformatics pipelines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22500002},
volume = {28},
year = {2012}
}
@inproceedings{5754191,
author = {Arebi, P and Gonbadipoor, N},
booktitle = {Computer Modelling and Simulation (UKSim), 2011 UkSim 13th International Conference on},
doi = {10.1109/UKSIM.2011.25},
keywords = {distributed databases;genetic algorithms;grid comp},
pages = {81--86},
title = {{A Genetic Algorithm for Query Optimization in Database Grid by Dynamic Cost Estimation}},
year = {2011}
}
@article{Eckman,
author = {Eckman, Barbara A and Healthcare, I B M and Gaasterland, Terry and Snyder, Ben and Vidal, Maria Esther},
file = {::},
title = {{Implementing a Bioinformatics Pipeline ( BIP ) on a Mediator Platform : Comparing Cost and Quality of Alternate Choices}}
}
@article{5291722,
author = {Gani, A and Gribok, A V and Lu, Yinghui and Ward, W K and Vigersky, R A and Reifman, J},
doi = {10.1109/TITB.2009.2034141},
issn = {1089-7771},
journal = {Information Technology in Biomedicine, IEEE Transactions on},
keywords = {Ambulatory;Reproducibility of Results;Signal Proc,Biological;Monitoring,Computer-Assisted;Subcutaneous Tissue,Factual;Diabetes Mellitus,Preschool;Databases,Type 1;Diabetes Mellitus,Type 2;Glucose;Humans;Middle Aged;Models,diseases;molecular biophysics;patient diagnosis;pa},
number = {1},
pages = {157--165},
title = {{Universal Glucose Models for Predicting Subcutaneous Glucose Concentration in Humans}},
volume = {14},
year = {2010}
}
@inproceedings{6320187,
author = {Guerbai, Y and Chibani, Y and Abbas, N},
booktitle = {Multimedia Computing and Systems (ICMCS), 2012 International Conference on},
doi = {10.1109/ICMCS.2012.6320187},
keywords = {handwriting recognition;handwritten character reco},
month = {may},
pages = {206--210},
title = {{One-class versus bi-class SVM classifier for off-line signature verification}},
year = {2012}
}
@article{Ping,
author = {Ping, Lim Yun},
file = {::},
title = {{What is a database ? Why databases ?}}
}
@inproceedings{1698291,
booktitle = {Database and Expert Systems Applications, 2006. DEXA '06. 17th International Workshop on},
doi = {10.1109/DEXA.2006.3},
issn = {1529-4188},
keywords = {database management systems;expert systems;P2P dat},
month = {sep},
pages = {i--iii},
title = {{17th International Conference on Database and Expert Systems Applications - Title}},
year = {2006}
}
@inproceedings{1626038,
author = {Kim, Tae-Kyung and Oh, Sang-Keun and Cho, Wan-Sup},
booktitle = {Advanced Communication Technology, 2006. ICACT 2006. The 8th International Conference},
doi = {10.1109/ICACT.2006.206433},
keywords = {Linux;biology computing;cellular biophysics;grid c},
month = {feb},
pages = {6 pp.--2192},
title = {{A LanLinux-based grid system for bioinformatics applications}},
volume = {3},
year = {2006}
}
@inproceedings{6174807,
abstract = {This paper develops a multiobjective evolutionary algorithm on the cloud computing environment to help planners solve multiobjective problems more efficiently and effectively. The cloud environment is emulated as a virtualized biological world with several isolated regions. The main population initially continues evolutionary processes as the most widely-known evolutionary algorithms. To yield both exploration and exploitation, two processes, such as migration and interaction, are deployed. In the process of migration, local optimal solutions can migrate to form new populations so that the search space can be expanded. To overcome the disadvantages in isolated evolutionary algorithms, the individuals in different populations will interact stochastically in the interaction process. Taking the advantage of cloud computing environment, planners can take less effort on deploying both computation power and storage space. Instead, the planners can focus on the design of encoding, crossover, and mutation. Also, it can further applied in various complicated applications more practically.},
author = {Jian, Ming-Shen and Chou, Ta-Yuan},
booktitle = {Advanced Communication Technology (ICACT), 2012 14th International Conference on},
issn = {1738-9445},
keywords = {cloud computing;evolutionary computation;cloud-com},
month = {feb},
pages = {881--886},
title = {{A real-world-like evolutionary algorithm on the cloud-computing environment}},
year = {2012}
}
@inproceedings{6220819,
author = {Hashiesh, Fahd and Mostafa, Hossam E and Helal, I and Mansour, Mohamed M},
booktitle = {Innovative Smart Grid Technologies - Middle East (ISGT Middle East), 2011 IEEE PES Conference on},
doi = {10.1109/ISGT-MidEast.2011.6220819},
keywords = {mathematics computing;phasor measurement;power eng},
pages = {1--7},
title = {{Determination of generators coherent groups based on synchrophasors using bioinformatics toolbox}},
year = {2011}
}
@inproceedings{1565698,
author = {Papadimitriou, S and Gionis, A and Tsaparas, P and Vaisanen, R A and Mannila, H and Faloutsos, C},
booktitle = {Data Mining, Fifth IEEE International Conference on},
doi = {10.1109/ICDM.2005.117},
issn = {1550-4786},
keywords = {data mining;visual databases;feature cooccurrence},
month = {nov},
pages = {8 pp.--},
title = {{Parameter-free spatial data mining using MDL}},
year = {2005}
}
@inproceedings{4031578,
author = {Xu, Guoshi and Luo, Yin and Yu, Huashan and Xu, Zhuoqun},
booktitle = {Grid and Cooperative Computing Workshops, 2006. GCCW '06. Fifth International Conference on},
doi = {10.1109/GCCW.2006.85},
keywords = {Web services;biology computing;grid computing;BLAS},
month = {oct},
pages = {375--380},
title = {{Study on Bioinformatics Grid Application and its Supporting Environment}},
year = {2006}
}
@article{gb-2010-11-5-207,
abstract = {With DNA sequencing now getting cheaper more quickly than data storage or computation, the time may have come for genome informatics to migrate to the cloud.},
author = {Stein, Lincoln},
doi = {10.1186/gb-2010-11-5-207},
file = {::},
issn = {1465-6906},
journal = {Genome Biology},
keywords = {Cloud,bioinformatics,cloud toread},
mendeley-tags = {Cloud,bioinformatics},
number = {5},
pages = {207},
title = {{The case for cloud computing in genome informatics}},
url = {http://genomebiology.com/2010/11/5/207},
volume = {11},
year = {2010}
}
@inproceedings{6337832,
abstract = {Bioinformatics is an emerging field with seemingly limitless possibilities for advances in numerous scientific research and applications domains. In this paper, we summaries the explosive cutting-edge acceleration engines for the emerging short read mapping problems. What's more, we propose a novel Cloud based web service solution to the short read mapping problem in DNA sequencing, which greatly accelerates the task of aligning continuous incoming short length reads to uncertain known reference genomes. This approach is based on the pre-process of the reference genomes and iterative MapReduce jobs for aligning the continuous incoming reads. The MapReduce-based read-mapping algorithm is modeled after RMAP. Preliminary experimental results on incorporated MapReduce programming framework demonstrate that our proposed architecture and methods efficiently reduces the waiting time for large scale short reads applications. This architecture would be much important and efficient in future commercial personal gnome sequencing service.},
author = {Dai, Dong and Li, Xi and Wang, Chao and Zhou, Xuehai},
booktitle = {2012 IEEE International Conference on Cluster Computing},
doi = {10.1109/CLUSTER.2012.60},
isbn = {978-0-7695-4807-4},
keywords = {Acceleration,Algorithm design and analysis,Bioinformatics,Clustering algorithms,Computer architecture,DNA,DNA sequencing,Genomics,Hadoop,MapReduce programming framework,MapReduce-based read-mapping algorithm,RMAP,Service,Web services,applications domain,cloud based Web service,cloud based short read mapping service,cloud computing,continuous incoming read alignment,continuous incoming short length read alignment,cutting-edge acceleration engine,data handling,iterative MapReduce job,iterative methods,map reduce,parallel programming,personal genome sequencing service,reference genome,scientific research,short read mapping problem,short reads mapping},
month = {sep},
pages = {601--604},
publisher = {IEEE},
title = {{Cloud Based Short Read Mapping Service}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6337832},
year = {2012}
}
@inproceedings{1410529,
author = {Amendolia, S R and Estrella, F and McClatchey, R and Rogulin, D and Solomonides, T},
booktitle = {Design of Reliable Communication Networks, 2003. (DRCN 2003). Proceedings. Fourth International Workshop on},
doi = {10.1109/IDEADH.2004.16},
keywords = {PACS;biological organs;cancer;grid computing;mammo},
month = {sep},
pages = {99--108},
title = {{Managing Pan-European mammography images and data using a service oriented architecture}},
year = {2004}
}
@inproceedings{1364380,
author = {McClelland, M},
booktitle = {Biotechnology and Bioinformatics, 2004. Proceedings. Technology for Life: North Carolina Symposium on},
doi = {10.1109/SBB.2004.1364380},
keywords = {Internet;XML;biology computing;grid computing;midd},
month = {oct},
pages = {93--95},
title = {{Emerging standards for interoperable biological systems}},
year = {2004}
}
@inproceedings{5587308,
abstract = {Biology problems are in general NP-hard that demands tremendous resource both in terms of time and computing resources. Most of the computing systems developed for quantifying biological objects suffer from such limitations. MAQ (Mapping and Assembly with Quality) is one such popular bioinformatics system developed for whole genome reference assembly - it is designed to handle the challenges related to short sequence reads generated by Illumina sequencing machines, and can support a maximum read length of 63 nucleotides. MAQ is not multithreaded or many core ready - it runs on single CPU and does not scale. Therefore, as the data size increases, it fails to scale efficiently and requires a supercomputer to perform the assembly within a desired time. In this paper we report Cloud-MAQ that uses the cloud computing paradigm to address the NP-hard related challenges of whole genome reference assembly. Through Hadoop and the cloud paradigm MAQ is made parallel and scalable. Also, MAQ functionality has been enhanced to support recent reads from Illumina that are of 76 nucleotides. This cloud-enabled Cloud-MAQ increases the performance of MAQ reference assembly multi-fold.},
author = {Talukder, A K and Gandham, S and Prahalad, H A and Bhattacharyya, N P},
booktitle = {Wireless And Optical Communications Networks (WOCN), 2010 Seventh International Conference On},
doi = {10.1109/WOCN.2010.5587308},
keywords = {Internet;bioinformatics;computational complexity;g},
month = {sep},
pages = {1--5},
title = {{Cloud-MAQ: The cloud-enabled scalable whole genome reference Assembly application}},
year = {2010}
}
@article{4220624,
author = {Bartocci, E and Cacciagrano, D and Cannata, N and Corradini, F and Merelli, E and Milanesi, L and Romano, P},
doi = {10.1109/TNB.2007.897492},
issn = {1536-1241},
journal = {NanoBioscience, IEEE Transactions on},
keywords = {Factual;Information Storage and Retrieval;Interne,Internet;biology computing;grid computing;medical},
number = {2},
pages = {142--148},
title = {{An Agent-Based Multilayer Architecture for Bioinformatics Grids}},
volume = {6},
year = {2007}
}
@inproceedings{5666126,
author = {Li, Jinghua and Wei, Hua and Xia, Xiaoqin},
booktitle = {Power System Technology (POWERCON), 2010 International Conference on},
doi = {10.1109/POWERCON.2010.5666126},
keywords = {power generation dispatch;power markets;power syst},
month = {oct},
pages = {1--8},
title = {{The multi-agent model and method for Energy-Saving Generation Dispatching system}},
year = {2010}
}
@inproceedings{5586151,
abstract = {Cloud computing is a latest new computing paradigm where applications, data and IT services are provided over the Internet. Cloud computing has become a main medium for Software as a Service (SaaS) providers to host their SaaS as it can provide the scalability a SaaS requires. The challenges in the composite SaaS placement process rely on several factors including the large size of the Cloud network, SaaS competing resource requirements, SaaS interactions between its components and SaaS interactions with its data components. However, existing applications' placement methods in data centres are not concerned with the placement of the component's data. In addition, a Cloud network is much larger than data center networks that have been discussed in existing studies. This paper proposes a penalty-based genetic algorithm (GA) to the composite SaaS placement problem in the Cloud. We believe this is the first attempt to the SaaS placement with its data in Cloud provider's servers. Experimental results demonstrate the feasibility and the scalability of the GA.},
author = {Yusoh, Z I M and Tang, Maolin},
booktitle = {Evolutionary Computation (CEC), 2010 IEEE Congress on},
doi = {10.1109/CEC.2010.5586151},
keywords = {Internet;genetic algorithms;Internet;Software as a},
pages = {1--8},
title = {{A penalty-based genetic algorithm for the composite SaaS placement problem in the Cloud}},
year = {2010}
}
@inproceedings{6732752,
author = {Hao, Peng and Zhang, Jintao and Huan, Jun},
booktitle = {Bioinformatics and Biomedicine (BIBM), 2013 IEEE International Conference on},
doi = {10.1109/BIBM.2013.6732752},
keywords = {biochemistry;biology computing;data visualisation;},
pages = {35--37},
title = {{A new on-line chemical biology data visualization system}},
year = {2013}
}
@article{Fisch2015a,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics_pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Santani2017,
abstract = {Context.—The number of targeted next-generation se-quencing (NGS) panels for genetic diseases offered by clinical laboratories is rapidly increasing. Before an NGS-based test is implemented in a clinical laboratory, appropriate validation studies are needed to determine the performance characteristics of the test. Objective.—To provide examples of assay design and validation of targeted NGS gene panels for the detection of germline variants associated with inherited disorders. Data Sources.—The approaches used by 2 clinical laboratories for the development and validation of targeted NGS gene panels are described. Important design and validation considerations are examined. Conclusions.—Clinical laboratories must validate per-formance specifications of each test prior to implementa-tion. Test design specifications and validation data are provided, outlining important steps in validation of targeted NGS panels by clinical diagnostic laboratories.},
author = {Santani, Avni and Murrell, Jill and Funke, Birgit and Yu, Zhenming and Hegde, Madhuri and Mao, Rong and Ferreira-Gonzalez, Andrea and Voelkerding, Karl V. and Weck, Karen E.},
doi = {10.5858/arpa.2016-0517-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santani et al. - 2017 - Development and validation of targeted next-generation sequencing panels for detection of germline variants in i.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {787--797},
title = {{Development and validation of targeted next-generation sequencing panels for detection of germline variants in inherited diseases}},
volume = {141},
year = {2017}
}
@article{McCarthy2014,
abstract = {BACKGROUND Variant annotation is a crucial step in the analysis of genome sequencing data. Functional annotation results can have a strong influence on the ultimate conclusions of disease studies. Incorrect or incomplete annotations can cause researchers both to overlook potentially disease-relevant DNA variants and to dilute interesting variants in a pool of false positives. Researchers are aware of these issues in general, but the extent of the dependency of final results on the choice of transcripts and software used for annotation has not been quantified in detail. METHODS This paper quantifies the extent of differences in annotation of 80 million variants from a whole-genome sequencing study. We compare results using the RefSeq and Ensembl transcript sets as the basis for variant annotation with the software Annovar, and also compare the results from two annotation software packages, Annovar and VEP (Ensembl's Variant Effect Predictor), when using Ensembl transcripts. RESULTS We found only 44% agreement in annotations for putative loss-of-function variants when using the RefSeq and Ensembl transcript sets as the basis for annotation with Annovar. The rate of matching annotations for loss-of-function and nonsynonymous variants combined was 79% and for all exonic variants it was 83%. When comparing results from Annovar and VEP using Ensembl transcripts, matching annotations were seen for only 65% of loss-of-function variants and 87% of all exonic variants, with splicing variants revealed as the category with the greatest discrepancy. Using these comparisons, we characterised the types of apparent errors made by Annovar and VEP and discuss their impact on the analysis of DNA variants in genome sequencing studies. CONCLUSIONS Variant annotation is not yet a solved problem. Choice of transcript set can have a large effect on the ultimate variant annotations obtained in a whole-genome sequencing study. Choice of annotation software can also have a substantial effect. The annotation step in the analysis of a genome sequencing study must therefore be considered carefully, and a conscious choice made as to which transcript set and software are used for annotation.},
author = {McCarthy, Davis J. and Humburg, Peter and Kanapin, Alexander and Rivas, Manuel A. and Gaulton, Kyle and Cazier, Jean Baptiste and Donnelly, Peter},
doi = {10.1186/gm543},
file = {:home/jennifer/Descargas/gm543:},
isbn = {1756994X (Linking)},
issn = {1756994X},
journal = {Genome Medicine},
number = {3},
pmid = {24944579},
title = {{Choice of transcripts and software has a large effect on variant annotation}},
volume = {6},
year = {2014}
}
@misc{Watson1953,
abstract = {A structure for nucleic acid has already been proposed by Pauling and Corey 1 . They kindly made their manuscript available to us in advance of publication. Their model consists of three intertwined chains, with the phosphates near},
author = {Watson, James D and Crick, Francis H C},
booktitle = {Nature},
doi = {10.1097/BLO.0b013e3181468780},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Crick - 1953 - Molecular structure of nucleic acids.pdf:pdf},
isbn = {0226284158},
issn = {0028-0836},
keywords = {nucleic acids},
number = {4356},
pages = {737--738},
pmid = {1943},
title = {{Molecular structure of nucleic acids}},
url = {http://www.nature.com/physics/looking-back/crick/%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/13054692},
volume = {171},
year = {1953}
}
@misc{variome2017,
title = {{About the Human Variome Project: what we do and why we do it - Human Variome Project}},
url = {http://www.humanvariomeproject.org/about/about-the-human-variome-project.html},
urldate = {2017-11-19}
}
@article{Warden2014,
abstract = {The Genome Analysis Toolkit (GATK) is commonly used for variant calling of single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels) from short-read sequencing data aligned against a reference genome. There have been a number of variant calling comparisons against GATK, but an equally comprehensive comparison for VarScan not yet been performed. More specifically, we compare (1) the effects of different pre-processing steps prior to variant calling with both GATK and VarScan, (2) VarScan variants called with increasingly conservative parameters, and (3) filtered and unfiltered GATK variant calls (for both the UnifiedGenotyper and the HaplotypeCaller). Variant calling was performed on three datasets (1 targeted exon dataset and 2 exome datasets), each with approximately a dozen subjects. In most cases, pre-processing steps (e.g., indel realignment and quality score base recalibration using GATK) had only a modest impact on the variant calls, but the importance of the pre-processing steps varied between datasets and variant callers. Based upon concordance statistics presented in this study, we recommend GATK users focus on "high-quality" GATK variants by filtering out variants flagged as low-quality. We also found that running VarScan with a conservative set of parameters (referred to as "VarScan-Cons") resulted in a reproducible list of variants, with high concordance (>97%) to high-quality variants called by the GATK UnifiedGenotyper and HaplotypeCaller. These conservative parameters result in decreased sensitivity, but the VarScan-Cons variant list could still recover 84-88% of the high-quality GATK SNPs in the exome datasets. This study also provides limited evidence that VarScan-Cons has a decreased false positive rate among novel variants (relative to high-quality GATK SNPs) and that the GATK HaplotypeCaller has an increased false positive rate for indels (relative to VarScan-Cons and high-quality GATK UnifiedGenotyper indels). More broadly, we believe the metrics used for comparison in this study can be useful in assessing the quality of variant calls in the context of a specific experimental design. As an example, a limited number of variant calling comparisons are also performed on two additional variant callers.},
author = {Warden, Charles D. and Adamson, Aaron W. and Neuhausen, Susan L. and Wu, Xiwei},
doi = {10.7717/peerj.600},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warden et al. - 2014 - Detailed comparison of two popular variant calling packages for exome and targeted exon studies.pdf:pdf},
isbn = {2167-8359 (Electronic)},
issn = {2167-8359},
journal = {PeerJ},
keywords = {exome,gatk,small indel,snp,targeted sequencing,variant calling,varscan},
pages = {e600},
pmid = {25289185},
title = {{Detailed comparison of two popular variant calling packages for exome and targeted exon studies}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4184249&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{Liu2013,
abstract = {Next generation sequencing (NGS) has been leading the genetic study of human disease into an era of unprecedented productivity. Many bioinformatics pipelines have been developed to call variants from NGS data. The performance of these pipelines depends crucially on the variant caller used and on the calling strategies implemented. We studied the performance of four prevailing callers, SAMtools, GATK, glftools and Atlas2, using single-sample and multiple-sample variant-calling strategies. Using the same aligner, BWA, we built four single-sample and three multiple-sample calling pipelines and applied the pipelines to whole exome sequencing data taken from 20 individuals. We obtained genotypes generated by Illumina Infinium HumanExome v1.1 Beadchip for validation analysis and then used Sanger sequencing as a "gold-standard" method to resolve discrepancies for selected regions of high discordance. Finally, we compared the sensitivity of three of the single-sample calling pipelines using known simulated whole genome sequence data as a gold standard. Overall, for single-sample calling, the called variants were highly consistent across callers and the pairwise overlapping rate was about 0.9. Compared with other callers, GATK had the highest rediscovery rate (0.9969) and specificity (0.99996), and the Ti/Tv ratio out of GATK was closest to the expected value of 3.02. Multiple-sample calling increased the sensitivity. Results from the simulated data suggested that GATK outperformed SAMtools and glfSingle in sensitivity, especially for low coverage data. Further, for the selected discrepant regions evaluated by Sanger sequencing, variant genotypes called by exome sequencing versus the exome array were more accurate, although the average variant sensitivity and overall genotype consistency rate were as high as 95.87% and 99.82%, respectively. In conclusion, GATK showed several advantages over other variant callers for general purpose NGS analyses. The GATK pipelines we developed perform very well.},
author = {Liu, Xiangtao and Han, Shizhong and Wang, Zuoheng and Gelernter, Joel and Yang, Bao Zhu},
doi = {10.1371/journal.pone.0075619},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Variant Callers for Next-Generation Sequencing Data A Comparison Study.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--11},
pmid = {24086590},
title = {{Variant Callers for Next-Generation Sequencing Data: A Comparison Study}},
volume = {8},
year = {2013}
}
@article{McKenna2009,
abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS—the 1000 Genome pilot alone includes nearly five terabases—make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management in- frastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide poly- morphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas},
author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and Depristo, Mark A},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenna et al. - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Cold Spring Harbor Laboratory Press Resource},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data}},
volume = {20},
year = {2009}
}
@article{Lopez2017,
author = {Lopez, Javier and Coll, Jacobo and Haimel, Matthias and Kandasamy, Swaathi and Tarraga, Joaquin and Furio-tari, Pedro and Bari, Wasim and Bleda, Marta and Rueda, Antonio and Rendon, Augusto and Dopazo, Joaquin and Medina, Ignacio},
doi = {10.1093/nar/gkx445},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez et al. - 2017 - HGVA the Human Genome Variation Archive.pdf:pdf},
pages = {1--6},
title = {{HGVA: the Human Genome Variation Archive}},
year = {2017}
}
@misc{Babraham2016,
author = {{Babraham Bioinformatics}},
booktitle = {www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules},
title = {{FASTQC manual}},
url = {http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules/},
urldate = {2016-06-25},
year = {2016}
}
@article{Merelli2014,
abstract = {The explosion of the data both in the biomedical research and in the healthcare systems demands urgent solutions. In particular, the research in omics sciences is moving from a hypothesis-driven to a data-driven approach. Healthcare is additionally always asking for a tighter integration with biomedical data in order to promote personalized medicine and to provide better treatments. Efficient analysis and interpretation of Big Data opens new avenues to explore molecular biology, new questions to ask about physiological and pathological states, and new ways to answer these open issues. Such analyses lead to better understanding of diseases and development of better and personalized diagnostics and therapeutics. However, such progresses are directly related to the availability of new solutions to deal with this huge amount of information. New paradigms are needed to store and access data, for its annotation and integration and finally for inferring knowledge and making it available to researchers. Bioinformatics can be viewed as the "glue" for all these processes. A clear awareness of present high performance computing (HPC) solutions in bioinformatics, Big Data analysis paradigms for computational biology, and the issues that are still open in the biomedical and healthcare fields represent the starting point to win this challenge.},
author = {Merelli, Ivan and P{\'{e}}rez-S{\'{a}}nchez, Horacio and Gesing, Sandra and D'Agostino, Daniele},
doi = {10.1155/2014/134023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Merelli et al. - 2014 - Managing, analysing, and integrating big data in medical bioinformatics open problems and future perspectives.pdf:pdf},
issn = {2314-6141},
journal = {BioMed research international},
month = {sep},
pages = {134023},
pmid = {25254202},
publisher = {Hindawi},
title = {{Managing, analysing, and integrating big data in medical bioinformatics: open problems and future perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25254202 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4165507},
volume = {2014},
year = {2014}
}
@article{Buchard2016,
abstract = {The HID-Ion AmpliSeq™ Identity Panel is a next-generation sequencing assay with 90 autosomal and 34 Y-chromosome SNPs that are amplified in one PCR step and subsequently sequenced using the Ion Personal Genome Machine (Ion PGM™) System. This assay was validated for relationship testing in our ISO 17025 accredited laboratory in 2015. Here, the essential parts of the validation report submitted to the Danish Accreditation Fund are presented. A total of 100 unrelated Danes were typed in duplicates and the locus balance, heterozygote balance (Hb) and noise levels were analysed in detail. Two loci were disregarded for casework because genotyping was uncertain. Hb for rs7520386 was skewed and high levels of noise were observed in rs576261. Three general acceptance criteria for analysis of single-source samples were defined: (i) sequencing depth > 200 reads, (ii) noise level < 3% and (iii) Hb > 0.3. A Python script named SNPonPGM was developed to assist the analyst by highlighting loci that do not fulfil the general acceptance criteria. Furthermore, SNPonPGM has functions that reduce the hands-on time of the reporting officer to a few minutes per case. Mixtures with DNA from two individuals in a 1:24 ratio were readily identified using the three criteria and the SNPonPGM script.},
author = {Buchard, Anders and Kampmann, Marie-Louise and Poulsen, Lena and B{\o}rsting, Claus and Morling, Niels},
doi = {10.1002/elps.201600269},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchard et al. - 2016 - ISO 17025 validation of a next-generation sequencing assay for relationship testing(3).pdf:pdf},
issn = {1522-2683},
journal = {Electrophoresis},
keywords = {Forensic genetics,ISO 17025 accreditation,Kinship testing,Next-generation sequencing,SNPs},
pages = {1--10},
pmid = {27709635},
title = {{ISO 17025 validation of a next-generation sequencing assay for relationship testing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27709635},
year = {2016}
}
@misc{Paez2012,
abstract = {El contexto din{\'{a}}mico y competitivo de la organizaci{\'{o}}n actual exige permanentes soluciones inform{\'{a}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\'{o}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\'{o}}n mayormente, desde el punto de vista tecnol{\'{o}}gico, sin considerar los aspectos organizacionales y metodol{\'{o}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{a}}ez - 2012 - Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio.pdf:pdf},
issn = {0120-341X},
keywords = {Sistemas de informaci{\'{o}}n en administraci{\'{o}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Biesecker2014,
abstract = {Sequencing of the genome or exome for clinical applications, hereafter referred to as clinical genome and exome sequencing (CGES), has now entered medical practice.1 Several thousand CGES tests have already been ordered for patients, with the goal of establishing diagnoses for rare, clinically unrecognizable, or puzzling disorders that are suspected to be genetic in origin. We anticipate increases in the use of CGES, the key attribute of which — its breadth — distinguishes it from other forms of laboratory testing. The interrogation of variation in about 20,000 genes simultaneously can be a powerful and effective diagnostic method.2 CGES has been hailed as an important tool in the implementation of predictive and individualized medicine, and there is intense research interest in the clinical benefits and risks of sequencing for screening healthy persons3; however, current practice recommendations4 do not support the use of sequencing for this purpose, and for that reason we do not further address it here. We have also limited this overview of CGES to the analysis of germline sequence variants for diagnostic purposes and do not discuss the use of CGES to uncover somatic variants in cancer in order to individualize cancer therapy. Clinicians should understand the diagnostic indications for CGES so that they can effectively deploy it in their practices. Because the success rate of CGES for the identification of a causative variant is approximately 25%,5 it is important to understand the basis of this testing and how to select the patients most likely to benefit from it. Here, we summarize the technologies underlying CGES and offer our insights into how clinicians should order such testing, interpret the results, and communicate the results to their patients (an interactive graphic giving an overview of the process is available with the full text of this article at NEJM.org).},
author = {Biesecker, Leslie G. and Green, Robert C.},
doi = {10.1056/NEJMra1312543},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesecker, Green - 2014 - Diagnostic Clinical Genome and Exome Sequencing.pdf:pdf},
isbn = {1533-4406 (Electronic)\r0028-4793 (Linking)},
issn = {1533-4406},
journal = {New England Journal of Medicine},
keywords = {Exome,Genetic Counseling,Genetic Diseases, Inborn,Genetic Diseases, Inborn: diagnosis,Genetic Diseases, Inborn: genetics,Genetic Testing,Genome, Human,Humans,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {25},
pages = {2418--2425},
pmid = {24941179},
title = {{Diagnostic Clinical Genome and Exome Sequencing}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMra1312543},
volume = {370},
year = {2014}
}
@article{Systems2009a,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - Structural variation and Medical Genomics.pdf:pdf},
title = {{Structural variation and Medical Genomics}},
year = {2009}
}
@article{Rishishwar2015,
abstract = {The human dimension of the Columbian Exchange entailed substantial genetic admixture between ancestral source populations from Africa, the Americas and Europe, which had evolved separately for many thousands of years. We sought to address the implications of the creation of admixed American genomes, containing novel allelic combinations, for human health and fitness via analysis of an admixed Colombian population from Medellin. Colombian genomes from Medellin show a wide range of three-way admixture contributions from ancestral source populations. The primary ancestry component for the population is European (average = 74.6%, range = 45.0%-96.7%), followed by Native American (average = 18.1%, range = 2.1%-33.3%) and African (average = 7.3%, range = 0.2%-38.6%). Locus-specific patterns of ancestry were evaluated to search for genomic regions that are enriched across the population for particular ancestry contributions. Adaptive and innate immune system related genes and pathways are particularly over-represented among ancestry-enriched segments, including genes (HLA-B and MAPK10) that are involved in defense against endemic pathogens such as malaria. Genes that encode functions related to skin pigmentation (SCL4A5) and cutaneous glands (EDAR) are also found in regions with anomalous ancestry patterns. These results suggest the possibility that ancestry-specific loci were differentially retained in the modern admixed Colombian population based on their utility in the New World environment.},
author = {Rishishwar, Lavanya and Conley, Andrew B. and Wigington, Charles H. and Wang, Lu and Valderrama-Aguirre, Augusto and {King Jordan}, I.},
doi = {10.1038/srep12376},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rishishwar et al. - 2015 - Ancestry, admixture and fitness in Colombian genomes.pdf:pdf},
isbn = {2045-2322 (ISSNLinking)},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {12376},
pmid = {26197429},
publisher = {Nature Publishing Group},
title = {{Ancestry, admixture and fitness in Colombian genomes}},
url = {http://www.nature.com/articles/srep12376},
volume = {5},
year = {2015}
}
@article{Lescai2014,
abstract = {The choice of an appropriate variant calling pipeline for exome sequencing data is becoming increasingly more important in translational medicine projects and clinical contexts. Within GOSgene, which facilitates genetic analysis as part of a joint effort of the University College London and the Great Ormond Street Hospital, we aimed to optimize a variant calling pipeline suitable for our clinical context. We implemented the GATK/Queue framework and evaluated the performance of its two callers: the classical UnifiedGenotyper and the new variant discovery tool HaplotypeCaller. We performed an experimental validation of the loss-of-function (LoF) variants called by the two methods using Sequenom technology. UnifiedGenotyper showed a total validation rate of 97.6% for LoF single-nucleotide polymorphisms (SNPs) and 92.0% for insertions or deletions (INDELs), whereas HaplotypeCaller was 91.7% for SNPs and 55.9% for INDELs. We confirm that GATK/Queue is a reliable pipeline in translational medicine and clinical context. We conclude that in our working environment, UnifiedGenotyper is the caller of choice, being an accurate method, with a high validation rate of error-prone calls like LoF variants. We finally highlight the importance of experimental validation, especially for INDELs, as part of a standard pipeline in clinical environments.},
author = {Lescai, Francesco and Marasco, Elena and Bacchelli, Chiara and Stanier, Philip and Mantovani, Vilma and Beales, Philip},
doi = {10.1002/mgg3.42},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lescai et al. - 2014 - Identification and validation of loss of function variants in clinical contexts.pdf:pdf},
issn = {2324-9269},
journal = {Molecular genetics & genomic medicine},
keywords = {gatk,pipelines,sequencing,variant calling},
number = {1},
pages = {58--63},
pmid = {24498629},
title = {{Identification and validation of loss of function variants in clinical contexts.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3907911&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{KonstantinosN.LazaridisMD;KimberlyA.SchahlCGC;MargotA.CousinPhD;DusicaBabovic-VuksanovicMD;DouglasL.Riegert-JohnsonMD;RalitzaH.GavrilovaMD;TammyM.McAllisterMA;NoralaneM.LindorMD;RoshiniS.AbrahamPhD;MichaelJ.Ac2016,
author = {{Konstantinos N. Lazaridis, MD; Kimberly A. Schahl, CGC; Margot A. Cousin, PhD; Dusica Babovic-Vuksanovic, MD; Douglas L. Riegert-Johnson, MD; Ralitza H. Gavrilova, MD; Tammy M. McAllister, MA; Noralane M. Lindor, MD; Roshini S. Abraham, PhD; Michael J. Ac}, MD; and the Individualized Medicine Clinic Members},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
journal = {Mayo Clinic Proceedings},
number = {3},
pages = {29},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
volume = {91},
year = {2016}
}
@incollection{Kulski2016,
author = {Kulski, Jerzy K.},
booktitle = {Next Generation Sequencing - Advances, Applications and Challenges},
doi = {10.5772/61964},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulski - 2016 - Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications(2).pdf:pdf},
month = {jan},
publisher = {InTech},
title = {{Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications}},
url = {http://www.intechopen.com/books/next-generation-sequencing-advances-applications-and-challenges/next-generation-sequencing-an-overview-of-the-history-tools-and-omic-applications},
year = {2016}
}
@article{Davidson2017,
abstract = {Motivation: RNA-Seq analyses can benefit from performing a genome-guided and de novo assem- bly, in particular for species where the reference genome is incomplete. However, tools to integrate assembled transcriptome with reference annotation are lacking. Results: Necklace is a software pipeline to run genome-guided and de novo assembly and combine the resulting transcriptomes with reference genome annotations. Necklace constructs a compact but comprehensive superTranscriptome out of the assembled and reference data. Reads are subse- quently aligned and counted in preparation for differential expression testing. Availability: Necklace is available from https://github.com/Oshlack/necklace/wiki under GPL 3.0. Contact: nadia.davidson@mcri.edu.au or alicia.oshlack@mcri.edu.au},
archivePrefix = {arXiv},
arxivId = {103549},
author = {Davidson, Nadia M and Oshlack, Alicia},
doi = {10.1093/bioinformatics/xxxxx},
eprint = {103549},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, Oshlack - 2017 - Necklace combining reference and assembled transcriptomes for RNA-Seq analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
pages = {0--0},
title = {{Necklace: combining reference and assembled transcriptomes for RNA-Seq analysis}},
url = {https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/PAP/10.1093_bioinformatics_btx400/1/btx400.pdf?Expires=1498600002&Signature=NUtYYcdBpBQNhzgudJGKJAFp6hRQqryxL76tZJuI1v1V7Eh15Dwr9tfCwfud5GIZHEv4qb2hVBrPwLzSGZ8zh2jNkPnFA$\sim$Yd},
year = {2017}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{Ramasamy2017,
author = {Ramasamy, S. and Nirmala, K.},
doi = {10.1080/1206212X.2017.1396415},
file = {:home/jennifer/Descargas/dies.pdf:pdf},
issn = {1206-212X},
journal = {International Journal of Computers and Applications},
keywords = {Data mining,association,association rule,classification,data mining,keyword-based clustering},
number = {December},
pages = {1--8},
publisher = {Taylor & Francis},
title = {{Disease prediction in data mining using association rule mining and keyword based clustering algorithms}},
url = {https://www.tandfonline.com/doi/full/10.1080/1206212X.2017.1396415},
volume = {7074},
year = {2017}
}
@article{Lazaridis2016,
abstract = {OBJECTIVE: To describe the experience and outcome of performing whole-exome sequencing (WES) for resolution of patients on a diagnostic odyssey in the first 18 months of an individualized medicine clinic (IMC).

PATIENTS AND METHODS: The IMC offered WES to physicians of Mayo Clinic practice for patients with suspected genetic disease. DNA specimens of the proband and relatives were submitted to WES laboratories. We developed the Genomic Odyssey Board with multidisciplinary expertise to determine the appropriateness for IMC services, review WES reports, and make the final decision about whether the exome findings explain the disease. This study took place from September 30, 2012, to March 30, 2014.

RESULTS: In the first 18 consecutive months, the IMC received 82 consultation requests for patients on a diagnostic odyssey. The Genomic Odyssey Board deferred 7 cases and approved 75 cases to proceed with WES. Seventy-one patients met with an IMC genomic counselor. Fifty-one patients submitted specimens for WES testing, and the results have been received for all. There were 15 cases in which a diagnosis was made on the basis of WES findings; thus, the positive diagnostic yield of this practice was 29%. The mean cost per patient for this service was approximately $8000. Medicaid supported 27% of the patients, and 38% of patients received complete or partial insurance coverage.

CONCLUSION: The significant diagnostic yield, moderate cost, and notable health marketplace acceptance for WES compared with conventional genetic testing make the former method a rational diagnostic approach for patients on a diagnostic odyssey.},
author = {Lazaridis, Konstantinos N and Schahl, Kimberly A and Cousin, Margot A and Babovic-Vuksanovic, Dusica and Riegert-Johnson, Douglas L and Gavrilova, Ralitza H and McAllister, Tammy M and Lindor, Noralane M and Abraham, Roshini S and Ackerman, Michael J and Pichurin, Pavel N and Deyle, David R and Gavrilov, Dimitar K and Hand, Jennifer L and Klee, Eric W and Stephens, Michael C and Wick, Myra J and Atkinson, Elizabeth J and Linden, David R and Ferber, Matthew J and Wieben, Eric D and Farrugia, Gianrico},
doi = {10.1016/j.mayocp.2015.12.018},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
issn = {1942-5546},
journal = {Mayo Clinic proceedings},
language = {English},
month = {mar},
number = {3},
pages = {297--307},
pmid = {26944241},
publisher = {Elsevier},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
url = {http://www.mayoclinicproceedings.org/article/S0025619616000240/fulltext},
volume = {91},
year = {2016}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Handl2005,
author = {Handl, J. and Knowles, J. and Kell, D. B.},
doi = {10.1093/bioinformatics/bti517},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handl, Knowles, Kell - 2005 - Computational cluster validation in post-genomic data analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {3201--3212},
title = {{Computational cluster validation in post-genomic data analysis}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bti517},
volume = {21},
year = {2005}
}
@article{Sims2014,
abstract = {Sequencing technologies have placed a wide range of genomic analyses within the capabilities of many laboratories. However, sequencing costs often set limits to the amount of sequences that can be generated and, consequently, the biological outcomes that can be achieved from an experimental design. In this Review, we discuss the issue of sequencing depth in the design of next-generation sequencing experiments. We review current guidelines and precedents on the issue of coverage, as well as their underlying considerations, for four major study designs, which include de novo genome sequencing, genome resequencing, transcriptome sequencing and genomic location analyses (for example, chromatin immunoprecipitation followed by sequencing (ChIP-seq) and chromosome conformation capture (3C)).},
author = {Sims, David and Sudbery, Ian and Ilott, Nicholas E and Heger, Andreas and Ponting, Chris P},
doi = {10.1038/nrg3642},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sims et al. - 2014 - Genomics is extending its reach into diverse fields of biomedical research from agriculture to clinical diag- nosti.pdf:pdf},
isbn = {1471-0056},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
number = {2},
pages = {121--32},
pmid = {24434847},
title = {{Sequencing depth and coverage: key considerations in genomic analyses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24434847},
volume = {15},
year = {2014}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Del et al. - 2014 - Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology.pdf:pdf},
pages = {51--68},
title = {{Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{%} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P and Houwing-Duistermaat, Jeanine J and {Eline Slagboom}, P and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Pietrelli2017,
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management(2).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
publisher = {Oxford University Press},
title = {{myVCF: a desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@incollection{Agrawal2016,
address = {Boston, MA},
author = {Agrawal, Ankit and Choudhary, Alok},
booktitle = {Data and Measures in Health Services Research},
doi = {10.1007/978-1-4899-7673-4_2-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Choudhary - 2016 - Health Services Data Big Data Analytics for Deriving Predictive Healthcare Insights.pdf:pdf},
pages = {1--17},
publisher = {Springer US},
title = {{Health Services Data: Big Data Analytics for Deriving Predictive Healthcare Insights}},
url = {http://link.springer.com/10.1007/978-1-4899-7673-4_2-1},
year = {2016}
}
@article{Bustos2007,
abstract = {El presente art{\'{i}}culo muestra los resultados de la investigaci{\'{o}}n en la cual se aplic{\'{o}} la metodolog{\'{i}}a Shainin del dise{\~{n}}o experimental en la planta de producci{\'{o}}n de un ingenio azucarero del Valle del Cauca. Este trabajo destaca la importancia que tiene el Dise{\~{n}}o Experimental como herramienta estad{\'{i}}stica para el mejoramiento de procesos productivos, que va m{\'{a}}s all{\'{a}} del simple monitoreo impuesto por las t{\'{e}}cnicas de control estad{\'{i}}stico de procesos, sin desmeritarlas como herramientas {\'{u}}tiles para controlar su rendimiento},
author = {Bustos, Ligia and Moreno, Ricardo and Duque, Nestor},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/SPARC (Organization), Universidad Tecnológica de Pereira., Duque - 1995 - Scientia et technica.pdf:pdf},
issn = {0122-1701},
journal = {Scientia},
keywords = {catequina,condensados,fenoles,florotaninos,s polifenoles,taninos,taninos hidrolizables,{\'{a}}cido el{\'{a}}gico,{\'{a}}cido g{\'{a}}lico},
number = {037},
pages = {13--18},
publisher = {Universidad Tecnológica de Pereira},
title = {{Modelo de una bodega de datos para el soporte a la investigaci{\'{o}}n bioinform{\'{a}}tica}},
url = {http://www.redalyc.org/resumen.oa?id=84922625025 http://revistas.utp.edu.co/index.php/revistaciencia/article/view/4125/2181 http://redalyc.uaemex.mx/src/inicio/ArtPdfRed.jsp?iCve=84903790},
volume = {XIII},
year = {2007}
}
@article{Fong2017,
author = {Fong, Kenneth and Bailey, Celeste V. and Tuttle, Peggy and Cunningham, Bari and McGrath, John A. and Cho, Raymond J.},
doi = {10.1111/pde.13029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong et al. - 2017 - Questioning the Clinical Utility of Exome Sequencing in Developing Countries.pdf:pdf},
issn = {07368046},
journal = {Pediatric Dermatology},
month = {jan},
number = {1},
pages = {e32--e34},
title = {{Questioning the Clinical Utility of Exome Sequencing in Developing Countries}},
url = {http://doi.wiley.com/10.1111/pde.13029},
volume = {34},
year = {2017}
}
@article{Stelzer,
abstract = {GeneCards, the human gene compendium, enables researchers to effectively navigate and inter-relate the wide universe of human genes, diseases, variants, proteins, cells, and biological pathways. Our recently launched Version 4 has a revamped infrastructure facilitating faster data updates, better-targeted data queries, and friendlier user experience. It also provides a stronger foundation for the GeneCards suite of companion databases and analysis tools. Improved data unification includes gene-disease links via MalaCards and merged biological pathways via PathCards, as well as drug information and proteome expression. VarElect, another suite member, is a phenotype prioritizer for next-generation sequencing, leveraging the GeneCards and MalaCards knowledgebase. It au-tomatically infers direct and indirect scored associations between hundreds or even thousands of variant-containing genes and disease phenotype terms. Var-Elect's capabilities, either independently or within TGex, our comprehensive variant analysis pipeline, help prepare for the challenge of clinical projects that involve thousands of exome/genome NGS analyses. C},
author = {Stelzer, Gil and Rosen, Naomi and Plaschkes, Inbar and Zimmerman, Shahar and Twik, Michal and Fishilevich, Simon and Stein, Tsippi Iny and Nudel, Ron and Lieder, Iris and Mazor, Yaron and Kaplan, Sergey and Dahary, Dvir and Warshawsky, David and Guan-Golan, Yaron and Kohn, Asher and Rappaport, Noa and Safran, Marilyn and Lancet, Doron},
doi = {10.1002/cpbi.5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer et al. - Unknown - The GeneCards Suite From Gene Data Mining to Disease Genome Sequence Analyses.pdf:pdf},
journal = {Curr. Protoc. Bioinform},
keywords = {Elect,Gene,Var,biological database r bioinformatics r diseases r},
number = {1},
pages = {1--130},
title = {{The GeneCards Suite: From Gene Data Mining to Disease Genome Sequence Analyses}},
volume = {5430}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/},
urldate = {2015-11-04}
}
@article{Niroula2016,
abstract = {Next generation sequencing (NGS) methods have revolutionized the speed of generating variation information. Sequence data have a plethora of applications and will increasingly be used for disease diagnosis. Interpretation of the identified variants is usually not possible with experimental methods. This has caused a bottleneck that many computational methods aim at addressing. Fast and efficient methods for explaining the significance and mechanisms of detected variants are required for efficient precision/personalized medicine. Computational prediction methods have been developed in three areas to address the issue. There are generic tolerance (pathogenicity) predictors for filtering harmful variants. Gene/protein/disease-specific tools are available for some applications. Mechanism and effect-specific computer programs aim at explaining the consequences of variations. Here, we discuss the different types of predictors and their applications. We review available variation databases and prediction methods useful for variation interpretation. We discuss how the performance of methods is assessed and summarize existing assessment studies. A brief introduction is provided to the principles of the methods developed for variation interpretation as well as guidelines for how to choose the optimal tools and where the field is heading in the future. This article is protected by copyright. All rights reserved.},
author = {Niroula, Abhishek and Vihinen, Mauno},
doi = {10.1002/humu.22987},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niroula, Vihinen - 2016 - Variation Interpretation Predictors Principles, Types, Performance and Choice.pdf:pdf},
issn = {1098-1004},
journal = {Human mutation},
month = {mar},
pmid = {26987456},
title = {{Variation Interpretation Predictors: Principles, Types, Performance and Choice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26987456},
year = {2016}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Lubin2017,
abstract = {A national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications. This flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants. To provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settings, use of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard to the existing variant file specifications presently used in the clinical setting. Adoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities.},
author = {Lubin, Ira M. and Aziz, Nazneen and Babb, Lawrence J. and Ballinger, Dennis and Bisht, Himani and Church, Deanna M. and Cordes, Shaun and Eilbeck, Karen and Hyland, Fiona and Kalman, Lisa and Landrum, Melissa and Lockhart, Edward R. and Maglott, Donna and Marth, Gabor and Pfeifer, John D. and Rehm, Heidi L. and Roy, Somak and Tezak, Zivana and Truty, Rebecca and Ullman-Cullere, Mollie and Voelkerding, Karl V. and Worthey, Elizabeth A. and Zaranek, Alexander W. and Zook, Justin M.},
doi = {10.1016/j.jmoldx.2016.12.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lubin et al. - 2017 - Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinica.pdf:pdf},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {3},
pages = {417--426},
pmid = {28315672},
publisher = {Elsevier Inc},
title = {{Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinical Settings}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.12.001},
volume = {19},
year = {2017}
}
@misc{Tetreault2015a,
abstract = {Whole-exome sequencing (WES) represents a significant breakthrough in the field of human genetics. This technology has largely contributed to the identification of new disease-causing genes and is now entering clinical laboratories. WES represents a powerful tool for diagnosis and could reduce the ‘diagnostic odyssey' for many patients. In this review, we present a technical overview of WES analysis, variants annotation and interpretation in a clinical setting. We evaluate the usefulness of clinical WES in different clinical indications, such as rare diseases, cancer and complex diseases. Finally, we discuss the efficacy of WES as a diagnostic tool and the impact on patient management.},
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
booktitle = {Expert Review of Molecular Diagnostics},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
keywords = {cancer,diagnostic,rare diseases,variants detection,whole-exome sequencing},
language = {en},
month = {may},
publisher = {Informa Healthcare},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities}},
url = {http://www.tandfonline.com/doi/abs/10.1586/14737159.2015.1039516?journalCode=iero20#.VxmiWVg_zcU.mendeley},
year = {2015}
}
@article{Robinson2017,
abstract = {Manual review of aligned reads for confirmation and interpretation of variant calls is an important step in many variant calling pipelines for next-generation sequencing (NGS) data. Visual inspection can greatly increase the confidence in calls, reduce the risk of false positives, and help characterize complex events. The Integrative Genomics Viewer (IGV) was one of the first tools to provide NGS data visualization, and it currently provides a rich set of tools for inspection, validation, and interpretation of NGS datasets, as well as other types of genomic data. Here, we present a short overview of IGV's variant review features for both single-nucleotide variants and structural variants, with examples from both cancer and germline datasets. IGV is freely available at https://www.igv.org Cancer Res; 77(21); e31-34. {\textcopyright}2017 AACR.},
author = {Robinson, James T. and Thorvaldsd{\'{o}}ttir, Helga and Wenger, Aaron M. and Zehir, Ahmet and Mesirov, Jill P.},
doi = {10.1158/0008-5472.CAN-17-0337},
issn = {0008-5472},
journal = {Cancer Research},
month = {nov},
number = {21},
pages = {e31--e34},
pmid = {29092934},
title = {{Variant Review with the Integrative Genomics Viewer}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29092934 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5678989 http://cancerres.aacrjournals.org/lookup/doi/10.1158/0008-5472.CAN-17-0337},
volume = {77},
year = {2017}
}
@article{He,
abstract = {Genomic medicine attempts to build individualized strategies for diagnostic or therapeutic decision-making by utilizing patients' genomic information. Big Data analytics uncovers hidden patterns, unknown correlations, and other insights through examining large-scale various data sets. While integration and manipulation of diverse genomic data and comprehensive electronic health records (EHRs) on a Big Data infrastructure exhibit challenges, they also provide a feasible opportunity to develop an efficient and effective approach to identify clinically actionable genetic variants for individualized diagnosis and therapy. In this paper, we review the challenges of manipulating large-scale next-generation sequencing (NGS) data and diverse clinical data derived from the EHRs for genomic medicine. We introduce possible solutions for different challenges in manipulating, managing, and analyzing genomic and clinical data to implement genomic medicine. Additionally, we also present a practical Big Data toolset for identifying clinically actionable genetic variants using high-throughput NGS data and EHRs.},
author = {He, Karen Y and Ge, Dongliang and He, Max M},
doi = {10.3390/ijms18020412},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Ge, He - Unknown - Big Data Analytics for Genomic Medicine.pdf:pdf},
keywords = {Big Data analytics,clinically actionable genetic variants,electronic health records,healthcare,next-generation sequencing},
title = {{Big Data Analytics for Genomic Medicine}}
}
@article{Zhou2013,
abstract = {Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7-8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis. QC-Chain is publicly available at: http://www.computationalbioenergy.org/qc-chain.html.},
author = {Zhou, Qian and Su, Xiaoquan and Wang, Anhui and Xu, Jian and Ning, Kang},
doi = {10.1371/journal.pone.0060234},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2013 - QC-Chain Fast and Holistic Quality Control Method for Next-Generation Sequencing Data.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {23565205},
title = {{QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data}},
volume = {8},
year = {2013}
}
@article{Hashem2015,
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. {\textcopyright} 2014 Elsevier Ltd.},
author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
doi = {10.1016/j.is.2014.07.006},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Big data,Cloud computing,Hadoop},
month = {jan},
pages = {98--115},
pmid = {1476196123},
publisher = {Pergamon},
title = {{The rise of "big data" on cloud computing: Review and open research issues}},
url = {http://www.sciencedirect.com/science/article/pii/S0306437914001288},
volume = {47},
year = {2015}
}
@article{Yates2016,
abstract = {The Ensembl project (http://www.ensembl.org) is a system for genome annotation, analysis, storage and dissemination designed to facilitate the access of genomic annotation from chordates and key model organisms. It provides access to data from 87 species across our main and early access Pre! websites. This year we introduced three newly annotated species and released numerous updates across our supported species with a concentration on data for the latest genome assemblies of human, mouse, zebrafish and rat. We also provided two data updates for the previous human assembly, GRCh37, through a dedicated website (http://grch37.ensembl.org). Our tools, in particular the VEP, have been improved significantly through integration of additional third party data. REST is now capable of larger-scale analysis and our regulatory data BioMart can deliver faster results. The website is now capable of displaying long-range interactions such as those found in cis-regulated datasets. Finally we have launched a website optimized for mobile devices providing views of genes, variants and phenotypes. Our data is made available without restriction and all code is available from our GitHub organization site (http://github.com/Ensembl) under an Apache 2.0 license.},
author = {Yates, Andrew and Akanni, Wasiu and Amode, M Ridwan and Barrell, Daniel and Billis, Konstantinos and Carvalho-Silva, Denise and Cummins, Carla and Clapham, Peter and Fitzgerald, Stephen and Gil, Laurent and Gir{\'{o}}n, Carlos Garc{\'{i}}a and Gordon, Leo and Hourlier, Thibaut and Hunt, Sarah E and Janacek, Sophie H and Johnson, Nathan and Juettemann, Thomas and Keenan, Stephen and Lavidas, Ilias and Martin, Fergal J and Maurel, Thomas and McLaren, William and Murphy, Daniel N and Nag, Rishi and Nuhn, Michael and Parker, Anne and Patricio, Mateus and Pignatelli, Miguel and Rahtz, Matthew and Riat, Harpreet Singh and Sheppard, Daniel and Taylor, Kieron and Thormann, Anja and Vullo, Alessandro and Wilder, Steven P and Zadissa, Amonida and Birney, Ewan and Harrow, Jennifer and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and Spudich, Giulietta and Trevanion, Stephen J and Cunningham, Fiona and Aken, Bronwen L and Zerbino, Daniel R and Flicek, Paul},
doi = {10.1093/nar/gkv1157},
issn = {1362-4962},
journal = {Nucleic acids research},
month = {jan},
number = {D1},
pages = {D710--6},
pmid = {26687719},
publisher = {Oxford University Press},
title = {{Ensembl 2016.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26687719 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4702834},
volume = {44},
year = {2016}
}
@article{Uppu2014,
abstract = {There have been many studies that depict genotype- phenotype relationships by identifying genetic variants associated with a specific disease. Researchers focus more attention on interactions between SNPs that are strongly associated with disease in the absence of main effect. In this context, a number of machine learning and data mining tools are applied to identify the combinations of multi-locus SNPs in higher order data. However, none of the current models can identify useful SNP- SNP interactions for high dimensional genome data. Detecting these interactions is challenging due to bio-molecular complexities and computational limitations. The goal of this research was to implement associative classification and study its effectiveness for detecting the epistasis in balanced and imbalanced datasets. The proposed approach was evaluated for two locus epistasis interactions using simulated data. The datasets were generated for 5 different penetrance functions by varying heritability, minor allele frequency and sample size. In total, 23,400 datasets were generated and several experiments are conducted to identify the disease causal SNP interactions. The accuracy of classification by the proposed approach was compared with the previous approaches. Though associative classification showed only relatively small improvement in accuracy for balanced datasets, it outperformed existing approaches in higher order multi-locus interactions in imbalanced datasets. Keywords—},
author = {Uppu, Suneetha and Krishna, Aneesh and Gopalan, Raj P.},
doi = {10.1109/BIBE.2014.29},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uppu, Krishna, Gopalan - 2014 - An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Geno.pdf:pdf},
isbn = {978-1-4799-7502-0},
journal = {2014 IEEE International Conference on Bioinformatics and Bioengineering},
keywords = {3,associative classification,billion-base human genome,epistasis,estimated that about 12,million snps occur along,multi-locus,snp-snp interactions,the 3-,the consequences of snps},
pages = {329--333},
title = {{An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Genome}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033602},
year = {2014}
}
@article{Wu2014,
abstract = {Exome sequencing has been widely used in detecting pathogenic nonsynonymous single nucleotide variants (SNVs) for human inherited diseases. However, traditional statistical genetics methods are ineffective in analyzing exome sequencing data, due to such facts as the large number of sequenced variants, the presence of non-negligible fraction of pathogenic rare variants or de novo mutations, and the limited size of affected and normal populations. Indeed, prevalent applications of exome sequencing have been appealing for an effective computational method for identifying causative nonsynonymous SNVs from a large number of sequenced variants. Here, we propose a bioinformatics approach called SPRING (Snv PRioritization via the INtegration of Genomic data) for identifying pathogenic nonsynonymous SNVs for a given query disease. Based on six functional effect scores calculated by existing methods (SIFT, PolyPhen2, LRT, MutationTaster, GERP and PhyloP) and five association scores derived from a variety of genomic data sources (gene ontology, protein-protein interactions, protein sequences, protein domain annotations and gene pathway annotations), SPRING calculates the statistical significance that an SNV is causative for a query disease and hence provides a means of prioritizing candidate SNVs. With a series of comprehensive validation experiments, we demonstrate that SPRING is valid for diseases whose genetic bases are either partly known or completely unknown and effective for diseases with a variety of inheritance styles. In applications of our method to real exome sequencing data sets, we show the capability of SPRING in detecting causative de novo mutations for autism, epileptic encephalopathies and intellectual disability. We further provide an online service, the standalone software and genome-wide predictions of causative SNVs for 5,080 diseases at http://bioinfo.au.tsinghua.edu.cn/spring.},
author = {Wu, Jiaxin and Li, Yanda and Jiang, Rui},
doi = {10.1371/journal.pgen.1004237},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Li, Jiang - 2014 - Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Se.pdf:pdf},
isbn = {1553-7404 (Electronic)\r1553-7390 (Linking)},
issn = {15537404},
journal = {PLoS Genetics},
number = {3},
pmid = {24651380},
title = {{Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Sequencing Studies}},
volume = {10},
year = {2014}
}
@article{Trapnell2009,
abstract = {MOTIVATION: A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.\n\nRESULTS: We mapped the RNA-Seq reads from a recent mammalian RNA-Seq experiment and recovered more than 72% of the splice junctions reported by the annotation-based software from that study, along with nearly 20,000 previously unreported junctions. The TopHat pipeline is much faster than previous systems, mapping nearly 2.2 million reads per CPU hour, which is sufficient to process an entire RNA-Seq experiment in less than a day on a standard desktop computer. We describe several challenges unique to ab initio splice site discovery from RNA-Seq reads that will require further algorithm development.\n\nAVAILABILITY: TopHat is free, open-source software available from http://tophat.cbcb.umd.edu.\n\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L.},
doi = {10.1093/bioinformatics/btp120},
eprint = {9605103},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trapnell, Pachter, Salzberg - 2009 - TopHat Discovering splice junctions with RNA-Seq.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {9},
pages = {1105--1111},
pmid = {19289445},
primaryClass = {cs},
title = {{TopHat: Discovering splice junctions with RNA-Seq}},
volume = {25},
year = {2009}
}
@article{Ren2015,
abstract = {The data explosion in the last decade is revolutionizing diagnostics research and the healthcare industry, offering both opportunities and challenges. These high-throughput "omics" techniques have generated more scientific data in the last few years than in the entire history of mankind. Here we present a brief summary of how "big data" have influenced early diagnosis of complex diseases. We will also review some of the most commonly used "omics" techniques and their applications in diagnostics. Finally, we will discuss the issues brought by these new techniques when translating laboratory discoveries to clinical practice.},
author = {Ren, Guomin and Krawetz, Roman},
doi = {10.3109/1354750X.2015.1105499},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Krawetz - 2015 - Applying computation biology and &quotbig data&quot to develop multiplex diagnostics for complex chronic diseases.pdf:pdf},
issn = {1366-5804},
journal = {Biomarkers : biochemical indicators of exposure, response, and susceptibility to chemicals},
keywords = {Chronic disease,computational biology,diagnostics,osteoarthritis},
number = {8},
pages = {533--9},
pmid = {26809774},
publisher = {Taylor & Francis},
title = {{Applying computation biology and "big data" to develop multiplex diagnostics for complex chronic diseases such as osteoarthritis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26809774 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822 http://www.ncbi.nlm.nih.gov/pubmed/26809774%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822},
volume = {20},
year = {2015}
}
@article{Wenger2017,
abstract = {Manual review of aligned reads for confirmation an; reduce the risk of false positives; and help characterize complex events. The Integrat; and it currently provides a rich set of tools for; validation; and interpretation of NGS datasets; as well as other types of genomic data. Here; we present a short overview of IGV's variant revie; with examples from both cancer and germline datase},
author = {Wenger, Aaron M and Robinson, James T and Zehir, Ahmet and Mesirov, Jill P},
doi = {10.1158/0008-5472.CAN-17-0337},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenger et al. - 2017 - Variant Review with the Integrative Genomics Viewer.pdf:pdf},
journal = {Cancer research},
number = {21},
pages = {31--35},
title = {{Variant Review with the Integrative Genomics Viewer}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {77},
year = {2017}
}
@article{Tsai2016,
abstract = {Effective implementation of precision medicine will be enhanced by a thorough understanding of each patient's genetic composition to better treat his or her presenting symptoms or mitigate the onset of disease. This ideally includes the sequence information of a complete genome for each individual. At Partners HealthCare Personalized Medicine, we have developed a clinical process for whole genome sequencing (WGS) with application in both healthy individuals and those with disease. In this manuscript, we will describe our bioinformatics strategy to efficiently process and deliver genomic data to geneticists for clinical interpretation. We describe the handling of data from FASTQ to the final variant list for clinical review for the final report. We will also discuss our methodology for validating this workflow and the cost implications of running WGS.},
author = {Tsai, Ellen and Shakbatyan, Rimma and Evans, Jason and Rossetti, Peter and Graham, Chet and Sharma, Himanshu and Lin, Chiao-Feng and Lebo, Matthew},
doi = {10.3390/jpm6010012},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai et al. - 2016 - Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine.pdf:pdf},
issn = {2075-4426},
journal = {Journal of Personalized Medicine},
keywords = {NGS,WGS,bioinformatics,clinical sequencing,next generation sequencing,precision medicine,validation},
number = {1},
pages = {12},
pmid = {26927186},
title = {{Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine}},
url = {http://www.mdpi.com/2075-4426/6/1/12/htm http://www.mdpi.com/2075-4426/6/1/12},
volume = {6},
year = {2016}
}
@article{Guo2016,
abstract = {The HID-Ion AmpliSeq??? Identity Panel (the HID Identity Panel) is designed to detect 124-plex single nucleotide polymorphisms (SNPs) with next generation sequencing (NGS) technology on the Ion Torrent PGM??? platform, including 90 individual identification SNPs (IISNPs) on autosomal chromosomes and 34 lineage informative SNPs (LISNPs) on Y chromosome. In this study, we evaluated performance for the HID Identity Panel to provide a reference for NGS-SNP application, focusing on locus strand balance, locus coverage balance, heterozygote balance, and background signals. Besides, several experiments were carried out to find out improvements and limitations of this panel, including studies of species specificity, repeatability and concordance, sensitivity, mixtures, case-type samples and degraded samples, population genetics and pedigrees following the Scientific Working Group on DNA Analysis Methods (SWGDAM) guidelines. In addition, Southern and Northern Chinese Han were investigated to assess applicability of this panel. Results showed this panel led to cross-reactivity with primates to some extent but rarely with non-primate animals. Repeatable and concordant genotypes could be obtained in triplicate with one exception at rs7520386. Full profiles could be obtained from 100 pg input DNA, but the optimal input DNA would be 1 ng???200 pg with 21 initial PCR cycles. A sample with ???20% minor contributor could be considered as a mixture by the number of homozygotes, and full profiles belonging to minor contributors could be detected between 9:1 and 1:9 mixtures with known reference profiles. Also, this assay could be used for case-type samples and degraded samples. For autosomal SNPs (A-SNPs), FST across all 90 loci was not significantly different between Southern and Northern Chinese Han or between male and female samples. All A-SNP loci were independent in Chinese Han population. Except for 18 loci with He <0.4, most of the A-SNPs in the HID Identity Panel presented high polymorphisms. Forensic parameters were calculated as >99.999% for combined discrimination power (CDP), 0.999999724 for combined power of exclusion (CPE), 1.390 ?? 1011 for combined likelihood ratio (CLR) of trios, and 2.361 ?? 106 for CLR of motherless duos. For Y-SNPs, a total of 8 haplotypes were observed with the value of 0.684 for haplotype diversity. As a whole, the HID Identity Panel is a well-performed, robust, reliable and high informative NGS-SNP assay and it can fully meet requirements for individual identification and paternity testing in forensic science.},
author = {Guo, Fei and Zhou, Yishu and Song, He and Zhao, Jinling and Shen, Hongying and Zhao, Bin and Liu, Feng and Jiang, Xianhua},
doi = {10.1016/j.fsigen.2016.07.021},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2016 - Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Evaluation,HID-Ion AmpliSeq???,Identity Panel,Ion Torrent PGM???,Next generation sequencing (NGS),Population genetics,Single nucleotide polymorphism (SNP)},
pages = {73--84},
pmid = {27500651},
publisher = {Elsevier Ireland Ltd},
title = {{Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform}},
url = {http://dx.doi.org/10.1016/j.fsigen.2016.07.021},
volume = {25},
year = {2016}
}
@article{Jurca2016a,
abstract = {Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. We utilized PubMed for the testing. We investigated gene–gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene–gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
issn = {1756-0500},
journal = {BMC Research Notes},
keywords = {Biomedicine general,Life Sciences,Medicine/Public Health,general},
month = {dec},
number = {1},
pages = {236},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@article{Pabinger2014,
abstract = {Recent advances in genome sequencing technologies provide unprecedented opportunities to characterize individual genomic landscapes and identify mutations relevant for diagnosis and therapy. Specifically, whole-exome sequencing using next-generation sequencing (NGS) technologies is gaining popularity in the human genetics community due to the moderate costs, manageable data amounts and straightforward interpretation of analysis results. While whole-exome and, in the near future, whole-genome sequencing are becoming commodities, data analysis still poses significant challenges and led to the development of a plethora of tools supporting specific parts of the analysis workflow or providing a complete solution. Here, we surveyed 205 tools for whole-genome/whole-exome sequencing data analysis supporting five distinct analytical steps: quality assessment, alignment, variant identification, variant annotation and visualization. We report an overview of the functionality, features and specific requirements of the individual tools. We then selected 32 programs for variant identification, variant annotation and visualization, which were subjected to hands-on evaluation using four data sets: one set of exome data from two patients with a rare disease for testing identification of germline mutations, two cancer data sets for testing variant callers for somatic mutations, copy number variations and structural variations, and one semi-synthetic data set for testing identification of copy number variations. Our comprehensive survey and evaluation of NGS tools provides a valuable guideline for human geneticists working on Mendelian disorders, complex diseases and cancers.},
archivePrefix = {arXiv},
arxivId = {209},
author = {Pabinger, Stephan and Dander, Andreas and Fischer, Maria and Snajder, Rene and Sperk, Michael and Efremova, Mirjana and Krabichler, Birgit and Speicher, Michael R. and Zschocke, Johannes and Trajanoski, Zlatko},
doi = {10.1093/bib/bbs086},
eprint = {209},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pabinger et al. - 2014 - A survey of tools for variant analysis of next-generation genome sequencing data.pdf:pdf},
isbn = {4351290037310},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics tools,Cancer,Mendelian disorders,Next-generation sequencing,Variants},
number = {2},
pages = {256--278},
pmid = {23341494},
title = {{A survey of tools for variant analysis of next-generation genome sequencing data}},
volume = {15},
year = {2014}
}
@article{Pietrelli2017a,
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
publisher = {Oxford University Press},
title = {{myVCF: a desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@article{Cartwright2012,
abstract = {Recent advances in high-throughput DNA sequencing technologies and associated statistical analyses have enabled in-depth analysis of whole-genome sequences. As this technology is applied to a growing number of individual human genomes, entire families are now being sequenced. Information contained within the pedigree of a sequenced family can be leveraged when inferring the donors' genotypes. The presence of a de novo mutation within the pedigree is indicated by a violation of Mendelian inheritance laws. Here, we present a method for probabilistically inferring genotypes across a pedigree using high-throughput sequencing data and producing the posterior probability of de novo mutation at each genomic site examined. This framework can be used to disentangle the effects of germline and somatic mutational processes and to simultaneously estimate the effect of sequencing error and the initial genetic variation in the population from which the founders of the pedigree arise. This approach is examined in detail through simulations and areas for method improvement are noted. By applying this method to data from members of a well-defined nuclear family with accurate pedigree information, the stage is set to make the most direct estimates of the human mutation rate to date.},
author = {Cartwright, Reed A and Keebler, Jonathan E M and Carolina, North and Stone, Eric A and Hussin, Julie},
doi = {10.2202/1544-6115.1713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright et al. - 2012 - S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput S.pdf:pdf},
isbn = {1544-6115 (Electronic)\r1544-6115 (Linking)},
issn = {1544-6115},
journal = {Statistical applications in genetics and molecular biology},
keywords = {Algorithms,Alleles,Computer Simulation,DNA Mutational Analysis,Family,Genetic,Genome,Genotype,High-Throughput Nucleotide Sequencing,Human,Humans,Models,Mutation,Pedigree,Probability,ROC Curve},
number = {2},
pmid = {22499693},
title = {{S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3728889&tool=pmcentrez&rendertype=abstract%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/22499693%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3728889},
volume = {11},
year = {2012}
}
@article{Bajcsy2005,
abstract = {Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.},
author = {Bajcsy, Peter and Han, Jiawei and Liu, Lei and Yang, Jiong},
doi = {10.1007/1-84628-059-1_2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bajcsy et al. - 2005 - Survey of Biodata Analysis from a Data Mining Perspective.pdf:pdf},
isbn = {1852336714},
journal = {Data Mining in Bioinformatics},
pages = {9--39},
title = {{Survey of Biodata Analysis from a Data Mining Perspective}},
url = {http://dx.doi.org/10.1007/1-84628-059-1_2},
year = {2005}
}
@article{Naulaerts,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solu-tions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address vari-ations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinfor-matics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future poten-tial and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - Unknown - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
title = {{A primer to frequent itemset mining for bioinformatics}}
}
@misc{Paez2012,
abstract = {El contexto din{{\'{a}}}mico y competitivo de la organizaci{{\'{o}}}n actual exige permanentes soluciones inform{{\'{a}}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{{\'{o}}}n innovadora al problema del manejo de datos enmarcando dicha soluci{{\'{o}}}n mayormente, desde el punto de vista tecnol{{\'{o}}}gico, sin considerar los aspectos organizacionales y metodol{{\'{o}}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
issn = {0120-341X},
keywords = {Sistemas de informaci{{\'{o}}}n en administraci{{\'{o}}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{{\'{o}}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Wang2017,
author = {Wang, Fei and Li, Xiao-li and Wang, Jason T L and Ng, See-kiong},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Guest Editorial Special Section on Biological Data Mining and Its Applications in Healthcare.pdf:pdf},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
volume = {14},
year = {2017}
}
@article{Paila2013,
abstract = {注释你 Mutation\r\n可以研究家系遗传病},
archivePrefix = {arXiv},
arxivId = {1304.4860},
author = {Paila, Umadevi and Chapman, Brad A. and Kirchner, Rory and Quinlan, Aaron R.},
doi = {10.1371/journal.pcbi.1003153},
eprint = {1304.4860},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paila et al. - 2013 - GEMINI Integrative Exploration of Genetic Variation and Genome Annotations.pdf:pdf},
isbn = {10.1371/journal.pcbi.1003153},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {7},
pmid = {23874191},
title = {{GEMINI: Integrative Exploration of Genetic Variation and Genome Annotations}},
volume = {9},
year = {2013}
}
@article{Pandey2016,
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1186/s12859-016-0915-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - ClinQC a tool for quality control and cleaning of Sanger and NGS data in clinical research.pdf:pdf},
isbn = {14712105 (Electronic)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Sanger sequencing,Next generation sequencing,Quali,molecular diagnostic testing,next generation sequencing,quality control,sanger sequencing},
number = {1},
pages = {56},
pmid = {26830926},
publisher = {BMC Bioinformatics},
title = {{ClinQC: a tool for quality control and cleaning of Sanger and NGS data in clinical research}},
url = {http://www.biomedcentral.com/1471-2105/17/56},
volume = {17},
year = {2016}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics_pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Li2009a,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Canuel2015,
author = {Canuel, V. and Rance, B. and Avillach, P. and Degoulet, P. and Burgun, A.},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data a review of publicly available solutions.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {biomedical research,clinical data,high-throughput technologies,information storage,translational medical research},
number = {2},
pages = {280--290},
title = {{Translational research platforms integrating clinical and omics data: a review of publicly available solutions}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu006},
volume = {16},
year = {2015}
}
@misc{Illumina2017,
author = {Illumina},
title = {{Whole Exome Sequencing | Detect exonic variants}},
url = {http://www.illumina.com/techniques/sequencing/dna-sequencing/targeted-resequencing/exome-sequencing.html},
urldate = {2017-11-15},
year = {2017}
}
@article{Dave2017,
abstract = {The era of huge data is snowballing at frequent swiftness in size (volume) and in different formats (variety). This data which comes from various sources e.g. media, communication devices, internet, business etc. and there are many difficulties and challenges that one faces while handling it. Data mining is a process intended to reconnoiter analytical data (typically business or market associated data - also acknowledged as "Big data"). There are several data mining techniques such as outlier analysis, organization, clustering, prediction and association rule mining. In this paper we have discussed several applications and the importance of clustering. To examine the huge volume of data, clustering algorithms aid in providing a powerful meta-Iearning tool. Numerous clustering techniques (including traditional and the recently developed) in reference to large data sets with their pros & cons are being discussed in this paper.},
author = {Dave, Meenu and Gianey, Hemant},
doi = {10.1109/SYSMART.2016.7894544},
file = {:home/jennifer/Descargas/dave2016.pdf:pdf},
isbn = {9781509035434},
journal = {Proceedings of the 5th International Conference on System Modeling and Advancement in Research Trends, SMART 2016},
keywords = {Clustering,Data Mining (DM),Density Based Methods (DBM),Grid Based Methods (GBM),Hierarchical Methods (HM),Partition Methods (PM)},
pages = {328--333},
title = {{Different clustering algorithms for Big Data analytics: A review}},
year = {2017}
}
@article{Mckenna2010,
abstract = {The concept of absorptive capacity was introduced by Cohen and Levinthal in 1989. Since then it has been enhanced through reconceptualizations and extended by various empirical studies. Despite the growing interest in absorptive capacity it is unclear what this large stream of papers has collectively accomplished. The used definitions, antecedents, components and outcomes of the construct are extremely heterogeneous. Due to this heterogeneity, the empirical study of the construct remains difficult. There is no standard measure and no standard method of measurement, which can be used in empirical research. To bring more clarity into this research area, this paper provides a critical review of previous empirical treatments of absorptive capacity. For this purpose, different methods of measurement are classified in the following way: within quantitative methods proxy indicators and perceptive instruments are differentiated. Proxy indicators use single firm-level data for measuring absorptive capacity and can be input-oriented (R&D efforts, R&D human capital) or output-oriented (R&D patents, R&D publications). Perceptive instruments imply that researchers develop single questions or a set of questions, which reflect absorptive capacity or parts of it at the operational level. The main weakness of both proxy indicators and perceptive instruments is that they don't meet the complexity and emergence of the construct. Only few qualitative studies have started to adopt a new perspective, recognizing the process and practice-based character of absorptive capacity. In summary, the critical review prints out the necessity of advancing research in this area. For this reason, we set out to develop an alternative approach to capture absorptive capacity. It is a practice-oriented approach that allows studying actual absorptive practices in real world situations and enables researchers to capture the complex, embedded, and context-dependent patterns of acting.},
author = {Schmidt, Stephanie},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Proceedings of the International Conference on Intellectual Capital, Knowledge Management & Organizational Learning},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data.}},
volume = {20},
year = {2009}
}
@inproceedings{Buchanan2012,
author = {Buchanan, Carrie C. and Wallace, John R. and Frase, Alex T. and Torstenson, Eric S. and Pendergrass, Sarah A. and Ritchie, Marylyn D.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29066-4_18},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchanan et al. - 2012 - A biologically informed method for detecting associations with rare variants.pdf:pdf},
isbn = {9783642290657},
issn = {03029743},
keywords = {Collapsing Tool,Pathway Analysis,Prior Knowledge,Rare Variants},
title = {{A biologically informed method for detecting associations with rare variants}},
year = {2012}
}
@article{Parla,
abstract = {Background: Human exome resequencing using commercial target capture kits has been and is being used for sequencing large numbers of individuals to search for variants associated with various human diseases. We rigorously evaluated the capabilities of two solution exome capture kits. These analyses help clarify the strengths and limitations of those data as well as systematically identify variables that should be considered in the use of those data. Results: Each exome kit performed well at capturing the targets they were designed to capture, which mainly corresponds to the consensus coding sequences (CCDS) annotations of the human genome. In addition, based on their respective targets, each capture kit coupled with high coverage Illumina sequencing produced highly accurate nucleotide calls. However, other databases, such as the Reference Sequence collection (RefSeq), define the exome more broadly, and so not surprisingly, the exome kits did not capture these additional regions. Conclusions: Commercial exome capture kits provide a very efficient way to sequence select areas of the genome at very high accuracy. Here we provide the data to help guide critical analyses of sequencing data derived from these products. Background},
author = {Parla, Jennifer S and Iossifov, Ivan and Grabill, Ian and Spector, Mona S and Kramer, Melissa and Mccombie, W Richard},
doi = {10.1186/gb-2011-12-9-r97},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parla et al. - Unknown - A comparative analysis of exome capture.pdf:pdf},
title = {{A comparative analysis of exome capture}}
}
@article{Sujansky2001,
abstract = {The rapid expansion of biomedical knowledge, reduction in computing costs, and spread of internet access have created an ocean of electronic data. The decentralized nature of our scientific community and healthcare system, however, has resulted in a patchwork of diverse, or heterogeneous, database implementations, making access to and aggregation of data across databases very difficult. The database heterogeneity problem applies equally to clinical data describing individual patients and biological data characterizing our genome. Specifically, databases are highly heterogeneous with respect to the data models they employ, the data schemas they specify, the query languages they support, and the terminologies they recognize. Heterogeneous database systems attempt to unify disparate databases by providing uniform conceptual schemas that resolve representational heterogeneities, and by providing querying capabilities that aggregate and integrate distributed data. Research in this area has applied a variety of database and knowledge-based techniques, including semantic data modeling, ontology definition, query translation, query optimization, and terminology mapping. Existing systems have addressed heterogeneous database integration in the realms of molecular biology, hospital information systems, and application portability.},
author = {Sujansky, W},
doi = {10.1006/jbin.2001.1024},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sujansky - 2001 - Heterogeneous database integration in biomedicine.pdf:pdf},
isbn = {1532-0464},
issn = {1532-0464},
journal = {Journal of biomedical informatics},
keywords = {ancient mariner,data warehouse,data warehouse.,database,database integration,drink,everywhere,federated database,heterogeneous database,nor any drop to,samuel taylor coleridge,the rime of the,water},
number = {2001},
pages = {285--298},
pmid = {11977810},
title = {{Heterogeneous database integration in biomedicine.}},
volume = {34},
year = {2001}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@article{Kutzera2017,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-642-15120-0},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Data Integration in the Life Sciences.pdf:pdf},
isbn = {978-3-642-15119-4},
pages = {22--28},
title = {{Data Integration in the Life Sciences}},
url = {http://link.springer.com/10.1007/978-3-642-15120-0},
volume = {6254},
year = {2017}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.$\$n$\$nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.$\$n$\$nDATA SOURCES: Literature review and authors' experience.$\$n$\$nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M and Durtschi, Jacob D and Margraf, Rebecca L and Voelkerding, Karl V},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)$\$r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Parnell2014,
abstract = {BACKGROUND: Genetic understanding of complex traits has developed immensely over the past decade but remains hampered by incomplete descriptions of contribution to phenotypic variance. Gene-environment (GxE) interactions are one of these contributors and in the guise of diet and physical activity are important modulators of cardiometabolic phenotypes and ensuing diseases.\n\nRESULTS: We mined the scientific literature to collect GxE interactions from 386 publications for blood lipids, glycemic traits, obesity anthropometrics, vascular measures, inflammation and metabolic syndrome, and introduce CardioGxE, a gene-environment interaction resource. We then analyzed the genes and SNPs supporting cardiometabolic GxEs in order to demonstrate utility of GxE SNPs and to discern characteristics of these important genetic variants. We were able to draw many observations from our extensive analysis of GxEs. 1) The CardioGxE SNPs showed little overlap with variants identified by main effect GWAS, indicating the importance of environmental interactions with genetic factors on cardiometabolic traits. 2) These GxE SNPs were enriched in adaptation to climatic and geographical features, with implications on energy homeostasis and response to physical activity. 3) Comparison to gene networks responding to plasma cholesterol-lowering or regression of atherosclerotic plaques showed that GxE genes have a greater role in those responses, particularly through high-energy diets and fat intake, than do GWAS-identified genes for the same traits. Other aspects of the CardioGxE dataset were explored.\n\nCONCLUSIONS: Overall, we demonstrate that SNPs supporting cardiometabolic GxE interactions often exhibit transcriptional effects or are under positive selection. Still, not all such SNPs can be assigned potential functional or regulatory roles often because data are lacking in specific cell types or from treatments that approximate the environmental factor of the GxE. With research on metabolic related complex disease risk embarking on genome-wide GxE interaction tests, CardioGxE will be a useful resource.},
author = {Parnell, Laurence D and Blokker, Britt A and Dashti, Hassan S and Nesbeth, Paula-Dene and Cooper, Brittany Elle and Ma, Yiyi and Lee, Yu-Chi and Hou, Ruixue and Lai, Chao-Qiang and Richardson, Kris and Ordov{\'{a}}s, Jos{\'{e}} M},
doi = {10.1186/1756-0381-7-21},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnell et al. - 2014 - CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.pdf:pdf},
issn = {1756-0381},
journal = {BioData mining},
pages = {21},
pmid = {25368670},
title = {{CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4217104&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {2014}
}
@article{Williams2011,
abstract = {Editorial Biological data mining is playing an increasingly important role throughout the spectrum of biological and biomedical research with broad implications for the understanding of life science questions such as the tree of life and practical applica-tions of such knowledge to improving human health. Perhaps nowhere is data mining needed more than the emerging discipline of precision medicine. The ability to predict individual risk of presenting with a disease or response to treatment is at the core of the concept of precision medicine, which is gaining ever-increasing levels of traction in the era of technology-driven measurement of biological systems. This has become especially important with the new Presidential initiative on precision medi-cine in the United States [1]. It is obvious to the readers of BioData Mining that this will require careful analyses of large and often complex data sets to best translate information into increasingly individualized risk. Here we ask why improved and appropriate data mining is not only positive but a vast improvement on most current analyses of genomic data. The answer lies to some extent in elucidating the present practice of -omic analyses and how we will need to expand it. Many current -omic approaches rely on univariate and linear analyses that can often miss the underlying architecture of complex traits. For example, univariate analyses of single genetic markers for association with disease risk, prognosis, or drug response that are the analytical standards for genetic analyses of human disease, and have been promoted as a means to develop personalized or more recently precision medicine, make many assumptions about architecture. Given the interest in precision medicine, it is important to ask explicitly what is being assayed in these types of studies that have been argued, incorrectly we believe, as the precursors to precision medicine. Most human geneticists study the association of genetic variants, be they common or rare, assessed across moderate to large samples of cases and controls. The effect of each allelic substitution is then measured as it associates with a particular phenotype. These estimates can provide useful population level risks; however, they are simply the average effect of an allelic substitution across the population, not necessarily predictive of results in an individual or a subgroup. The concept of average allelic effect is one that is well developed in quantitative genetics, but by its very name is suggestive not of precision medicine but of average medicine. Hence, it is possible in a large outbreeding},
author = {Williams, Scott M and Moore, Jason H},
doi = {10.1186/s13040-015-0049-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Moore - 2011 - Lumping versus splitting the need for biological data mining in precision medicine.pdf:pdf},
title = {{Lumping versus splitting: the need for biological data mining in precision medicine}},
volume = {8},
year = {2011}
}
@article{Poliakov2015,
author = {Poliakov, Eugenia and Cooper, David N and Stepchenkova, Elena I and Rogozin, Igor B},
doi = {10.1155/2015/364960},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poliakov et al. - 2015 - Genetics in genomic era.pdf:pdf},
issn = {2090-3154},
journal = {Genetics research international},
pages = {364960},
pmid = {25883807},
publisher = {Hindawi},
title = {{Genetics in genomic era.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25883807 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4390167},
volume = {2015},
year = {2015}
}
@article{Arias-blanco2015,
author = {Arias-blanco, Juan Felipe and Fonseca-mendoza, Dora Janeth and Gamboa-garay, Oscar},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arias-blanco, Fonseca-mendoza, Gamboa-garay - 2015 - FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2.pdf:pdf},
journal = {Revista Colombiana de Obstetricia y Ginecolog{\'{i}}a},
number = {4},
pages = {287--296},
title = {{FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2 EN UNA MUESTRA DE MUJERES COLOMBIANAS CON SOSPECHA DE S{\'{I}}NDROME DE C{\'{A}}NCER DE MAMA HEREDITARIO: SERIE DE CASOS}},
url = {http://www.nature.com/articles/srep12376},
volume = {65},
year = {2015}
}
@incollection{Kutzera2017b,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
year = {2017}
}
@article{Mayer2017,
author = {Mayer, Gerhard and Quast, Christian and Felden, Janine and Lange, Matthias and Prinz, Manuel and P{\"{u}}hler, Alfred and Lawerenz, Chris and Scholz, Uwe and Gl{\"{o}}ckner, Frank Oliver and M{\"{u}}ller, Wolfgang and Marcus, Katrin and Eisenacher, Martin},
doi = {10.1093/bib/bbx140},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer et al. - 2017 - A generally applicable lightweight method for calculating a value structure for tools and services in bioinformati.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {oct},
title = {{A generally applicable lightweight method for calculating a value structure for tools and services in bioinformatics infrastructure projects}},
url = {http://academic.oup.com/bib/article/doi/10.1093/bib/bbx140/4582343},
year = {2017}
}
@article{Bao2014,
abstract = {The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigentic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing vaiants and discover gene targets. In this review, we outline the general framework of whole exome processing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis and summarization in the field of bioinformatics.},
author = {Bao, Riyue and Huang, Lei and Andrade, Jorge and Tan, Wei and Kibbe, Warren a and Jiang, Hongmei and Feng, Gang},
doi = {10.4137/CIN.S13779.Received},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bao et al. - 2014 - Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequenci.pdf:pdf},
isbn = {1176-9351 (Electronic)\r1176-9351 (Linking)},
issn = {1176-9351},
journal = {Libertas Academica},
keywords = {10,13,4137,67,82 doi,a,and data management for,and statistical analysis of,applications,bao et al,big data,cancer data,cancer informatics 2014,cin,citation,classification,indel,next generation sequencing,predictive modelling,review of current methods,s13779,s2,sequence alignment,snv,supplement,the bioinformatics analysis of,variant analysis,whole exome sequencing},
pages = {67--82},
pmid = {25288881},
title = {{Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing}},
volume = {13},
year = {2014}
}
@inproceedings{Moore99,
author = {Moore, R and Lopes, J},
booktitle = {TEMPLATE'06, 1st International Conference on Template Production},
publisher = {SCITEPRESS},
title = {{Paper templates}},
year = {1999}
}
@book{Herraez2012,
address = {Barcelona},
author = {Herr{\'{a}}ez, Angel},
isbn = {978-84-8086-647-7},
pages = {241},
publisher = {Elsevier Ltd},
title = {{Biolog{\'{i}}a Molecular e Ingenier{\'{i}}a Gen{\'{e}}tica. 2{\textordfeminine} ed.}},
year = {2012}
}
@article{Staccini2014,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and {Vanden Berghe}, Wim and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - 2013 - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
number = {2},
pages = {3066--3076},
pmid = {24162173},
title = {{A primer to frequent itemset mining for bioinformatics.}},
url = {http://link.springer.com/10.1007/978-2-8178-0478-1_13%5Cnhttp://link.springer.com/chapter/10.1007/978-2-8178-0478-1_13 http://linkinghub.elsevier.com/retrieve/pii/S0957417408000195%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24162173},
volume = {36},
year = {2013}
}
@article{Brueffer2015,
abstract = {Summary: TopHat is a popular spliced junction mapper for RNA sequencing data, and writes files in the BAM format - the binary version of the Sequence Alignment/Map (SAM) format. BAM is the standard exchange format for aligned sequencing reads, thus correct format implementation is paramount for software interoperability and correct analysis. However, TopHat writes its unmapped reads in a way that is not compatible with other software that implements the SAM/BAM format. We have developed TopHat-Recondition, a post-processor for TopHat unmapped reads that restores read information in the proper format. TopHat-Recondition thus enables downstream software to process the plethora of BAM files written by TopHat. Availability and implementation: TopHat-Recondition is implemented in Python using the Pysam library and is freely available under a 2-clause BSD license on GitHub: https://github.com/cbrueffer/tophat-recondition. Contact: christian.brueffer@med.lu.se, lao.saal@med.lu.se},
author = {Brueffer, Christian and Saal, Lao H},
doi = {10.1101/033530},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brueffer, Saal - 2015 - TopHat-Recondition A post-processor for TopHat unmapped reads.pdf:pdf},
issn = {1471-2105},
journal = {bioRxiv},
keywords = {deep sequencing,rna-seq,sequence alignment,sequence analysis},
pages = {1--6},
pmid = {27142976},
publisher = {BMC Bioinformatics},
title = {{TopHat-Recondition: A post-processor for TopHat unmapped reads}},
url = {http://dx.doi.org/10.1186/s12859-016-1058-x},
volume = {2},
year = {2015}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192%7B&%7Dtool=pmcentrez%7B&%7Drendertype=abstract},
volume = {9},
year = {2014}
}
@article{Hehir-Kwa2016,
abstract = {Structural variation (SV) represents a major source of differences between individual human genomes and has been linked to disease phenotypes. However, the majority of studies pro-vide neither a global view of the full spectrum of these variants nor integrate them into reference panels of genetic variation. Here, we analyse whole genome sequencing data of 769 individuals from 250 Dutch families, and provide a haplotype-resolved map of 1.9 million genome variants across 9 different variant classes, including novel forms of complex indels, and retrotransposition-mediated insertions of mobile elements and processed RNAs. A large proportion are previously under reported variants sized between 21 and 100 bp. We detect 4 megabases of novel sequence, encoding 11 new transcripts. Finally, we show 191 known, trait-associated SNPs to be in strong linkage disequilibrium with SVs and demonstrate that our panel facilitates accurate imputation of SVs in unrelated individuals.},
author = {Hehir-Kwa, Jayne Y and Marschall, Tobias and Kloosterman, Wigard P and Francioli, Laurent C},
doi = {10.1038/ncomms12989},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hehir-Kwa et al. - 2016 - ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural var.pdf:pdf},
journal = {Nature Communications},
number = {11},
title = {{ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural variants}},
volume = {7},
year = {2016}
}
@article{Field2015,
abstract = {A diversity of tools is available for identification of variants from genome sequence data. Given the current complexity of incorporating external software into a genome analysis infrastructure, a tendency exists to rely on the results from a single tool alone. The quality of the output variant calls is highly variable however, depending on factors such as sequence library quality as well as the choice of short-read aligner, variant caller, and variant caller filtering strategy. Here we present a two-part study first using the high quality 'genome in a bottle' reference set to demonstrate the significant impact the choice of aligner, variant caller, and variant caller filtering strategy has on overall variant call quality and further how certain variant callers outperform others with increased sample contamination, an important consideration when analyzing sequenced cancer samples. This analysis confirms previous work showing that combining variant calls of multiple tools results in the best quality resultant variant set, for either specificity or sensitivity, depending on whether the intersection or union, of all variant calls is used respectively. Second, we analyze a melanoma cell line derived from a control lymphocyte sample to determine whether software choices affect the detection of clinically important melanoma risk-factor variants finding that only one of the three such variants is unanimously detected under all conditions. Finally, we describe a cogent strategy for implementing a clinical variant detection pipeline; a strategy that requires careful software selection, variant caller filtering optimizing, and combined variant calls in order to effectively minimize false negative variants. While implementing such features represents an increase in complexity and computation the results offer indisputable improvements in data quality.},
author = {Field, Matthew A. and Cho, Vicky and Andrews, T. Daniel and Goodnow, Chris C.},
doi = {10.1371/journal.pone.0143199},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Field et al. - 2015 - Reliably detecting clinically important variants requires both combined variant calls and optimized filtering stra.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--19},
title = {{Reliably detecting clinically important variants requires both combined variant calls and optimized filtering strategies}},
volume = {10},
year = {2015}
}
@article{Urbanczyk2016,
author = {Urbanczyk, Tomas and Peter, Lukas},
doi = {10.1016/j.ifacol.2016.12.047},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urbanczyk, Peter - 2016 - Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Co.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Database,MySQL,RFID,The Urgent department,environment analysis},
number = {25},
pages = {278--283},
publisher = {Elsevier B.V.},
title = {{Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Concept}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2405896316326830},
volume = {49},
year = {2016}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey#0},
volume = {16},
year = {2004}
}
@article{Ghoneim2014,
abstract = {BACKGROUND: Insertions/deletions (indels) are the second most common type of genomic variant and the most common type of structural variant. Identification of indels in next generation sequencing data is a challenge, and algorithms commonly used for indel detection have not been compared on a research cohort of human subject genomic data. Guidelines for the optimal detection of biologically significant indels are limited. We analyzed three sets of human next generation sequencing data (48 samples of a 200 gene target exon sequencing, 45 samples of whole exome sequencing, and 2 samples of whole genome sequencing) using three algorithms for indel detection (Pindel, Genome Analysis Tool Kit's UnifiedGenotyper and HaplotypeCaller). RESULTS: We observed variation in indel calls across the three algorithms. The intersection of the three tools comprised only 5.70% of targeted exon, 19.52% of whole exome, and 14.25% of whole genome indel calls. The majority of the discordant indels were of lower read depth and likely to be false positives. When software parameters were kept consistent across the three targets, HaplotypeCaller produced the most reliable results. Pindel results did not validate well without adjustments to parameters to account for varied read depth and number of samples per run. Adjustments to Pindel's M (minimum support for event) parameter improved both concordance and validation rates. Pindel was able to identify large deletions that surpassed the length capabilities of the GATK algorithms. CONCLUSIONS: Despite the observed variability in indel identification, we discerned strengths among the individual algorithms on specific data sets. This allowed us to suggest best practices for indel calling. Pindel's low validation rate of indel calls made in targeted exon sequencing suggests that HaplotypeCaller is better suited for short indels and multi-sample runs in targets with very high read depth. Pindel allows for optimization of minimum support for events and is best used for detection of larger indels at lower read depths.},
author = {Ghoneim, Dalia H and Myers, Jason R and Tuttle, Emily and Paciorkowski, Alex R},
doi = {10.1186/1756-0500-7-864},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghoneim et al. - 2014 - Comparison of insertiondeletion calling algorithms on human next-generation sequencing data.pdf:pdf},
isbn = {1756-0500 (Electronic)\r1756-0500 (Linking)},
issn = {1756-0500},
journal = {BMC research notes},
keywords = {concordance,gatk,indels,next generation sequencing,pindel,validation},
number = {1},
pages = {864},
pmid = {25435282},
title = {{Comparison of insertion/deletion calling algorithms on human next-generation sequencing data.}},
url = {http://www.biomedcentral.com/1756-0500/7/864},
volume = {7},
year = {2014}
}
@article{Luo2017,
author = {Luo, Ping and Ruan, Jishou and Member, Senior},
doi = {10.1109/TCBB.2017.2770120},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Ruan, Member - 2017 - Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data.pdf:pdf},
journal = {IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS 1 Disease},
number = {306},
pages = {1--11},
title = {{Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data}},
volume = {1},
year = {2017}
}
@article{BernalAcevedo2011,
abstract = {86 osCar bernaL-aCevedo • juan CaMiLo forero-CaMaCHo Rev. Gerenc. Polit. Salud, Bogot{\'{a}} (Colombia), 10 (21): 85-100, julio-diciembre de 2011 Resumen Objetivo: caracterizar y evaluar los sistemas de informaci{\'{o}}n del sector salud en Colombia. Metodolog{\'{i}}a: Se desarroll{\'{o}} un marco conceptual que incluy{\'{o}} contexto legal del pa{\'{i}}s y confor-maci{\'{o}}n de sistemas de informaci{\'{o}}n en otros pa{\'{i}}ses. Posteriormente se caracteriz{\'{o}} el sistema de informaci{\'{o}}n de salud colombiano, a partir de entrevistas con actores relevantes y literatura pertinente. Finalmente, se analiz{\'{o}} la conformaci{\'{o}}n del sistema, el flujo de informaci{\'{o}}n y las fortalezas y debilidades de {\'{e}}ste, para la posterior formulaci{\'{o}}n de recomendaciones. Resultados y conclusiones: el sistema de informaci{\'{o}}n en salud colombiano se encuentra fragmentado y presenta problemas de calidad, situaci{\'{o}}n similar a la de otros pa{\'{i}}ses. Es esencial el desarrollo de una cultura de producci{\'{o}}n, difusi{\'{o}}n y utilizaci{\'{o}}n de la informaci{\'{o}}n. Se debe aprovechar el momento de cambio que sufre el sistema de salud para buscar la mejor{\'{i}}a de la informaci{\'{o}}n. Los mecanismos de captura de la informaci{\'{o}}n requieren una simplificaci{\'{o}}n y estandarizaci{\'{o}}n. Palabras clave autor: servicios de informaci{\'{o}}n, Colombia, comunicaci{\'{o}}n, salud p{\'{u}}blica, informaci{\'{o}}n, administraci{\'{o}}n en salud p{\'{u}}blica.},
author = {{Bernal Acevedo}, Oscar and {Forero Camacho}, Juan Camilo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernal Acevedo, Forero Camacho - 2011 - Sistemas de informaci{\'{o}}n en el sector salud en Colombia.pdf:pdf},
journal = {Revista Gerencia y Politicas de Salud},
keywords = {de informaci{\'{o}}n en el,sector salud},
number = {21},
pages = {85--100},
title = {{Sistemas de informaci{\'{o}}n en el sector salud en Colombia}},
url = {http://www.scielo.org.co/pdf/rgps/v10n21/v10n21a06.pdf},
volume = {10},
year = {2011}
}
@article{Dudley2010,
abstract = {With the continued exponential expansion of publicly available genomic data and access to low-cost, high-throughput molecular technologies for profiling patient populations, computational technologies and informatics are becoming vital considerations in genomic medicine. Although cloud computing technology is being heralded as a key enabling technology for the future of genomic research, available case studies are limited to applications in the domain of high-throughput sequence data analysis. The goal of this study was to evaluate the computational and economic characteristics of cloud computing in performing a large-scale data integration and analysis representative of research problems in genomic medicine. We find that the cloud-based analysis compares favorably in both performance and cost in comparison to a local computational cluster, suggesting that cloud computing technologies might be a viable resource for facilitating large-scale translational research in genomic medicine.},
author = {Dudley, Joel T and Pouliot, Yannick and Chen, Rong and Morgan, Alexander A and Butte, Atul J},
doi = {10.1186/gm172},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudley et al. - 2010 - Translational bioinformatics in the cloud an affordable alternative.pdf:pdf},
isbn = {1756-994X (Electronic)},
issn = {1756-994X},
journal = {Genome medicine},
number = {8},
pages = {51},
pmid = {20691073},
title = {{Translational bioinformatics in the cloud: an affordable alternative.}},
url = {http://genomemedicine.com/content/2/8/51},
volume = {2},
year = {2010}
}
@article{Li2014,
abstract = {In ''Omics'' era of the life sciences, data is presented in many forms, which represent the information at various levels of bio-logical systems, including data about genome, transcriptome, epigenome, proteome, metabolome, molecular imaging, molec-ular pathways, different population of people and clinical/med-ical records. The biological data is big, and its scale has already been well beyond petabyte (PB) even exabyte (EB). Nobody doubts that the biological data will create huge amount of val-ues, if scientists can overcome many challenges, e.g., how to handle the complexity of information, how to integrate the data from very heterogeneous resources, what kind of principles or standards to be adopted when facing with the big data. Tools and techniques for analyzing big biological data enable us to translate massive amount of information into a better under-standing of the basic biomedical mechanisms, which can be fur-ther applied to translational or personalized medicine. Today, big data is one of the hottest topics in information science, but its concept can be misleading or confusing. The name itself suggests huge amount of data, which, however, represents only one aspect. In general, big data has four impor-tant features, so called four V's: volume of data, velocity of processing the data, variability of data sources, and veracity of the data quality. These four hallmarks of big data require to be characterized by special theory and technology; however, currently there is no satisfactory solution. Now, more biolo-gists are involved with the big data due to the rapid advance of high-throughput biotechnologies. As an example, the Human Genome Project utilized the expertise, infrastructure, and people from 20 institutions and took 13 years of work with over $3 billion to determine the whole genome structure of approximately three billion nucleotides. But now we can sequence a whole human genome for $1000 and within three days. We have spent decades struggling to collect enough bio-logical and biomedical data, but when big data overwhelms us, are we ready to face the challenge? The new bottleneck to this problem in biology is how to reveal the essential mechanisms of biological systems by understanding the big noisy data. Life sciences today need more robust, expressive, computable, quantitative, accurate and precise ways to handle the big data. As a matter of fact, recent works in this area have already brought remarkable advantage and opportunities, which implies the central roles of bioinformatics and bioinformati-cians in the future research of the biological and biomedical fields. In the following text, we describe several aspects of big biological data based on our recent studies.},
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {1672-0229},
journal = {Genomics, Proteomics \& Bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Data Mining,Data Mining: methods,Gene Expression Profiling,High-Throughput Screening Assays,Humans,Software},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China},
title = {{Big Biological Data: Challenges and Opportunities Expanding volume of the big biological data and its bonanza}},
url = {http://www.sciencedirect.com/science/article/pii/S1672022914001041},
volume = {12},
year = {2014}
}
@article{Cassa2017,
abstract = {Shamil Sunyaev, David Beier and colleagues report an analysis of the fitness effects of heterozygous protein-truncating variants from the Exome Aggregation Consortium. They find that high heterozygous selection coefficients are enriched in Mendelian disease-associated genes and essential mouse genes, suggesting that this coefficient can be used to prioritize candidate disease-associated genes from clinical exome-sequencing data.},
author = {Cassa, Christopher A. and Weghorn, Donate and Balick, Daniel J. and Jordan, Daniel M. and Nusinow, David and Samocha, Kaitlin E. and O'Donnell-Luria, Anne and MacArthur, Daniel G. and Daly, Mark J. and Beier, David R. and Sunyaev, Shamil R.},
doi = {10.1038/ng.3831},
file = {:home/jennifer/Descargas/cassa2017.pdf:pdf},
issn = {15461718},
journal = {Nature Genetics},
number = {5},
pages = {806--810},
pmid = {28369035},
publisher = {Nature Publishing Group},
title = {{Estimating the selective effects of heterozygous protein-truncating variants from human exome data}},
url = {http://dx.doi.org/10.1038/ng.3831},
volume = {49},
year = {2017}
}
@article{Kashyap2015,
abstract = {Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.},
archivePrefix = {arXiv},
arxivId = {1506.05101},
author = {Kashyap, Hirak and Ahmed, Hasin Afzal and Hoque, Nazrul and Roy, Swarup and Bhattacharyya, Dhruba Kumar},
eprint = {1506.05101},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kashyap et al. - 2014 - Big Data Analytics in Bioinformatics A Machine Learning Perspective.pdf:pdf},
journal = {Journal of Latex Class Files},
number = {9},
pages = {1--20},
title = {{Big Data Analytics in Bioinformatics: A Machine Learning Perspective}},
url = {http://arxiv.org/abs/1506.05101},
volume = {13},
year = {2014}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells, Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,Sequence Analysis, RNA,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192&tool=pmcentrez&rendertype=abstract},
volume = {9},
year = {2014}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{Holzinger2013,
author = {Holzinger, Andreas and Zupan, Mario},
doi = {10.1186/1471-2105-14-191},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holzinger, Zupan - 2013 - KNODWAT A scientific framework application for testing knowledge discovery methods for the biomedical domain.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {191},
publisher = {BioMed Central},
title = {{KNODWAT: A scientific framework application for testing knowledge discovery methods for the biomedical domain}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-191},
volume = {14},
year = {2013}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95% confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P. and Houwing-Duistermaat, Jeanine J. and {Eline Slagboom}, P. and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\n\nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\n\nAVAILABILITY: http://maq.sourceforge.net.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Wu2017a,
abstract = {Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/Descargas/wu2016.pdf:pdf},
isbn = {0018-9294 VO  - 64},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {263--273},
pmid = {28113246},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
url = {http://ieeexplore.ieee.org/document/7587347/},
volume = {64},
year = {2017}
}
@article{Triplet2014,
abstract = {To facilitate the integration and querying of genomics data, a number of generic data warehousing frameworks have been developed. They differ in their design and capabilities, as well as their intended audience. We provide a comprehensive and quantitative review of those genomic data warehousing frameworks in the context of large-scale systems biology. We reviewed in detail four genomic data warehouses (BioMart, BioXRT, InterMine and PathwayTools) freely available to the academic community. We quantified 20 aspects of the warehouses, covering the accuracy of their responses, their computational requirements and development efforts. Performance of the warehouses was evaluated under various hardware configurations to help laboratories optimize hardware expenses. Each aspect of the benchmark may be dynamically weighted by scientists using our online tool BenchDW (http://warehousebenchmark.fungalgenomics.ca/benchmark/) to build custom warehouse profiles and tailor our results to their specific needs.},
author = {Triplet, T. and Butler, G.},
doi = {10.1093/bib/bbt031},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {jul},
number = {4},
pages = {471--483},
pmid = {23673292},
title = {{A review of genomic data warehousing systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23673292 https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbt031},
volume = {15},
year = {2014}
}
@misc{Littlefield,
author = {Littlefield, Rayan},
title = {{An introduction into Data Mining in Bioinformatics.}},
url = {https://littlefield.co/an-introduction-into-data-mining-in-bioinformatics-964511e9ea21},
urldate = {2017-11-19}
}
@article{ORawe2013,
abstract = {BACKGROUND: To facilitate the clinical implementation of genomic medicine by next-generation sequencing, it will be critically important to obtain accurate and consistent variant calls on personal genomes. Multiple software tools for variant calling are available, but it is unclear how comparable these tools are or what their relative merits in real-world scenarios might be.\n\nMETHODS: We sequenced 15 exomes from four families using commercial kits (Illumina HiSeq 2000 platform and Agilent SureSelect version 2 capture kit), with approximately 120X mean coverage. We analyzed the raw data using near-default parameters with five different alignment and variant-calling pipelines (SOAP, BWA-GATK, BWA-SNVer, GNUMAP, and BWA-SAMtools). We additionally sequenced a single whole genome using the sequencing and analysis pipeline from Complete Genomics (CG), with 95% of the exome region being covered by 20 or more reads per base. Finally, we validated 919 single-nucleotide variations (SNVs) and 841 insertions and deletions (indels), including similar fractions of GATK-only, SOAP-only, and shared calls, on the MiSeq platform by amplicon sequencing with approximately 5000X mean coverage.\n\nRESULTS: SNV concordance between five Illumina pipelines across all 15 exomes was 57.4%, while 0.5 to 5.1% of variants were called as unique to each pipeline. Indel concordance was only 26.8% between three indel-calling pipelines, even after left-normalizing and intervalizing genomic coordinates by 20 base pairs. There were 11% of CG variants falling within targeted regions in exome sequencing that were not called by any of the Illumina-based exome analysis pipelines. Based on targeted amplicon sequencing on the MiSeq platform, 97.1%, 60.2%, and 99.1% of the GATK-only, SOAP-only and shared SNVs could be validated, but only 54.0%, 44.6%, and 78.1% of the GATK-only, SOAP-only and shared indels could be validated. Additionally, our analysis of two families (one with four individuals and the other with seven), demonstrated additional accuracy gained in variant discovery by having access to genetic data from a multi-generational family.\n\nCONCLUSIONS: Our results suggest that more caution should be exercised in genomic medicine settings when analyzing individual genomes, including interpreting positive and negative findings with scrutiny, especially for indels. We advocate for renewed collection and sequencing of multi-generational families to increase the overall accuracy of whole genomes.},
author = {O'Rawe, Jason and Jiang, Tao and Sun, Guangqing and Wu, Yiyang and Wang, Wei and Hu, Jingchu and Bodily, Paul and Tian, Lifeng and Hakonarson, Hakon and Johnson, W Evan and Wei, Zhi and Wang, Kai and Lyon, Gholson J},
doi = {10.1186/gm432},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Rawe et al. - 2013 - Low concordance of multiple variant-calling pipelines practical implications for exome and genome sequencing.pdf:pdf},
isbn = {1756-994X (Print)},
issn = {1756-994X},
journal = {Genome medicine},
number = {3},
pages = {28},
pmid = {23537139},
title = {{Low concordance of multiple variant-calling pipelines: practical implications for exome and genome sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3706896&tool=pmcentrez&rendertype=abstract},
volume = {5},
year = {2013}
}
@book{Quinlan2014,
abstract = {Technological advances have enabled the use of DNA sequencing as a flexible tool to characterize genetic variation and to measure the activity of diverse cellular phenomena such as gene isoform expression and transcription factor binding. Extracting biological insight from the experiments enabled by these advances demands the analysis of large, multi-dimensional datasets. This unit describes the use of the BEDTools toolkit for the exploration of high-throughput genomics datasets. Several protocols are presented for common genomic analyses, demonstrating how simple BEDTools operations may be combined to create bespoke pipelines addressing complex questions. Curr. Protoc. Bioinform. 47:11.12.1-11.12.34. {\textcopyright} 2014 by John Wiley & Sons, Inc.},
author = {Quinlan, Aaron R.},
booktitle = {Current Protocols in Bioinformatics},
doi = {10.1002/0471250953.bi1112s47},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan - 2014 - BEDTools The Swiss-Army tool for genome feature analysis.pdf:pdf},
isbn = {0471250953},
issn = {1934340X},
keywords = {Bioinformatics,Genome analysis,Genome features,Genome intervals,Genomics},
pages = {11.12.1--11.12.34},
pmid = {25199790},
title = {{BEDTools: The Swiss-Army tool for genome feature analysis}},
volume = {2014},
year = {2014}
}
@article{Li2012,
abstract = {Family samples, which can be enriched for rare causal variants by focusing on families with multiple extreme individuals and which facilitate detection of de novo mutation events, provide an attractive resource for next-generation sequencing studies. Here, we describe, implement, and evaluate a likelihood-based framework for analysis of next generation sequence data in family samples. Our framework is able to identify variant sites accurately and to assign individual genotypes, and can handle de novo mutation events, increasing the sensitivity and specificity of variant calling and de novo mutation detection. Through simulations we show explicit modeling of family relationships is especially useful for analyses of low-frequency variants and that genotype accuracy increases with the number of individuals sequenced per family. Compared with the standard approach of ignoring relatedness, our methods identify and accurately genotype more variants, and have high specificity for detecting de novo mutation events. The improvement in accuracy using our methods over the standard approach is particularly pronounced for low-frequency variants. Furthermore the family-aware calling framework dramatically reduces Mendelian inconsistencies and is beneficial for family-based analysis. We hope our framework and software will facilitate continuing efforts to identify genetic factors underlying human diseases.},
author = {Li, Bingshan and Chen, Wei and Zhan, Xiaowei and Busonero, Fabio and Sanna, Serena and Sidore, Carlo and Cucca, Francesco and Kang, Hyun M. and Abecasis, Gon??alo R.},
doi = {10.1371/journal.pgen.1002944},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families.pdf:pdf},
isbn = {1553-7404 (Electronic)\n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {10},
pmid = {23055937},
title = {{A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families}},
volume = {8},
year = {2012}
}
@article{Li2015,
abstract = {Background: Conditions associated with sudden cardiac arrest/death (SCA/D) in youth often have a genetic etiology. While SCA/D is uncommon, a pro-active family screening approach may identify these inherited structural and electrical abnormalities prior to symptomatic events and allow appropriate surveillance and treatment. This study investigated the diagnostic utility of exome sequencing (ES) by evaluating the capture and coverage of genes related to SCA/D. Methods: Samples from 102 individuals (13 with known molecular etiologies for SCA/D, 30 individuals without known molecular etiologies for SCA/D and 59 with other conditions) were analyzed following exome capture and sequencing at an average read depth of 100X. Reads were mapped to human genome GRCh37 using Novoalign, and post-processing and analysis was done using Picard and GATK. A total of 103 genes (2,190 exons) related to SCA/D were used as a primary filter. An additional 100 random variants within the targeted genes associated with SCA/D were also selected and evaluated for depth of sequencing and coverage. Although the primary objective was to evaluate the adequacy of depth of sequencing and coverage of targeted SCA/D genes and not for primary diagnosis, all patients who had SCA/D (known or unknown molecular etiologies) were evaluated with the project's variant analysis pipeline to determine if the molecular etiologies could be successfully identified. Results: The majority of exons (97.6 %) were captured and fully covered on average at minimum of 20x sequencing depth. The proportion of unique genomic positions reported within poorly covered exons remained small (4 %). Exonic regions with less coverage reflect the need to enrich these areas to improve coverage. Despite limitations in coverage, we identified 100 % of cases with a prior known molecular etiology for SCA/D, and analysis of an additional 30 individuals with SCA/D but no known molecular etiology revealed a diagnostic answer in 5/30 (17 %). We also demonstrated 95 % of 100 randomly selected reported variants within our targeted genes would have been picked up on ES based on our coverage analysis. Conclusions: ES is a helpful clinical diagnostic tool for SCA/D given its potential to successfully identify a molecular diagnosis, but clinicians should be aware of limitations of available platforms from technical and diagnostic perspectives.},
author = {Li, Mindy H. and Abrudan, Jenica L. and Dulik, Matthew C. and Sasson, Ariella and Brunton, Joshua and Jayaraman, Vijayakumar and Dugan, Noreen and Haley, Danielle and Rajagopalan, Ramakrishnan and Biswas, Sawona and Sarmady, Mahdi and DeChene, Elizabeth T. and Deardorff, Matthew A. and Wilkens, Alisha and Noon, Sarah E. and Scarano, Maria I. and Santani, Avni B. and White, Peter S. and Pennington, Jeffrey and Conlin, Laura K. and Spinner, Nancy B. and Krantz, Ian D. and Vetter, Victoria L.},
doi = {10.1186/s40246-015-0038-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sud.pdf:pdf},
issn = {1479-7364},
journal = {Human Genomics},
number = {1},
pages = {15},
pmid = {26187847},
publisher = {Human Genomics},
title = {{Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sudden cardiac arrest/sudden cardiac death}},
url = {http://www.humgenomics.com/content/9/1/15/abstract%5Cnhttp://www.humgenomics.com/content/9/1/15%5Cnhttp://www.humgenomics.com/content/pdf/s40246-015-0038-y.pdf},
volume = {9},
year = {2015}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Laboratories2015,
author = {Laboratories, Knight Diagnostic and Genetics, Medical and Health, Oregon and Road, Plank and Molecular, Clinical and Children, Nationwide and State, Ohio},
doi = {10.1038/gim.2015.30.Standards},
file = {:home/jennifer/Descargas/nihms697486.pdf:pdf},
journal = {Medical Genetics},
number = {5},
pages = {405--424},
title = {{Standards and Guidelines for the Interpretation of Sequence Variants: A Joint Consensus Recommendation of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology}},
volume = {17},
year = {2015}
}
@article{Hubbard2002,
abstract = {The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.},
author = {Hubbard, T and Barker, D and Birney, E and Cameron, G and Chen, Y and Clark, L and Cox, T and Cuff, J and Curwen, V and Down, T and Durbin, R and Eyras, E and Gilbert, J and Hammond, M and Huminiecki, L and Kasprzyk, A and Lehvaslaiho, H and Lijnzaad, P and Melsopp, C and Mongin, E and Pettett, R and Pocock, M and Potter, S and Rust, A and Schmidt, E and Searle, S and Slater, G and Smith, J and Spooner, W and Stabenau, A and Stalker, J and Stupka, E and Ureta-Vidal, A and Vastrik, I and Clamp, M},
issn = {1362-4962},
journal = {Nucleic acids research},
month = {jan},
number = {1},
pages = {38--41},
pmid = {11752248},
publisher = {Oxford University Press},
title = {{The Ensembl genome database project.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11752248 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC99161},
volume = {30},
year = {2002}
}
@article{Fu2013,
abstract = {Establishing the age of each mutation segregating in contemporary human populations is important to fully understand our evolutionary history and will help to facilitate the development of new approaches for disease-gene discovery. Large-scale surveys of human genetic variation have reported signatures of recent explosive population growth, notable for an excess of rare genetic variants, suggesting that many mutations arose recently. To more quantitatively assess the distribution of mutation ages, we resequenced 15,336 genes in 6,515 individuals of European American and African American ancestry and inferred the age of 1,146,401 autosomal single nucleotide variants (SNVs). We estimate that approximately 73% of all protein-coding SNVs and approximately 86% of SNVs predicted to be deleterious arose in the past 5,000-10,000 years. The average age of deleterious SNVs varied significantly across molecular pathways, and disease genes contained a significantly higher proportion of recently arisen deleterious SNVs than other genes. Furthermore, European Americans had an excess of deleterious variants in essential and Mendelian disease genes compared to African Americans, consistent with weaker purifying selection due to the Out-of-Africa dispersal. Our results better delimit the historical details of human protein-coding variation, show the profound effect of recent human history on the burden of deleterious SNVs segregating in contemporary populations, and provide important practical information that can be used to prioritize variants in disease-gene discovery.},
author = {Fu, Wenqing and O'Connor, Timothy D. and Jun, Goo and Kang, Hyun Min and Abecasis, Goncalo and Leal, Suzanne M. and Gabriel, Stacey and Altshuler, David and Shendure, Jay and Nickerson, Deborah A. and Bamshad, Michael J. and Akey, Joshua M.},
doi = {10.1038/nature11690},
file = {:home/jennifer/Descargas/fu2012.pdf:pdf},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {7431},
pages = {216--220},
pmid = {23201682},
title = {{Analysis of 6,515 exomes reveals the recent origin of most human protein-coding variants}},
volume = {493},
year = {2013}
}
@book{Klug2013,
abstract = {10a. ed. Acompaña código de acceso de 12 meses a los recursos online. Incluye Pearson eText. Índice analítico. Glosario: p. [892]-917.},
author = {Klug, William S. and {Pascual Calaforra}, Luis F.},
isbn = {9788415552482},
publisher = {Pearson Educacin{\'{o}}n},
title = {{Conceptos de genn{\'{e}}tica}},
url = {https://www.casadellibro.com/libro-conceptos-de-genetica/9788415552482/2547204},
year = {2013}
}
@article{Cock2009,
abstract = {FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.},
author = {Cock, Peter J A and Fields, Christopher J. and Goto, Naohisa and Heuer, Michael L. and Rice, Peter M.},
doi = {10.1093/nar/gkp1137},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cock et al. - 2009 - The Sanger FASTQ file format for sequences with quality scores, and the SolexaIllumina FASTQ variants.pdf:pdf},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {6},
pages = {1767--1771},
pmid = {20015970},
title = {{The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants}},
volume = {38},
year = {2009}
}
@article{Li2009,
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
title = {{Fast and accurate short read alignment with Burrows–Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Rehm2013,
abstract = {Next-generation sequencing technologies have been and continue to be deployed in clinical laboratories, enabling rapid transformations in genomic medicine. These technologies have reduced the cost of large-scale sequencing by several orders of magnitude, and continuous advances are being made. It is now feasible to analyze an individual's near-complete exome or genome to assist in the diagnosis of a wide array of clinical scenarios. Next-generation sequencing technologies are also facilitating further advances in therapeutic decision making and disease prediction for at-risk patients. However, with rapid advances come additional challenges involving the clinical validation and use of these constantly evolving technologies and platforms in clinical laboratories. To assist clinical laboratories with the validation of next-generation sequencing methods and platforms, the ongoing monitoring of next-generation sequencing testing to ensure quality results, and the interpretation and reporting of variants found using these technologies, the American College of Medical Genetics and Genomics has developed the following professional standards and guidelines.},
author = {Rehm, Heidi L and Bale, Sherri J and Bayrak-Toydemir, Pinar and Berg, Jonathan S and Brown, Kerry K and Deignan, Joshua L and Friez, Michael J and Funke, Birgit H and Hegde, Madhuri R and Lyon, Elaine},
doi = {10.1038/gim.2013.92},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehm et al. - 2013 - ACMG clinical laboratory standards for next-generation sequencing.pdf:pdf},
isbn = {1098-3600},
issn = {1530-0366},
journal = {Genetics in medicine : official journal of the American College of Medical Genetics},
keywords = {DNA,DNA: instrumentation,DNA: methods,DNA: standards,Exome,Genetic Testing,Genetic Testing: standards,Genome,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: standards,Human,Humans,Laboratories,Laboratories: standards,Reproducibility of Results,Sequence Analysis,Translational Medical Research,United States},
number = {9},
pages = {733--47},
pmid = {23887774},
title = {{ACMG clinical laboratory standards for next-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4098820&tool=pmcentrez&rendertype=abstract},
volume = {15},
year = {2013}
}
@article{Cleary2014,
author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M.},
doi = {10.1089/cmb.2014.0029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleary et al. - 2014 - Joint Variant and iDe Novoi Mutation Identification on Pedigrees from High-Throughput Sequencing Data.pdf:pdf},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and <i>De Novo</i> Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@misc{FBI,
author = {FBI},
pages = {https://www.fbi.gov/services/laboratory/biometric--},
title = {{Combined DNA Index System (CODIS)}}
}
@article{Morimoto2016,
abstract = {We developed a new approach for pairwise kinship analysis in forensic genetics based on chromosomal sharing between two individuals. Here, we defined "index of chromosome sharing" (ICS) calculated using 174,254 single nucleotide polymorphism (SNP) loci typed by SNP microarray and genetic length of the shared segments from the genotypes of two individuals. To investigate the expected ICS distributions from first- to fifth-degree relatives and unrelated pairs, we used computationally generated genotypes to consider the effect of linkage disequilibrium and recombination. The distributions were used for probabilistic evaluation of the pairwise kinship analysis, such as likelihood ratio (LR) or posterior probability, without allele frequencies and haplotype frequencies. Using our method, all actual sample pairs from volunteers showed significantly high LR values (i.e., ≥ 108); therefore, we can distinguish distant relationships (up to the fifth-degree) from unrelated pairs based on LR. Moreover, we can determine accurate degrees of kinship in up to third-degree relationships with a probability of > 80% using the criterion of posterior probability ≥ 0.90, even if the kinship of the pair is totally unpredictable. This approach greatly improves pairwise kinship analysis of distant relationships, specifically in cases involving identification of disaster victims or missing persons.},
author = {Morimoto, Chie and Manabe, Sho and Kawaguchi, Takahisa and Kawai, Chihiro and Fujimoto, Shuntaro and Hamano, Yuya and Yamada, Ryo and Matsuda, Fumihiko and Tamaki, Keiji},
doi = {10.1371/journal.pone.0160287},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morimoto et al. - 2016 - Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
number = {7},
pages = {e0160287},
pmid = {27472558},
title = {{Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.}},
url = {http://dx.plos.org/10.1371/journal.pone.0160287%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27472558%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4966930},
volume = {11},
year = {2016}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for molecular biology : AMB},
pages = {4},
pmid = {17428327},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
volume = {2},
year = {2007}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Cook2016,
abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI's vast public data resources.},
author = {Cook, Charles E. and Bergman, Mary Todd and Finn, Robert D. and Cochrane, Guy and Birney, Ewan and Apweiler, Rolf},
doi = {10.1093/nar/gkv1352},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook et al. - 2016 - The European Bioinformatics Institute in 2016 Data growth and integration.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D20--D26},
pmid = {26673705},
title = {{The European Bioinformatics Institute in 2016: Data growth and integration}},
volume = {44},
year = {2016}
}
@article{Shendure2016,
author = {Shendure, Jay},
doi = {10.1038/536277a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shendure - 2016 - Human genomics A deep dive into genetic variation.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Genetics,Genomics},
month = {aug},
number = {7616},
pages = {277--278},
publisher = {Nature Publishing Group},
title = {{Human genomics: A deep dive into genetic variation}},
url = {http://www.nature.com/doifinder/10.1038/536277a},
volume = {536},
year = {2016}
}
@article{Hannah-Shmouni2015,
author = {Hannah-Shmouni, Fady and Seidelmann, Sara B. and Sirrs, Sandra and Mani, Arya and Jacoby, Daniel},
doi = {10.1016/j.cjca.2015.07.735},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannah-Shmouni et al. - 2015 - The Genetic Challenges and Opportunities in Advanced Heart Failure.pdf:pdf},
issn = {0828282X},
journal = {Canadian Journal of Cardiology},
number = {11},
pages = {1338--1350},
publisher = {Elsevier Ltd},
title = {{The Genetic Challenges and Opportunities in Advanced Heart Failure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0828282X15013161},
volume = {31},
year = {2015}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Searls2010,
author = {Searls, David B.},
doi = {10.1371/journal.pcbi.1000809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Searls - 2010 - The Roots of Bioinformatics.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {jun},
number = {6},
pages = {e1000809},
publisher = {Public Library of Science},
title = {{The Roots of Bioinformatics}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1000809},
volume = {6},
year = {2010}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics{_}pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M and Loguercio, Salvatore and Su, Andrew I},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@inproceedings{Hu2011,
author = {Hu, Xiaohua},
booktitle = {2011 IEEE International Conference on Granular Computing},
doi = {10.1109/GRC.2011.6122559},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu - 2011 - Data mining and its applications in bioinformatics Techniques and methods.pdf:pdf},
isbn = {978-1-4577-0371-3},
month = {nov},
pages = {3--3},
publisher = {IEEE},
title = {{Data mining and its applications in bioinformatics: Techniques and methods}},
url = {http://ieeexplore.ieee.org/document/6122559/},
year = {2011}
}
@article{Jurca2016,
abstract = {BACKGROUND Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. RESULTS We utilized PubMed for the testing. We investigated gene-gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. CONCLUSIONS Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene-gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
isbn = {1756-0500},
issn = {1756-0500},
journal = {BMC Research Notes},
keywords = {Breast cancer,Data mining,Network analysis,Text mining,breast cancer,data mining,network analysis,text mining},
number = {1},
pages = {236},
pmid = {27112211},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@incollection{Kutzera2017a,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
year = {2017}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.\n\nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3% (87.0%) and average genotype concordance in correctly identified SNVs was 99.2% (99.2%) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.\n\nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Harold2016,
abstract = {Motivation: Integrating heterogeneous datasets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access, data filtering, format conversions, identifier mapping, among further diverse operations. Data integration is especially important when annotating next generation sequencing data, where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic locations, such a single nucleotide variants or genes. Each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose. However, software that always produces all available data is difficult to maintain and quickly leads to an excess of data, creating an information overload rather than the desired goal-oriented and integrated result. Results: We present SoFIA, a framework for workflow-driven data integration with a focus on genomic annotation. SoFIA conceptualizes workflow templates as comprehensive workflows that cover as many data integration operations as possible in a given domain. However, these templates are not intended to be executed as a whole; instead, when given an integration task consisting of a set of input data and a set of desired output data, SoFIA derives a minimal workflow that completes the task. These workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work. Using a comprehensive genome annotation template, we highlight the flexibility, extensibility and power of the framework using real-life case studies},
author = {Harold, Liam and Mamlouk, Soulafa and Brandt, J{\"{o}}rgen and Sers, Christine and Lesser, Ulf},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harold et al. - 2016 - SoFIA a data integration framework for annotating high-throughput datasets.pdf:pdf},
journal = {Bioinformatics (Oxford, England)},
number = {17},
pages = {2590--2597},
title = {{SoFIA a data integration framework for annotating high-throughput datasets}},
volume = {32},
year = {2016}
}
@article{Terlizzi2017,
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and Miriam, Antonella and Lullo, Di and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Terlizzi et al. - 2017 - Genotype – phenotype correlation and functional studies in patients with cystic fi brosis bearing CFTR comple.pdf:pdf},
journal = {Journal of Medical Genetics},
pages = {224--235},
title = {{Genotype – phenotype correlation and functional studies in patients with cystic fi brosis bearing CFTR complex alleles}},
volume = {54},
year = {2017}
}
@article{Xu2016,
author = {Xu, Zhiwei and Chi, Xuebin and Xiao, Nong},
doi = {10.1093/nsr/nww001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Chi, Xiao - 2016 - High-Performance Computing Environment A Review of Twenty Years of Experiments in China.pdf:pdf},
issn = {2053714X},
keywords = {22-dec-2015,24-sep-2015,25-dec-2015,accepted,cyberinfrastructure,e-science environment,middleware,received,revised,supercomputing},
pages = {1--24},
title = {{High-Performance Computing Environment : A Review of Twenty Years of Experiments in China}},
year = {2016}
}
@article{Feero2017,
author = {Feero, W. Gregory},
doi = {10.1001/jama.2016.20625},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feero - 2017 - Introducing “Genomics and Precision Health”(2).pdf:pdf},
issn = {0098-7484},
journal = {JAMA},
keywords = {genomics,precision medicine},
month = {may},
number = {18},
pages = {1842},
publisher = {American Medical Association},
title = {{Introducing “Genomics and Precision Health”}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.20625},
volume = {317},
year = {2017}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Palmisano2016a,
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming-Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmisano et al. - 2016 - OpenGeneMed a portable, flexible and customizable informatics hub for the coordination of next-generation sequ.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {jul},
number = {5},
pages = {bbw059},
publisher = {Oxford University Press},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
volume = {18},
year = {2016}
}
@article{Li2014,
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {2210-3244},
journal = {Genomics, proteomics & bioinformatics},
month = {oct},
number = {5},
pages = {187--9},
pmid = {25462151},
publisher = {Elsevier},
title = {{Big biological data: challenges and opportunities.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25462151 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4411415},
volume = {12},
year = {2014}
}
@article{Wang2014,
abstract = {Motivation: The transition/transversion (Ti/Tv) ratio and heterozygous/nonreference-homozygous (het/nonref-hom) ratio have been commonly computed in genetic studies as a quality control (QC) measurement. Additionally, these two ratios are helpful in our understanding of the patterns of DNA sequence evolution.Results: To thoroughly understand these two genomic measures, we performed a study using 1000 Genomes Project (1000G) released genotype data (N = 1092). An additional two datasets (N = 581 and N = 6) were used to validate our findings from the 1000G dataset. We compared the two ratios among continental ancestry, genome regions and gene functionality. We found that the Ti/Tv ratio can be used as a quality indicator for single nucleotide polymorphisms inferred from high-throughput sequencing data. The Ti/Tv ratio varies greatly by genome region and functionality, but not by ancestry. The het/nonref-hom ratio varies greatly by ancestry, but not by genome regions and functionality. Furthermore, extreme guanine + cytosine content (either high or low) is negatively associated with the Ti/Tv ratio magnitude. Thus, when performing QC assessment using these two measures, care must be taken to apply the correct thresholds based on ancestry and genome region. Failure to take these considerations into account at the QC stage will bias any following analysis.Contact: yan.guo@vanderbilt.eduSupplementary information: Supplementary data are available at Bioinformatics online. },
author = {Wang, Jing and Raskin, Leon and Samuels, David C. and Shyr, Yu and Guo, Yan},
doi = {10.1093/bioinformatics/btu668},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Genome measures used for quality control are dependent on gene function and ancestry.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {3},
pages = {318--323},
pmid = {25297068},
title = {{Genome measures used for quality control are dependent on gene function and ancestry}},
volume = {31},
year = {2014}
}
@article{Li2014a,
abstract = {MOTIVATION: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods.\n\nRESULTS: We made 10 single nucleotide polymorphism and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10-15 kb, but the error rate of post-filtered calls is reduced to 1 in 100-200 kb without significant compromise on the sensitivity. Availability and implementation: BWA-MEM alignment and raw variant calls are available at http://bit.ly/1g8XqRt scripts and miscellaneous data at https://github.com/lh3/varcmp.\n\nCONTACT: hengli@broadinstitute.org Supplementary information: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.0929v1},
author = {Li, Heng},
doi = {10.1093/bioinformatics/btu356},
eprint = {arXiv:1404.0929v1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2014 - Toward better understanding of artifacts in variant calling from high-coverage samples.pdf:pdf},
isbn = {1367-4803},
issn = {13674811},
journal = {Bioinformatics (Oxford, England)},
number = {20},
pages = {2843--2851},
pmid = {24974202},
title = {{Toward better understanding of artifacts in variant calling from high-coverage samples}},
volume = {30},
year = {2014}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Canuel2015,
abstract = {The rise of personalized medicine and the availability of high-throughput molecular analyses in the context of clinical care have increased the need for adequate tools for translational researchers to manage and explore these data. We reviewed the biomedical literature for translational platforms allowing the management and exploration of clinical and omics data, and identified seven platforms: BRISK, caTRIP, cBio Cancer Portal, G-DOC, iCOD, iDASH and tranSMART. We analyzed these platforms along seven major axes. (1) The community axis regrouped information regarding initiators and funders of the project, as well as availability status and references. (2) We regrouped under the information content axis the nature of the clinical and omics data handled by each system. (3) The privacy management environment axis encompassed functionalities allowing control over data privacy. (4) In the analysis support axis, we detailed the analytical and statistical tools provided by the platforms. We also explored (5) interoperability support and (6) system requirements. The final axis (7) platform support listed the availability of documentation and installation procedures. A large heterogeneity was observed in regard to the capability to manage phenotype information in addition to omics data, their security and interoperability features. The analytical and visualization features strongly depend on the considered platform. Similarly, the availability of the systems is variable. This review aims at providing the reader with the background to choose the platform best suited to their needs. To conclude, we discuss the desiderata for optimal translational research platforms, in terms of privacy, interoperability and technical features.},
author = {Canuel, Vincent and Rance, Bastien and Avillach, Paul and Degoulet, Patrice and Burgun, Anita},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data A review of publicly available solutions.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical research,Clinical data,High-throughput technologies,Information storage and retrieval,Translationalmedical research},
number = {2},
pages = {280--290},
pmid = {24608524},
title = {{Translational research platforms integrating clinical and omics data: A review of publicly available solutions}},
volume = {16},
year = {2015}
}
@article{Zaki2007,
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
issn = {1748-7188},
journal = {Algorithms for molecular biology : AMB},
month = {apr},
pages = {4},
pmid = {17428327},
publisher = {BioMed Central},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17428327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1852315},
volume = {2},
year = {2007}
}
@article{Holst-Hansen2017,
abstract = {Allele number, or zygosity, is a clear determinant of gene expression in diploid cells. However, the relationship between the number of copies of a gene and its expression can be hard to anticipate, especially when the gene in question is embedded in a regulatory circuit that contains feedback. Here, we study this question making use of the natural genetic variability of human populations, which allows us to compare the expression profiles of a receptor protein in natural killer cells among donors infected with human cytomegalovirus with one or two copies of the allele. Crucially, the distribution of gene expression in many of the donors is bimodal, which indicates the presence of a positive feedback loop somewhere in the regulatory environment of the gene. Three separate gene-circuit models differing in the location of the positive feedback loop with respect to the gene can all reproduce the homozygous data. However, when the resulting fitted models are applied to the hemizygous donors, one model (the one with the positive feedback located at the level of gene transcription) is superior in describing the experimentally observed gene-expression profile. In that way, our work shows that zygosity can help us relate the structure and function of gene regulatory networks.},
author = {Holst-Hansen, Thomas and Abad, Elena and Muntasell, Aura and L{\'{o}}pez-Botet, Miguel and Jensen, Mogens H. and Trusina, Ala and Garcia-Ojalvo, Jordi},
doi = {10.1016/j.bpj.2017.05.010},
file = {:home/jennifer/Descargas/holsthansen2017.pdf:pdf},
issn = {15420086},
journal = {Biophysical Journal},
number = {1},
pages = {148--156},
pmid = {28700913},
title = {{Impact of Zygosity on Bimodal Phenotype Distributions}},
volume = {113},
year = {2017}
}
@article{Conesa2016,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Micha{\l} Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
doi = {10.1186/s13059-016-0881-8},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conesa et al. - 2016 - A survey of best practices for RNA-seq data analysis.pdf:pdf},
isbn = {1474-760X (Electronic)\r1474-7596 (Linking)},
issn = {1474-760X},
journal = {Genome Biology},
keywords = {Animal Genetics and Genomics,Bioinformatics,Evolutionary Biology,Human Genetics,Microbial Genetics and Genomics,Plant Genetics & Genomics},
number = {1},
pages = {13},
pmid = {26813401},
title = {{A survey of best practices for RNA-seq data analysis}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0881-8},
volume = {17},
year = {2016}
}
@article{Liu2014,
abstract = {Discriminative pattern mining is one of the most important techniques in data mining. This challenging task is concerned with finding a set of patterns that occur with disproportionate frequency in data sets with various class labels. Such patterns are of great value for group difference detection and classifier construction. Research on finding interesting discriminative patterns in class-labeled data evolves rapidly and lots of algorithms have been proposed to specifically address this problem. Discriminative pattern mining techniques have proven their considerable value in biological data analysis. The archetypical applications in bioinformatics include phosphorylation motif discovery, differentially expressed gene identification, discriminative genotype pattern detection, etc. In this article, we present an overview of discriminative pattern mining and the corresponding effective methods, and subsequently we illustrate their applications to tackling the bioinformatics problems. In the end, we give a general discussion of potential challenges and future work for this task.},
author = {Liu, Xiaoqing and Wu, Jun and Gu, Feiyang and Wang, Jie and He, Zengyou},
doi = {10.1093/bib/bbu042},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Discriminative pattern mining and its applications in bioinformatics.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {contrast sets,discriminative pattern mining,emerging patterns,subgroup discovery},
number = {October},
pages = {1--17},
title = {{Discriminative pattern mining and its applications in bioinformatics}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu042},
year = {2014}
}
@article{Salter2017,
abstract = {The rise of bioinformatics is a direct response to the political difficulties faced by genomics in its quest to be a new biomedical innovation, and the value of bioinformatics lies in its role as the bridge between the promise of genomics and its realization in the form of health benefits. Western scientific elites are able to use their close relationship with the state to control and facilitate the emergence of new domains compatible with the existing distribution of epistemic power – all within the embrace of public trust. The incorporation of bioinformatics as the saviour of genomics had to be integrated with the operation of two key aspects of governance in this field: the definition and ownership of the new knowledge. This was achieved mainly by the development of common standards and by the promotion of the values of communality, open access and the public ownership of data to legitimize and maintain the governance power of publicly funded genomic science. Opposition from industry advocating the pri...},
author = {Salter, Brian and Salter, Charlotte},
doi = {10.1177/0306312716681210},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salter, Salter - 2017 - Controlling new knowledge Genomic science, governance and the politics of bioinformatics.pdf:pdf},
issn = {0306-3127},
journal = {Social Studies of Science},
keywords = {bioinformatics,genomics,governance,ideology,politics},
month = {apr},
number = {2},
pages = {263--287},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Controlling new knowledge: Genomic science, governance and the politics of bioinformatics}},
url = {http://journals.sagepub.com/doi/10.1177/0306312716681210},
volume = {47},
year = {2017}
}
@article{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
doi = {10.1007/s13398-014-0173-7.2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cossio et al. - 2012 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Uma {\'{e}}tica para quantos?},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics & numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics & numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,epidural,esth{\'{e}}tique,est{\'{e}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,needle through a,nes corporales,perforaci{\'{o}}n corporal,piel,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{No Title No Title}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233&partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@misc{Xuan2013,
abstract = {The advent of next generation sequencing (NGS) technologies has revolutionized the field of genomics, enabling fast and cost-effective generation of genome-scale sequence data with exquisite resolution and accuracy. Over the past years, rapid technological advances led by academic institutions and companies have continued to broaden NGS applications from research to the clinic. A recent crop of discoveries have highlighted the medical impact of NGS technologies on Mendelian and complex diseases, particularly cancer. However, the ever-increasing pace of NGS adoption presents enormous challenges in terms of data processing, storage, management and interpretation as well as sequencing quality control, which hinder the translation from sequence data into clinical practice. In this review, we first summarize the technical characteristics and performance of current NGS platforms. We further highlight advances in the applications of NGS technologies towards the development of clinical diagnostics and therapeutics. Common issues in NGS workflows are also discussed to guide the selection of NGS platforms and pipelines for specific research purposes. ?? 2012.},
author = {Xuan, Jiekun and Yu, Ying and Qing, Tao and Guo, Lei and Shi, Leming},
booktitle = {Cancer Letters},
doi = {10.1016/j.canlet.2012.11.025},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xuan et al. - 2013 - Next-generation sequencing in the clinic Promises and challenges.pdf:pdf},
isbn = {1872-7980 (Electronic)\r0304-3835 (Linking)},
issn = {03043835},
keywords = {Bioinformatics,Clinical applications,Exome sequencing,FFPE,RNA-Seq,Tumor heterogeneity,Whole-genome sequencing},
number = {2},
pages = {284--295},
pmid = {23174106},
title = {{Next-generation sequencing in the clinic: Promises and challenges}},
volume = {340},
year = {2013}
}
@article{Weitschek2014,
author = {Weitschek, Emanuel and Fiscon, Giulia and Felici, Giovanni},
doi = {10.1186/1756-0381-7-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitschek, Fiscon, Felici - 2014 - Supervised DNA Barcodes species classification analysis, comparisons and results.pdf:pdf},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {DNA Barcoding,Species identification,Supervised classification methods},
number = {1},
pages = {4},
publisher = {BioData Mining},
title = {{Supervised DNA Barcodes species classification: analysis, comparisons and results}},
url = {http://www.biodatamining.org/content/7/1/4},
volume = {7},
year = {2014}
}
@article{Seren2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4788},
author = {Seren, {\"{U}}mit and Grimm, Dominik and Fitz, Joffrey and Weigel, Detlef and Nordborg, Magnus and Borgwardt, Karsten and Korte, Arthur},
doi = {10.1093/nar/gkw986},
eprint = {arXiv:1212.4788},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seren et al. - 2016 - AraPheno a public database for Arabidopsis thaliana phenotypes.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {October 2016},
pages = {gkw986},
pmid = {27924043},
title = {{AraPheno: a public database for Arabidopsis thaliana phenotypes}},
url = {http://nar.oxfordjournals.org/lookup/doi/10.1093/nar/gkw986},
volume = {45},
year = {2016}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@misc{CoriellInstitute,
author = {{Coriell Institute}},
title = {1000 genomes project},
url = {https://catalog.coriell.org/0/Sections/Collections/NHGRI/1000Clm.aspx?PgId=675&coll=HG}
}
@article{Hwang2015,
abstract = {The success of clinical genomics using next generation sequencing (NGS) requires the accurate and consistent identification of personal genome variants. Assorted variant calling methods have been developed, which show low concordance between their calls. Hence, a systematic comparison of the variant callers could give important guidance to NGS-based clinical genomics. Recently, a set of high-confident variant calls for one individual (NA12878) has been published by the Genome in a Bottle (GIAB) consortium, enabling performance benchmarking of different variant calling pipelines. Based on the gold standard reference variant calls from GIAB, we compared the performance of thirteen variant calling pipelines, testing combinations of three read aligners-BWA-MEM, Bowtie2, and Novoalign-and four variant callers-Genome Analysis Tool Kit HaplotypeCaller (GATK-HC), Samtools mpileup, Freebayes and Ion Proton Variant Caller (TVC), for twelve data sets for the NA12878 genome sequenced by different platforms including Illumina2000, Illumina2500, and Ion Proton, with various exome capture systems and exome coverage. We observed different biases toward specific types of SNP genotyping errors by the different variant callers. The results of our study provide useful guidelines for reliable variant identification from deep sequencing of personal genomes.},
author = {Hwang, Sohyun and Kim, Eiru and Lee, Insuk and Marcotte, Edward M.},
doi = {10.1038/srep17875},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang et al. - 2015 - Systematic comparison of variant calling pipelines using gold standard personal exome variants.pdf:pdf},
isbn = {8010628980},
issn = {2045-2322},
journal = {Scientific Reports},
number = {December},
pages = {17875},
pmid = {26639839},
publisher = {Nature Publishing Group},
title = {{Systematic comparison of variant calling pipelines using gold standard personal exome variants}},
url = {http://www.nature.com/articles/srep17875%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26639839%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4671096},
volume = {5},
year = {2015}
}
@article{Oellrich2014,
author = {Oellrich, Anika and Jacobsen, Julius and Papatheodorou, Irene and Smedley, Damian},
doi = {10.1093/bioinformatics/btu260},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oellrich et al. - 2014 - Using association rule mining to determine promising secondary phenotyping hypotheses.pdf:pdf},
issn = {1460-2059},
journal = {Bioinformatics},
month = {jun},
number = {12},
pages = {i52--i59},
publisher = {Oxford University Press},
title = {{Using association rule mining to determine promising secondary phenotyping hypotheses}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu260},
volume = {30},
year = {2014}
}
@article{FigueroaFranco2014,
abstract = {La investigaci{\'{o}}n, recuperaci{\'{o}}n e identificaci{\'{o}}n de cuerpos relacionados con la desaparici{\'{o}}n de personas dentro del marco del conflicto armado en Colombia se ha llevado a cabo por parte del Estado: Fiscal{\'{i}}a General de la Naci{\'{o}}n (FGN), Polic{\'{i}}a Nacional, Departamento Administrativo de Seguridad (DAS) e Instituto Nacional de Medicina Legal y Ciencias Forenses (INMLCF). El an{\'{a}}lisis de ADN corresponde a una etapa del proceso de identificaci{\'{o}}n cuando otros abordajes –como el dactilosc{\'{o}}pico, odontol{\'{o}}gico y/o antropol{\'{o}}gico– no han permitido la identificaci{\'{o}}n fehaciente. Este an{\'{a}}lisis es de uso sistem{\'{a}}tico en laboratorios forenses a nivel mundial para comparar la informaci{\'{o}}n gen{\'{e}}tica de cad{\'{a}}veres en condici{\'{o}}n de no identificados con los posibles familiares de personas reportadas como desaparecidas. El presente estudio revis{\'{o}} la documentaci{\'{o}}n asociada a 154 solicitudes de identificaci{\'{o}}n recibidas durante 2009, analizadas por los peritos del Grupo de Gen{\'{e}}tica del INMLCF, sede Bogot{\'{a}}, D. C., logrando contribuir a la identificaci{\'{o}}n positiva de 95 cuerpos que inicialmente se encontraban en condici{\'{o}}n de no identificados (CNI). La mayor{\'{i}}a de muestras del estudio correspondieron a cuerpos exhumados de la regi{\'{o}}n Caribe colombiana. En el 80% de las solicitudes se realiz{\'{o}} cotejo gen{\'{e}}tico entre perfiles gen{\'{e}}ticos de familiares y de restos humanos; en el 20% los perfiles gen{\'{e}}ticos se almacenaron en la “Base Nacional de Perfiles Gen{\'{e}}ticos de Aplicaci{\'{o}}n Judicial”, conocida como CODIS por sus siglas en ingl{\'{e}}s (Combined DNA Index System). De las muestras {\'{o}}seas analizadas, el f{\'{e}}mur y la tibia arrojaron mejores resultados. El 77% de los cotejos fueron no exclusiones, el 9% resultados negativos o no concluyentes y el 14% exclusiones.},
author = {{Figueroa Franco}, Ruth Marl{\'{e}}n and {Romero M{\'{a}}rtinez}, Rosa Elena and {Terreros Ib{\'{a}}{\~{n}}ez}, Grace Alejandra and {Alava Narv{\'{a}}ez}, Mar{\'{i}}a Cristina and {Vicu{\~{n}}a Giraldo}, Gloria Carolina and {Mart{\'{i}}n La Rotta}, Claudia Jannet},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueroa Franco et al. - 2014 - Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lis.pdf:pdf},
journal = {Revista Colombiana de medicina Legal y Ciencias Forense},
keywords = {Cromosomas,Mitochondrial ADN},
number = {Revista Colombiana de Medicina Legal y Ciencias Forenses},
pages = {8--14},
title = {{Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lisis Gen{\'{e}}tico De Restos Humanos Durante 2009 En El Inmlcf De Bogot{\'{a}}, D. C.}},
volume = {2},
year = {2014}
}
@article{Pirooznia2014,
abstract = {BACKGROUND: The processing and analysis of the large scale data generated by next-generation sequencing (NGS) experiments is challenging and is a burgeoning area of new methods development. Several new bioinformatics tools have been developed for calling sequence variants from NGS data. Here, we validate the variant calling of these tools and compare their relative accuracy to determine which data processing pipeline is optimal. RESULTS: We developed a unified pipeline for processing NGS data that encompasses four modules: mapping, filtering, realignment and recalibration, and variant calling. We processed 130 subjects from an ongoing whole exome sequencing study through this pipeline. To evaluate the accuracy of each module, we conducted a series of comparisons between the single nucleotide variant (SNV) calls from the NGS data and either gold-standard Sanger sequencing on a total of 700 variants or array genotyping data on a total of 9,935 single-nucleotide polymorphisms. A head to head comparison showed that Genome Analysis Toolkit (GATK) provided more accurate calls than SAMtools (positive predictive value of 92.55% vs. 80.35%, respectively). Realignment of mapped reads and recalibration of base quality scores before SNV calling proved to be crucial to accurate variant calling. GATK HaplotypeCaller algorithm for variant calling outperformed the UnifiedGenotype algorithm. We also showed a relationship between mapping quality, read depth and allele balance, and SNV call accuracy. However, if best practices are used in data processing, then additional filtering based on these metrics provides little gains and accuracies of >99% are achievable. CONCLUSIONS: Our findings will help to determine the best approach for processing NGS data to confidently call variants for downstream analyses. To enable others to implement and replicate our results, all of our codes are freely available at http://metamoodics.org/wes.},
author = {Pirooznia, Mehdi and Kramer, Melissa and Parla, Jennifer and Goes, Fernando S and Potash, James B and McCombie, W Richard and Zandi, Peter P},
doi = {10.1186/1479-7364-8-14},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pirooznia et al. - 2014 - Validation and assessment of variant calling pipelines for next-generation sequencing.pdf:pdf},
isbn = {1479-7364 (Electronic)\r1473-9542 (Linking)},
issn = {1479-7364},
journal = {Human genomics},
keywords = {Bipolar Disorder,Bipolar Disorder: genetics,DNA,Data Interpretation,Exome,High-Throughput Nucleotide Sequencing,Humans,Polymorphism,Sequence Analysis,Single Nucleotide,Software,Statistical},
number = {1},
pages = {14},
pmid = {25078893},
title = {{Validation and assessment of variant calling pipelines for next-generation sequencing.}},
url = {http://www.humgenomics.com/content/8/1/14},
volume = {8},
year = {2014}
}
@article{Wang2010,
abstract = {High-throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes, but it remains a challenge to pinpoint a small subset of functionally important variants. To fill these unmet needs, we developed the ANNOVAR tool to annotate single nucleotide variants (SNVs) and insertions/deletions, such as examining their functional consequence on genes, inferring cytogenetic bands, reporting functional importance scores, finding variants in conserved regions, or identifying variants reported in the 1000 Genomes Project and dbSNP. ANNOVAR can utilize annotation databases from the UCSC Genome Browser or any annotation data set conforming to Generic Feature Format version 3 (GFF3). We also illustrate a 'variants reduction' protocol on 4.7 million SNVs and indels from a human genome, including two causal mutations for Miller syndrome, a rare recessive disease. Through a stepwise procedure, we excluded variants that are unlikely to be causal, and identified 20 candidate genes including the causal gene. Using a desktop computer, ANNOVAR requires ∼4 min to perform gene-based annotation and ∼15 min to perform variants reduction on 4.7 million variants, making it practical to handle hundreds of human genomes in a day. ANNOVAR is freely available at http://www.openbioinformatics.org/annovar/.},
author = {Wang, Kai and Li, Mingyao and Hakonarson, Hakon},
doi = {10.1093/nar/gkq603},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Li, Hakonarson - 2010 - ANNOVAR functional annotation of genetic variants from high-throughput sequencing data.pdf:pdf},
isbn = {0305-1048},
issn = {13624962},
journal = {Nucleic acids research},
number = {16},
pages = {e164},
pmid = {20601685},
title = {{ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data}},
volume = {38},
year = {2010}
}
@article{Lonardi2010,
author = {Lonardi, Stefano and Chen, Jake},
doi = {10.1109/TCBB.2010.28},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {apr},
number = {2},
pages = {195--196},
title = {{Data Mining in Bioinformatics: Selected Papers from BIOKDD}},
url = {http://ieeexplore.ieee.org/document/5460415/},
volume = {7},
year = {2010}
}
@article{Cleary2014,
author = {Cleary, John G and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S and Inglis, Stuart and Irvine, Sean A and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M},
doi = {10.1089/cmb.2014.0029},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and {<}i{>}De Novo{<}/i{>} Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@article{Quinlan2010,
abstract = {MOTIVATION: Testing for correlations between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.\n\nRESULTS: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format. BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features. The tools are extremely efficient and allow the user to compare large datasets (e.g. next-generation sequencing data) with both public and custom genome annotation tracks. BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.\n\nAVAILABILITY AND IMPLEMENTATION: BEDTools was written in C++. Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtools\n\nCONTACT: aaronquinlan@gmail.com; imh4y@virginia.edu\n\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Quinlan, Aaron R. and Hall, Ira M.},
doi = {10.1093/bioinformatics/btq033},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan, Hall - 2010 - BEDTools A flexible suite of utilities for comparing genomic features.pdf:pdf},
isbn = {1367-4811 (Electronic)\n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {841--842},
pmid = {20110278},
title = {{BEDTools: A flexible suite of utilities for comparing genomic features}},
volume = {26},
year = {2010}
}
@article{Frazer2009,
abstract = {The last few years have seen extensive efforts to catalogue human genetic variation and correlate it with phenotypic differences. Most common SNPs have now been assessed in genome-wide studies for statistical associations with many complex traits, including many important common diseases. Although these studies have provided new biological insights, only a limited amount of the heritable component of any complex trait has been identified and it remains a challenge to elucidate the functional link between associated variants and phenotypic traits. Technological advances, such as the ability to detect rare and structural variants, and a clear understanding of the challenges in linking different types of variation with phenotype, will be essential for future progress.},
author = {Frazer, KA and Murray, SS and Schork, NJ and Topol, EJ},
doi = {10.1038/nrg2554},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frazer et al. - 2009 - Human genetic variation and its contribution to complex traits.pdf:pdf},
isbn = {1471-0064 (Electronic)1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Reviews Genetics},
number = {4},
pages = {241--51},
pmid = {19293820},
title = {{Human genetic variation and its contribution to complex traits}},
volume = {10},
year = {2009}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803$\$r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Matthijs2015,
abstract = {We present, on behalf of EuroGentest and the European Society of Human Genetics, guidelines for the evaluation and validation of next-generation sequencing (NGS) applications for the diagnosis of genetic disorders. The work was performed by a group of laboratory geneticists and bioinformaticians, and discussed with clinical geneticists, industry and patients' representatives, and other stakeholders in the field of human genetics. The statements that were written during the elaboration of the guidelines are presented here. The background document and full guidelines are available as supplementary material. They include many examples to assist the laboratories in the implementation of NGS and accreditation of this service. The work and ideas presented by others in guidelines that have emerged elsewhere in the course of the past few years were also considered and are acknowledged in the full text. Interestingly, a few new insights that have not been cited before have emerged during the preparation of the guidelines. The most important new feature is the presentation of a 'rating system' for NGS-based diagnostic tests. The guidelines and statements have been applauded by the genetic diagnostic community, and thus seem to be valuable for the harmonization and quality assurance of NGS diagnostics in Europe. Next-generation sequencing (NGS) allows for the fast generation of thousands to millions of base pairs of DNA sequence of an individual patient. The relatively fast emergence and the great success of these technologies in research herald a new era in genetic diagnostics. However, the new technologies bring challenges, both at the technical level and in terms of data management, as well as for the interpreta-tion of the results and for counseling. We believe that all these aspects warrant consideration of what the precise role of NGS in diagnostics will be, today and tomorrow. Before even embarking on acquisition of machines and skills for performing NGS in diagnostics, many issues have to be dealt with. It is in this context that we propose the guidelines. These guidelines mostly deal with NGS testing in the context of rare and mostly monogenic diseases. They mainly focus on the targeted analysis of gene panels, either through specific capture assays, or by extracting data from whole-exome sequencing. In principle, whole-genome sequencing may – and shortly will – also be used to extract similar information. In that case, the guidelines would still apply, but because whole-genome sequencing would also allow detecting other molecular features of disease, they would have to be extended accordingly. The different aspects of NGS and diagnostics were discussed during three workshops. The first took place in Leuven, 25–26 February 2013. The preliminary views were presented during the EuroGentest Scientific Meeting in Prague, 7–8 March 2013. The second was an editorial workshop in Leuven, 1–2 October 2013, where the different people involved in writing the document came together to discuss the layout of the document and prepare},
author = {Matthijs, Gert and Souche, Erika and Alders, Mari{\"{e}}lle and Corveleyn, Anniek and Eck, Sebastian and Feenstra, Ilse and Race, Val{\'{e}}rie and Sistermans, Erik and Sturm, Marc and Weiss, Marjan and Yntema, Helger and Bakker, Egbert and Scheffer, Hans and Bauer, Peter},
doi = {10.1038/ejhg.2015.226},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthijs et al. - 2015 - Guidelines for diagnostic next-generation sequencing.pdf:pdf},
journal = {European Journal of Human Genetics},
number = {10},
pages = {2--5},
title = {{Guidelines for diagnostic next-generation sequencing}},
volume = {24},
year = {2015}
}
@article{Maharjan2011,
author = {Maharjan, Merina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maharjan - 2011 - Genome Analysis with MapReduce.pdf:pdf},
pages = {1--23},
title = {{Genome Analysis with MapReduce}},
year = {2011}
}
@article{Wang2009,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
doi = {10.1038/nrg2484},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gerstein, Snyder - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
keywords = {Animals,Base Sequence,Chromosome Mapping,Exons,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Genetic,Molecular Sequence Data,RNA,RNA: analysis,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Transcription, Genetic},
number = {1},
pages = {57--63},
pmid = {19015660},
title = {{RNA-Seq: a revolutionary tool for transcriptomics.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2949280&tool=pmcentrez&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Li2017,
abstract = {In 2015, the American College of Medical Genetics and Genomics (ACMG) and the Association for Molecular Pathology (AMP) published updated standards and guidelines for the clinical interpretation of sequence variants with respect to human diseases on the basis of 28 criteria. However, variability between individual interpreters can be extensive because of reasons such as the different understandings of these guidelines and the lack of standard algorithms for implementing them, yet computational tools for semi-automated variant interpretation are not available. To address these problems, we propose a suite of methods for implementing these criteria and have developed a tool called InterVar to help human reviewers interpret the clinical significance of variants. InterVar can take a pre-annotated or VCF file as input and generate automated interpretation on 18 criteria. Furthermore, we have developed a companion web server, wInterVar, to enable user-friendly variant interpretation with an automated interpretation step and a manual adjustment step. These tools are especially useful for addressing severe congenital or very early-onset developmental disorders with high penetrance. Using results from a few published sequencing studies, we demonstrate the utility of InterVar in significantly reducing the time to interpret the clinical significance of sequence variants.},
author = {Li, Quan and Wang, Kai},
doi = {10.1016/j.ajhg.2017.01.004},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Wang - 2017 - InterVar Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines.pdf:pdf},
isbn = {0002-9297},
issn = {15376605},
journal = {American Journal of Human Genetics},
keywords = {ACMG,ANNOVAR,ClinVar,InterVar,clinical interpretation,genetic diagnosis,variant annotation,variant interpretation},
number = {2},
pages = {267--280},
pmid = {28132688},
publisher = {ElsevierCompany.},
title = {{InterVar: Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines}},
url = {http://dx.doi.org/10.1016/j.ajhg.2017.01.004},
volume = {100},
year = {2017}
}
@misc{Louie2007,
abstract = {Genomic medicine aims to revolutionize health care by applying our growing understanding of the molecular basis of disease. Research in this arena is data intensive, which means data sets are large and highly heterogeneous. To create knowledge from data, researchers must integrate these large and diverse data sets. This presents daunting informatic challenges such as representation of data that is suitable for computational inference (knowledge representation), and linking heterogeneous data sets (data integration). Fortunately, many of these challenges can be classified as data integration problems, and technologies exist in the area of data integration that may be applied to these challenges. In this paper, we discuss the opportunities of genomic medicine as well as identify the informatics challenges in this domain. We also review concepts and methodologies in the field of data integration. These data integration concepts and methodologies are then aligned with informatics challenges in genomic medicine and presented as potential solutions. We conclude this paper with challenges still not addressed in genomic medicine and gaps that remain in data integration research to facilitate genomic medicine. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Louie, Brenton and Mork, Peter and Martin-Sanchez, Fernando and Halevy, Alon and Tarczy-Hornoch, Peter},
booktitle = {Journal of Biomedical Informatics},
doi = {10.1016/j.jbi.2006.02.007},
file = {:home/jennifer/Descargas/1-s2.0-S1532046406000244-main.pdf:pdf},
isbn = {1532-0480 (Electronic)\n1532-0464 (Linking)},
issn = {15320464},
keywords = {Biomedical informatics,Data integration,Genomic medicine,Genomics,Knowledge representation},
month = {feb},
number = {1},
pages = {5--16},
pmid = {16574494},
publisher = {Academic Press},
title = {{Data integration and genomic medicine}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046406000244},
volume = {40},
year = {2007}
}
@article{Palmisano2016,
abstract = {Trials involving genomic-driven treatment selection require the
coordination of many teams interacting with a great variety of
information. The need of better informatics support to manage this
complex set of operations motivated the creation of OpenGeneMed.
OpenGeneMed is a stand-alone and customizable version of GeneMed (Zhao
et al. GeneMed: an informatics hub for the coordination of
next-generation sequencing studies that support precision oncology
clinical trials. Cancer Inform 2015; 14(Suppl 2): 45), a web-based
interface developed for the National Cancer Institute Molecular
Profiling-based Assignment of Cancer Therapy (NCI-MPACT) clinical trial
coordinated by the NIH. OpenGeneMed streamlines clinical trial
management and it can be used by clinicians, lab personnel,
statisticians and researchers as a communication hub. It automates the
annotation of genomic variants identified by sequencing tumor DNA,
classifies the actionable mutations according to customizable rules and
facilitates quality control in reviewing variants. The system generates
summarized reports with detected genomic alterations that a treatment
review team can use for treatment assignment. OpenGeneMed allows
collaboration to happen seamlessly along the clinical pipeline; it helps
reduce errors made transferring data between groups and facilitates
clear documentation along the pipeline. OpenGeneMed is distributed as a
stand-alone virtual machine, ready for deployment and use from a web
browser; its code is customizable to address specific needs of different
clinical trials and research teams. Examples on how to change the code
are provided in the technical documentation distributed with the virtual
machine. In summary, OpenGeneMed offers an initial set of features
inspired by our experience with GeneMed, a system that has been proven
to be efficient and successful for coordinating the application of
next-generation sequencing in the NCI-MPACT trial.},
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming-Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/Descargas/palmisano2016.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {alida palmisano,and applications to support,and diagnosis,at the biometric research,clinical trial management,clinical trial management system,division of cancer treatment,genomics,her research,his research,interests include development of,is a computational biologist,is a postdoctoral fellow,nci,next-generation sequencing,nih,open source software,phd,precision medicine,program,software for genomic analysis,yingdong zhao},
number = {March},
pages = {bbw059},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
year = {2016}
}
@article{Yang2015,
abstract = {Recent developments in sequencing techniques have enabled rapid and high-throughput generation of sequence data, democratizing the ability to compile information on large amounts of genetic variations in individual laboratories. However, there is a growing gap between the generation of raw sequencing data and the extraction of meaningful biological information. Here, we describe a protocol to use the ANNOVAR (ANNOtate VARiation) software to facilitate fast and easy variant annotations, including gene-based, region-based and filter-based annotations on a variant call format (VCF) file generated from human genomes. We further describe a protocol for gene-based annotation of a newly sequenced nonhuman species. Finally, we describe how to use a user-friendly and easily accessible web server called wANNOVAR to prioritize candidate genes for a Mendelian disease. The variant annotation protocols take 5-30 min of computer time, depending on the size of the variant file, and 5-10 min of hands-on time. In summary, through the command-line tool and the web server, these protocols provide a convenient means to analyze genetic variants generated in humans and other species.},
author = {Yang, H and Wang, K},
doi = {10.1038/nprot.2015.105},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Wang - 2015 - Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR.pdf:pdf},
isbn = {1750-2799 (Electronic)\r1750-2799 (Linking)},
issn = {1750-2799},
journal = {Nat Protoc},
number = {10},
pages = {1556--1566},
pmid = {26379229},
publisher = {Nature Publishing Group},
title = {{Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26379229%5Cnhttp://www.nature.com/nprot/journal/v10/n10/pdf/nprot.2015.105.pdf},
volume = {10},
year = {2015}
}
@article{Eduardoff2015,
abstract = {Abstract Next generation sequencing (NGS) offers the opportunity to analyse forensic DNA samples and obtain massively parallel coverage of targeted short sequences with the variants they carry. We evaluated the levels of sequence coverage, genotyping precision, sensitivity and mixed DNA patterns of a prototype version of the first commercial forensic NGS kit: the HID-Ion AmpliSeq™ Identity Panel with 169-markers designed for the Ion PGM™ system. Evaluations were made between three laboratories following closely matched Ion PGM™ protocols and a simple validation framework of shared DNA controls. The sequence coverage obtained was extensive for the bulk of SNPs targeted by the HID-Ion AmpliSeq™ Identity Panel. Sensitivity studies showed 90-95% of SNP genotypes could be obtained from 25 to 100 pg of input DNA. Genotyping concordance tests included Coriell cell-line control DNA analyses checked against whole-genome sequencing data from 1000 Genomes and Complete Genomics, indicating a very high concordance rate of 99.8%. Discordant genotypes detected in rs1979255, rs1004357, rs938283, rs2032597 and rs2399332 indicate these loci should be excluded from the panel. Therefore, the HID-Ion AmpliSeq™ Identity Panel and Ion PGM™ system provide a sensitive and accurate forensic SNP genotyping assay. However, low-level DNA produced much more varied sequence coverage and in forensic use the Ion PGM™ system will require careful calibration of the total samples loaded per chip to preserve the genotyping reliability seen in routine forensic DNA. Furthermore, assessments of mixed DNA indicate the user's control of sequence analysis parameter settings is necessary to ensure mixtures are detected robustly. Given the sensitivity of Ion PGM™, this aspect of forensic genotyping requires further optimisation before massively parallel sequencing is applied to routine casework.},
author = {Eduardoff, M. and Santos, C. and {De La Puente}, M. and Gross, T. E. and Fondevila, M. and Strobl, C. and Sobrino, B. and Ballard, D. and Schneider, P. M. and Carracedo and Lareu, M. V. and Parson, W. and Phillips, C.},
doi = {10.1016/j.fsigen.2015.04.007},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eduardoff et al. - 2015 - Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Identification SNPs,Ion PGM™,Ion Torrent,Massively parallel sequencing,Next generation sequencing},
pages = {110--121},
pmid = {25955683},
publisher = {Elsevier Ireland Ltd},
title = {{Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the Ion PGM™}},
url = {http://dx.doi.org/10.1016/j.fsigen.2015.04.007},
volume = {17},
year = {2015}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en%7B&%7DbtnG=Search%7B&%7Dq=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey%7B#%7D0},
volume = {16},
year = {2004}
}
@article{Cornish2015,
abstract = {High-throughput sequencing, especially of exomes, is a popular diagnostic tool, but it is difficult to determine which tools are the best at analyzing this data. In this study, we use the NIST Genome in a Bottle results as a novel resource for validation of our exome analysis pipeline. We use six different aligners and five different variant callers to determine which pipeline, of the 30 total, performs the best on a human exome that was used to help generate the list of variants detected by the Genome in a Bottle Consortium. Of these 30 pipelines, we found that Novoalign in conjunction with GATK UnifiedGenotyper exhibited the highest sensitivity while maintaining a low number of false positives for SNVs. However, it is apparent that indels are still difficult for any pipeline to handle with none of the tools achieving an average sensitivity higher than 33% or a Positive Predictive Value (PPV) higher than 53%. Lastly, as expected, it was found that aligners can play as vital a role in variant detection as variant callers themselves},
author = {Cornish, Adam and Guda, Chittibabu},
doi = {10.1155/2015/456479},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cornish, Guda - 2015 - A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference.pdf:pdf},
issn = {23146141},
journal = {BioMed Research International},
number = {BioMed Research International},
pages = {11},
pmid = {26539496},
title = {{A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference}},
volume = {2015},
year = {2015}
}
@article{Hegde2017,
abstract = {Context.—With the decrease in the cost of sequencing, the clinical testing paradigm has shifted from single gene to gene panel and now whole-exome and whole-genome sequencing. Clinical laboratories are rapidly implementing next-generation sequencing–based whole-exome and whole-genome sequencing. Because a large number of targets are covered by whole-exome and whole-genome sequencing, it is critical that a laboratory perform appropriate validation studies, develop a quality assurance and quality control program, and participate in proficiency testing. Objective.—To provide recommendations for whole-exome and whole-genome sequencing assay design, validation, and implementation for the detection of germ-line variants associated in inherited disorders. Data Sources.—An example of trio sequencing, filtration and annotation of variants, and phenotypic consideration to arrive at clinical diagnosis is discussed. Conclusions.—It is critical that clinical laboratories planning to implement whole-exome and whole-genome sequencing design and validate the assay to specifications and ensure adequate performance prior to implementa-tion. Test design specifications, including variant filtering and annotation, phenotypic consideration, guidance on consenting options, and reporting of incidental findings, are provided. These are important steps a laboratory must take to validate and implement whole-exome and whole-genome sequencing in a clinical setting for germline variants in inherited disorders.},
author = {Hegde, Madhuri and Santani, Avni and Mao, Rong and Ferreira-Gonzalez, Andrea and Weck, Karen E. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2016-0622-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hegde et al. - 2017 - Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {798--805},
pmid = {28362156},
title = {{Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants in inherited disease}},
volume = {141},
year = {2017}
}
@article{Higasa2016,
abstract = {Whole-genome and -exome resequencing using next-generation sequencers is a powerful approach for identifying genomic variations that are associated with diseases. However, systematic strategies for prioritizing causative variants from many candidates to explain the disease phenotype are still far from being established, because the population-specific frequency spectrum of genetic variation has not been characterized. Here, we have collected exomic genetic variation from 1208 Japanese individuals through a collaborative effort, and aggregated the data into a prevailing catalog. In total, we identified 156 622 previously unreported variants. The allele frequencies for the majority (88.8%) were lower than 0.5% in allele frequency and predicted to be functionally deleterious. In addition, we have constructed a Japanese-specific major allele reference genome by which the number of unique mapping of the short reads in our data has increased 0.045% on average. Our results illustrate the importance of constructing an ethnicity-specific reference genome for identifying rare variants. All the collected data were centralized to a newly developed database to serve as useful resources for exploring pathogenic variations. Public access to the database is available at INTRODUCTION Next-generation sequencing technologies are revolutionizing the approach in identifying genetic variants that are associated with diseases. A current promising strategy focuses on rare variants that},
author = {Higasa, Koichiro and Miyake, Noriko and Yoshimura, Jun and Okamura, Kohji and Niihori, Tetsuya and Saitsu, Hirotomo and Doi, Koichiro and Shimizu, Masakazu and Nakabayashi, Kazuhiko and Aoki, Yoko and Tsurusaki, Yoshinori and Morishita, Shinichi and Kawaguchi, Takahisa and Migita, Osuke and Nakayama, Keiko and Nakashima, Mitsuko and Mitsui, Jun and Narahara, Maiko and Hayashi, Keiko and Funayama, Ryo and Yamaguchi, Daisuke and Ishiura, Hiroyuki and Ko, Wen-Ya and Hata, Kenichiro and Nagashima, Takeshi and Yamada, Ryo and Matsubara, Yoichi and Umezawa, Akihiro and Tsuji, Shoji and Matsumoto, Naomichi and Matsuda, Fumihiko},
doi = {10.1038/jhg.2016.12},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higasa et al. - 2016 - Human genetic variation database, a reference database of genetic variations in the Japanese population.pdf:pdf},
issn = {1434-5161},
journal = {Journal of Human Genetics},
number = {6},
pages = {547--553},
pmid = {26911352},
publisher = {Nature Publishing Group},
title = {{Human genetic variation database, a reference database of genetic variations in the Japanese population}},
url = {http://www.nature.com/doifinder/10.1038/jhg.2016.12},
volume = {61},
year = {2016}
}
@book{Yunis2002,
author = {Yunis, E and Yunis, J},
publisher = {Temis S.A.},
title = {{El ADN en la Identificaci{\'{o}}n Humana}},
year = {2002}
}
@article{Bacardit2014,
abstract = {Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented.},
author = {Bacardit, Jaume and Widera, Pawe{\l} and Lazzarini, Nicola and Krasnogor, Natalio},
doi = {10.1089/big.2014.0023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacardit et al. - 2014 - Hard Data Analytics Problems Make for Better Data Analysis Algorithms Bioinformatics as an Example.pdf:pdf},
issn = {2167-6461},
journal = {Big data},
number = {3},
pages = {164--176},
pmid = {25276500},
title = {{Hard Data Analytics Problems Make for Better Data Analysis Algorithms: Bioinformatics as an Example.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4174911&tool=pmcentrez&rendertype=abstract},
volume = {2},
year = {2014}
}
@article{Breuer2017,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breuer et al. - 2017 - Genotype-phenotype association mining in bipolar disorder market research meets complex genetics.pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {116624},
publisher = {Cold Spring Harbor Laboratory},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Systems2009,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - FASTQ format and data quality.pdf:pdf},
title = {{FASTQ format and data quality}},
year = {2009}
}
@article{Balasubramanian2017,
abstract = {Variants predicted to result in the loss of function of human genes have attracted interest because of their clinical impact and surprising prevalence in healthy individuals. Here, we present ALoFT (annotation of loss-of-function transcripts), a method to annotate and predict the disease-causing potential of loss-of-function variants. Using data from Mendelian disease-gene discovery projects, we show that ALoFT can distinguish between loss-of-function variants that are deleterious as heterozygotes and those causing disease only in the homozygous state. Investigation of variants discovered in healthy populations suggests that each individual carries at least two heterozygous premature stop alleles that could potentially lead to disease if present as homozygotes. When applied to de novo putative loss-of-function variants in autism-affected families, ALoFT distinguishes between deleterious variants in patients and benign variants in unaffected siblings. Finally, analysis of somatic variants in >6500 cancer exomes shows that putative loss-of-function variants predicted to be deleterious by ALoFT are enriched in known driver genes.},
author = {Balasubramanian, Suganthi and Fu, Yao and Pawashe, Mayur and McGillivray, Patrick and Jin, Mike and Liu, Jeremy and Karczewski, Konrad J. and MacArthur, Daniel G. and Gerstein, Mark},
doi = {10.1038/s41467-017-00443-5},
file = {:home/jennifer/Descargas/s41467-017-00443-5.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pmid = {28851873},
publisher = {Springer US},
title = {{Using ALoFT to determine the impact of putative loss-of-function variants in protein-coding genes}},
url = {http://dx.doi.org/10.1038/s41467-017-00443-5},
volume = {8},
year = {2017}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
pages = {51--68},
title = {{Salud P{{\'{u}}}blica • Epidemiolog{{\'{i}}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{DanecekPAutonA2011,
abstract = {SUMMARY: The Variant Call Format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing, and also provides a general Perl API. AVAILABILITY: http://vcftools.sourceforge.net CONTACT: rd@sanger.ac.uk.},
author = {Danecek, P and Auton, A and Abecasis, G and Albers, C and Banks, E and Depristo, M and Handsaker, R and Lunter, G and Marth, G and Sherry, S and McVean, G and Durbin, R},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danecek et al. - 2011 - The Variant Call Format and VCFtools.pdf:pdf},
journal = {Bioinformatics},
pages = {3--5},
title = {{The Variant Call Format and VCFtools}},
url = {http://bioinformatics.oxfordjournals.org/content/early/2011/06/07/bioinformatics.btr330.long},
year = {2011}
}
@article{Li2017a,
author = {Li, Marilyn M and Datto, Michael and Duncavage, Eric J and Kulkarni, Shashikant and Lindeman, Neal I and Roy, Somak and Tsimberidou, Apostolia M and Vnencak-jones, Cindy L and Wolff, Daynna J and Younes, Anas and Nikiforova, Marina N},
doi = {10.1016/j.jmoldx.2016.10.002},
file = {:home/jennifer/Descargas/main.pdf:pdf},
issn = {1525-1578},
journal = {The Journal of Molecular Diagnostics},
number = {1},
pages = {4--23},
publisher = {American Society for Investigative Pathology and the Association for Molecular Pathology},
title = {{Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.10.002},
volume = {19},
year = {2017}
}
@article{Tetreault2015,
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
journal = {Expert Review of Molecular Diagnostics},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities.}},
year = {2015}
}
@article{Kumari2014,
author = {Kumari, D Aruna and Bhavana, D Poojitha and Aditya, V Venkata Sai},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumari, Bhavana, Aditya - 2014 - Data Mining in Biodata Analysis.pdf:pdf},
number = {9},
pages = {1--3},
title = {{Data Mining in Biodata Analysis}},
volume = {14},
year = {2014}
}
@article{Farid2016,
abstract = {In this paper, we introduce a new adaptive rule-based classifier for multi-class classification of biological data, where several problems of classifying biological data are addressed: overfitting, noisy instances and class-imbalance data. It is well known that rules are interesting way for representing data in a human interpretable way. The proposed rule-based classifier combines the random subspace and boosting approaches with ensemble of decision trees to construct a set of classification rules without involving global optimisation. The classifier considers random subspace approach to avoid overfitting, boosting approach for classifying noisy instances and ensemble of decision trees to deal with class-imbalance problem. The classifier uses two popular classification techniques: decision tree and k-nearest-neighbor algorithms. Decision trees are used for evolving classification rules from the training data, while k-nearest-neighbor is used for analysing the misclassified instances and removing vagueness between the contradictory rules. It considers a series of k iterations to develop a set of classification rules from the training data and pays more attention to the misclassified instances in the next iteration by giving it a boosting flavour. This paper particularly focuses to come up with an optimal ensemble classifier that will help for improving the prediction accuracy of DNA variant identification and classification task. The performance of proposed classifier is tested with compared to well-approved existing machine learning and data mining algorithms on genomic data (148 Exome data sets) of Brugada syndrome and 10 real benchmark life sciences data sets from the UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed classifier has exemplary classification accuracy on different types of biological data. Overall, the proposed classifier offers good prediction accuracy to new DNA variants classification where noisy and misclassified variants are optimised to increase test performance.},
annote = {NULL},
author = {Farid, Dewan Md. and Al-Mamun, Mohammad Abdullah and Manderick, Bernard and Nowe, Ann},
doi = {10.1016/j.eswa.2016.08.008},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farid et al. - 2016 - An adaptive rule-based classifier for mining big biological data.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Brugada syndrome,Classification,Decision tree,Genomic data,Rule-based classifier},
pages = {305--316},
title = {{An adaptive rule-based classifier for mining big biological data}},
volume = {64},
year = {2016}
}
@article{Wu2017b,
abstract = {—Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant in-formation for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Re-sults: To demonstrate how big data analytics enables preci-sion medicine, we provide two case studies, including iden-tifying disease biomarkers from multi-omic data and incor-porating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal im-pact. Index Terms—Big data analytics, bioinformatics, elec-tronic health records (EHRs), health informatics, –omic data, precision medicine.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D and Venugopalan, Janani and Hoffman, Ryan and Wang, May D},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
number = {2},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
volume = {64},
year = {2017}
}
@book{Auwera2014,
author = {Auwera, Geraldine A Van Der and Carneiro, Mauricio O and Hartl, Chris and Poplin, Ryan and Levy-moonshine, Ami and Jordan, Tadeusz and Shakir, Khalid and Roazen, David and Thibault, Joel and Banks, Eric and Garimella, Kiran V and Altshuler, David and Gabriel, Stacey and Depristo, Mark A},
booktitle = {Curr Protoc Bioinformatics},
doi = {10.1002/0471250953.bi1110s43.From},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auwera et al. - 2014 - From FastQ data to high confidence varant calls the Genonme Analysis Toolkit best practices pipeline.pdf:pdf},
isbn = {0471250953},
keywords = {exome,genotyping,ngs,variant detection,wgs},
number = {1110},
title = {{From FastQ data to high confidence varant calls: the Genonme Analysis Toolkit best practices pipeline}},
volume = {11},
year = {2014}
}
@article{Pandey2016a,
abstract = {Traditional Sanger sequencing as well as Next-Generation Sequencing have been used for the identification of disease causing mutations in human molecular research. The majority of currently available tools are developed for research and explorative purposes and often do not provide a complete, efficient, one-stop solution. As the focus of currently developed tools is mainly on NGS data analysis, no integrative solution for the analysis of Sanger data is provided and consequently a one-stop solution to analyze reads from both sequencing platforms is not available. We have therefore developed a new pipeline called MutAid to analyze and interpret raw sequencing data produced by Sanger or several NGS sequencing platforms. It performs format conversion, base calling, quality trimming, filtering, read mapping, variant calling, variant annotation and analysis of Sanger and NGS data under a single platform. It is capable of analyzing reads from multiple patients in a single run to create a list of potential disease causing base substitutions as well as insertions and deletions. MutAid has been developed for expert and non-expert users and supports four sequencing platforms including Sanger, Illumina, 454 and Ion Torrent. Furthermore, for NGS data analysis, five read mappers including BWA, TMAP, Bowtie, Bowtie2 and GSNAP and four variant callers including GATK-HaplotypeCaller, SAMTOOLS, Freebayes and VarScan2 pipelines are supported. MutAid is freely available at https://sourceforge.net/projects/mutaid.},
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1371/journal.pone.0147697},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - MutAid Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human m.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--22},
pmid = {26840129},
title = {{MutAid: Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human molecular genetics}},
volume = {11},
year = {2016}
}
@article{Baes2014a,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle(2).pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Roy2018,
abstract = {Bioinformatics pipelines are an integral component of next-generation sequencing (NGS). Processing raw sequence data to detect genomic alterations has significant impact on disease management and patient care. Because of the lack of published guidance, there is currently a high degree of variability in how members of the global molecular genetics and pathology community establish and validate bioinformatics pipelines. Improperly developed, validated, and/or monitored pipelines may generate inaccurate results that may have negative consequences for patient care. To address this unmet need, the Association of Molecular Pathology, with organizational representation from the College of American Pathologists and the American Medical Informatics Association, has developed a set of 17 best practice consensus recommendations for the validation of clinical NGS bioinformatics pipelines. Recommendations include practical guidance for laboratories regarding NGS bioinformatics pipeline design, development, and operation, with additional emphasis on the role of a properly trained and qualified molecular professional to achieve optimal NGS testing quality.},
author = {Roy, Somak and Coldren, Christopher and Karunamurthy, Arivarasan and Kip, Nefize S. and Klee, Eric W. and Lincoln, Stephen E. and Leon, Annette and Pullambhatla, Mrudula and Temple-Smolkin, Robyn L. and Voelkerding, Karl V. and Wang, Chen and Carter, Alexis B.},
doi = {10.1016/j.jmoldx.2017.11.003},
file = {:home/jennifer/Descargas/PIIS1525157817303732.pdf:pdf},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {1},
pages = {4--27},
pmid = {29154853},
title = {{Standards and Guidelines for Validating Next-Generation Sequencing Bioinformatics Pipelines: A Joint Recommendation of the Association for Molecular Pathology and the College of American Pathologists}},
volume = {20},
year = {2018}
}
@article{Peng2018,
abstract = {OBJECTIVE
Data quality assessment is a challenging facet for research using coded administrative health data. Current assessment approaches are time and resource intensive. We explored whether association rule mining (ARM) can be used to develop rules for assessing data quality. 

MATERIALS AND METHODS
We extracted 2013 and 2014 records from the hospital discharge abstract database (DAD) for patients between the ages of 55 and 65 from five acute care hospitals in Alberta, Canada. The ARM was conducted using the 2013 DAD to extract rules with support ≥ 0.0019 and confidence ≥ 0.5 using the bootstrap technique, and tested in the 2014 DAD. The rules were compared against the method of coding frequency and assessed for their ability to detect error introduced by two kinds of data manipulation: random permutation and random deletion. 

RESULTS
The association rules generally had clear clinical meanings. Comparing 2014 data to 2013 data (both original), there were 3 rules with a confidence difference > 0.1, while coding frequency difference of codes in the right hand of rules was less than 0.004. After random permutation of 50% of codes in the 2014 data, average rule confidence dropped from 0.72 to 0.27 while coding frequency remained unchanged. Rule confidence decreased with the increase of coding deletion, as expected. Rule confidence was more sensitive to code deletion compared to coding frequency, with slope of change ranging from 1.7 to 184.9 with a median of 9.1. 

CONCLUSION
The ARM is a promising technique to assess data quality. It offers a systematic way to derive coding association rules hidden in data, and potentially provides a sensitive and efficient method of assessing data quality compared to standard methods.},
author = {Peng, Mingkai and Sundararajan, Vijaya and Williamson, Tyler and Minty, Evan P. and Smith, Tony C. and Doktorchik, Chelsea T.A. and Quan, Hude},
doi = {10.1016/j.jbi.2018.02.001},
file = {:home/jennifer/Descargas/10.1016@j.jbi.2018.02.001.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
pmid = {29425732},
title = {{Exploration of association rule mining for coding consistency and completeness assessment in inpatient administrative health data}},
url = {https://doi.org/10.1016/j.jbi.2018.02.001},
year = {2018}
}
@article{Deng2011,
abstract = {BACKGROUND The popularity of massively parallel exome and transcriptome sequencing projects demands new data mining tools with a comprehensive set of features to support a wide range of analysis tasks. RESULTS SeqGene, a new data mining tool, supports mutation detection and annotation, dbSNP and 1000 Genome data integration, RNA-Seq expression quantification, mutation and coverage visualization, allele specific expression (ASE), differentially expressed genes (DEGs) identification, copy number variation (CNV) analysis, and gene expression quantitative trait loci (eQTLs) detection. We also developed novel methods for testing the association between SNP and expression and identifying genotype-controlled DEGs. We showed that the results generated from SeqGene compares favourably to other existing methods in our case studies. CONCLUSION SeqGene is designed as a general-purpose software package. It supports both paired-end reads and single reads generated on most sequencing platforms; it runs on all major types of computers; it supports arbitrary genome assemblies for arbitrary organisms; and it scales well to support both large and small scale sequencing projects. The software homepage is http://seqgene.sourceforge.net.},
author = {Deng, Xutao},
doi = {10.1186/1471-2105-12-267},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng - 2011 - SeqGene a comprehensive software solution for mining exome- and transcriptome- sequencing data.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
month = {jun},
pages = {267},
pmid = {21714929},
publisher = {BioMed Central},
title = {{SeqGene: a comprehensive software solution for mining exome- and transcriptome- sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21714929 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3148209},
volume = {12},
year = {2011}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Mohammed2014,
abstract = {The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis. This so called “big data” challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields. Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability. It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structured/unstructured data. The MapReduce programming framework uses two tasks common in functional programming: Map and Reduce. MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters. Compared with existing parallel processing paradigms (e.g. grid computing and graphical processing unit (GPU)), MapReduce and Hadoop have two advantages: 1) fault-tolerant storage resulting in reliable data processing by replicating the computing tasks, and cloning the data chunks on different computing nodes across the computing cluster; 2) high-throughput data processing via a batch processing framework and the Hadoop distributed file system (HDFS). Data are stored in the HDFS and made available to the slave nodes for computation. In this paper, we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields. The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization, and opens up new opportunities in the emerging era of big data analytics. The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools. This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields.},
author = {Mohammed, Emad a. and Far, Behrouz H. and Naugler, Christopher},
doi = {10.1186/1756-0381-7-22},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammed, Far, Naugler - 2014 - Applications of the MapReduce programming framework to clinical big data analysis current landscape and.pdf:pdf},
isbn = {1756-0381 (Linking)},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {Big data,Bioinformatics,Clinical big data analysis,Clinical data analysis,Distributed programming,Hadoop,MapReduce},
number = {1},
pages = {1--23},
pmid = {25383096},
title = {{Applications of the MapReduce programming framework to clinical big data analysis: current landscape and future trends}},
url = {http://link.springer.com/article/10.1186/1756-0381-7-22%5Cnhttp://link.springer.com/content/pdf/10.1186/1756-0381-7-22.pdf},
volume = {7},
year = {2014}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.\n\nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.\n\nDATA SOURCES: Literature review and authors' experience.\n\nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M. and Durtschi, Jacob D. and Margraf, Rebecca L. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)\r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Neto2014,
abstract = {Inherited myopathies are a heterogeneous group of disabling disorders with still barely understood pathological mechanisms. Around 40% of afflicted patients remain without a molecular diagnosis after exclusion of known genes. The advent of high-throughput sequencing has opened avenues to the discovery of new implicated genes, but a working list of prioritized candidate genes is necessary to deal with the complexity of analyzing large-scale sequencing data. Here we used an integrative data mining strategy to analyze the genetic network linked to myopathies, derive specific signatures for inherited myopathy and related disorders, and identify and rank candidate genes for these groups. Training sets of genes were selected after literature review and used in Manteia, a public web-based data mining system, to extract disease group signatures in the form of enriched descriptor terms, which include functional annotation, human and mouse phenotypes, as well as biological pathways and protein interactions. These specific signatures were then used as an input to mine and rank candidate genes, followed by filtration against skeletal muscle expression and association with known diseases. Signatures and identified candidate genes highlight both potential common pathological mechanisms and allelic disease groups. Recent discoveries of gene associations to diseases, like B3GALNT2, GMPPB and B3GNT1 to congenital muscular dystrophies, were prioritized in the ranked lists, suggesting a posteriori validation of our approach and predictions. We show an example of how the ranked lists can be used to help analyze high-throughput sequencing data to identify candidate genes, and highlight the best candidate genes matching genomic regions linked to myopathies without known causative genes. This strategy can be automatized to generate fresh candidate gene lists, which help cope with database annotation updates as new knowledge is incorporated.},
author = {Neto, Osorio Abath and Tassy, Olivier and Biancalana, Val?? Rie and Zanoteli, Edmar and Pourqui??, Olivier and Laporte, Jocelyn},
doi = {10.1371/journal.pone.0110888},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto et al. - 2014 - Integrative data mining highlights candidate genes for monogenic myopathies.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {25353622},
title = {{Integrative data mining highlights candidate genes for monogenic myopathies}},
volume = {9},
year = {2014}
}
@article{Sherry2001,
author = {Sherry, S. T. and Ward, M.-H. and Kholodov, M. and Baker, J. and Phan, L. and Smigielski, E. M. and Sirotkin, K.},
doi = {10.1093/nar/29.1.308},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sherry et al. - 2001 - dbSNP the NCBI database of genetic variation.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
keywords = {biotechnology,chromosome mapping,genbank,genetics,genome,genome, human,national library of medicine (u.s.),patients' rooms,single nucleotide polymorphism,united states national institutes of health},
month = {jan},
number = {1},
pages = {308--311},
publisher = {Oxford University Press},
title = {{dbSNP: the NCBI database of genetic variation}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/29.1.308},
volume = {29},
year = {2001}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Huttenhower2010,
abstract = {The aim of this work is to be able to publish the information concerning communication with cancer patients as recommended in England. The observation and the study protocol during the stay abroad have been given the opportunity to stylize specific information on the methodology of communication of important information to terminally ill patients. It seems readily apparent as they characterized by both technical precision and sensivity to emotions and descriptions for the individual patient. How is shared by all chronic pain is predominantly complex emotion, a mix of additions and perceived physical and emotional pain - emotional. Because accurate information is beneficial to the patient and that really is not turned, so to speak, a "bullet" it is necessary that you have created, over time, a concrete "therapeutic alliance" between body physician, patient and possibly family. This arises, for sure, even at first accepted the patient during the clinical visit attentive to detail, is renewed in the definition of the common objective to be achieved, so analgesia and it is expressed in the certainty that the physician provides all the resources realistically available. It is then up to the sensitivity of the operator, doctor and/or nurse, described in the "take charge" find, from time to time, the words and manners, verbal and nonverbal, to respond fully to questions of the patient same.},
author = {Huttenhower, Curtis and Hofmann, Oliver},
doi = {10.1371/journal.pcbi.1000779},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huttenhower, Hofmann - 2010 - A quick guide to large-scale genomic data mining.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {5},
pages = {1--6},
pmid = {20523745},
title = {{A quick guide to large-scale genomic data mining}},
volume = {6},
year = {2010}
}
@article{Medina2016,
abstract = {As sequencing technologies progress, the amount of data produced grows exponentially, shifting the bottleneck of discovery towards the data analysis phase. In particular, currently available mapping solutions for RNA-seq leave room for improvement in terms of sensitivity and performance, hindering an efficient analysis of transcriptomes by massive sequencing. Here, we present an innovative approach that combines re-engineering, optimization and parallelization. This solution results in a significant increase of mapping sensitivity over a wide range of read lengths and substantial shorter runtimes when compared with current RNA-seq mapping methods available.},
author = {Medina, I and T{\'{a}}rraga, J and Mart{\'{i}}nez, H and Barrachina, S and Castillo, M I and Paschall, J and Salavert-Torres, J and Blanquer-Espert, I and Hern{\'{a}}ndez-Garc{\'{i}}a, V and Quintana-Ort{\'{i}}, E S and Dopazo, J},
doi = {10.1093/dnares/dsv039},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medina et al. - 2016 - Highly sensitive and ultrafast read mapping for RNA-seq analysis.pdf:pdf},
issn = {1756-1663},
journal = {DNA research : an international journal for rapid publication of reports on genes and genomes},
keywords = {burrows-wheeler transform,high-performance computing,mapping,rna-seq},
number = {January},
pages = {dsv039},
pmid = {26740642},
title = {{Highly sensitive and ultrafast read mapping for RNA-seq analysis.}},
url = {http://dnaresearch.oxfordjournals.org/content/early/2016/01/05/dnares.dsv039.full},
volume = {23},
year = {2016}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bash - 2015 - VARIATION INTERPRETATION PREDICTORS PRINCIPLES, TYPES, PERFORMANCE AND CHOICE.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Bamshad2011,
abstract = {Nature Reviews Genetics 12, 745 (2011). doi:10.1038/nrg3031},
archivePrefix = {arXiv},
arxivId = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
author = {Bamshad, Michael J and Ng, Sarah B and Bigham, Abigail W and Tabor, Holly K and Emond, Mary J and Nickerson, Deborah A and Shendure, Jay},
doi = {10.1038/nrg3031},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bamshad et al. - 2011 - Exome sequencing as a tool for Mendelian disease gene discovery.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Publishing Group},
keywords = {Alleles,Base Sequence,Exome,Exome: genetics,Genetic Predisposition to Disease,Genome, Human,Genome-Wide Association Study,Humans,Molecular Sequence Data,Pedigree,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {11},
pages = {745--755},
pmid = {21946919},
primaryClass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:},
publisher = {Nature Publishing Group},
title = {{Exome sequencing as a tool for Mendelian disease gene discovery}},
url = {http://dx.doi.org/10.1038/nrg3031%5Cnpapers2://publication/doi/10.1038/nrg3031},
volume = {12},
year = {2011}
}
@article{Moore2010,
abstract = {MOTIVATION: The sequencing of the human genome has made it possible to identify an informative set of >1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs). The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing. This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies. However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing. One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology. Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context. There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype-phenotype relationship that is characterized by significant heterogeneity and gene-gene and gene-environment interaction. We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases. The goal of this review is to identify and discuss those GWAS challenges that will require computational methods.},
author = {Moore, Jason H. and Asselbergs, Folkert W. and Williams, Scott M.},
doi = {10.1093/bioinformatics/btp713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Asselbergs, Williams - 2010 - Bioinformatics challenges for genome-wide association studies.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pages = {445--455},
pmid = {20053841},
title = {{Bioinformatics challenges for genome-wide association studies}},
volume = {26},
year = {2010}
}
@article{Wu2017,
abstract = {—Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant in-formation for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Re-sults: To demonstrate how big data analytics enables preci-sion medicine, we provide two case studies, including iden-tifying disease biomarkers from multi-omic data and incor-porating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal im-pact. Index Terms—Big data analytics, bioinformatics, elec-tronic health records (EHRs), health informatics, –omic data, precision medicine.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D and Venugopalan, Janani and Hoffman, Ryan and Wang, May D},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
number = {2},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
volume = {64},
year = {2017}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@article{Pei,
abstract = {High-throughput next generation sequencing (NGS) has been quickly adapted into many aspects of biomedical research and begun to engage with the clinical practice. The latter aspect will enable the application of genomic knowl- edge into clinical practice in this and next decades and will profoundly change the diagnosis, prognosis and treatment of many human diseases. It will further demand both philosophical and medical curriculum reforms in the training of our future physicians. However, significant huddles need to be overcome before an ultimate application},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {{Pei Hui}},
doi = {10.1007/128},
eprint = {15334406},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pei Hui - 2012 - Next Generation Sequencing Chemistry, Technology and Applications.pdf:pdf},
isbn = {0002-9297},
issn = {0392856X},
journal = {Chemical Diagnostics Topics in Current Chemistry},
keywords = {genomic medicine,next generation sequencing {\'{a}}},
number = {1},
pages = {1--18},
pmid = {28642624},
title = {{Next Generation Sequencing: Chemistry, Technology and Applications}},
volume = {336},
year = {2012}
}
@article{Guo2013,
abstract = {Advances in next-generation sequencing (NGS) technologies have greatly improved our ability to detect genomic variants for biomedical research. In particular, NGS technologies have been recently applied with great success to the discovery of mutations associated with the growth of various tumours and in rare Mendelian diseases. The advance in NGS technologies has also created significant challenges in bioinformatics. One of the major challenges is quality control of the sequencing data. In this review, we discuss the proper quality control procedures and parameters for Illumina technology-based human DNA re-sequencing at three different stages of sequencing: raw data, alignment and variant calling. Monitoring quality control metrics at each of the three stages of NGS data provides unique and independent evaluations of data quality from differing perspectives. Properly conducting quality control protocols at all three stages and correctly interpreting the quality control results are crucial to ensure a successful and meaningful study.},
author = {Guo, Yan and Ye, Fei and Sheng, Quanghu and Clark, Travis and Samuels, David C.},
doi = {10.1093/bib/bbt069},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2013 - Three-stage quality control strategies for DNA re-sequencing data.pdf:pdf},
isbn = {1477-4054 (Electronic)\r1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Alignment,FASTQ,Quality control,Sequencing,Variant calling},
number = {6},
pages = {879--889},
pmid = {24067931},
title = {{Three-stage quality control strategies for DNA re-sequencing data}},
volume = {15},
year = {2013}
}
@article{Lauzon2016,
author = {Lauzon, David and Kanzki, Beatriz and Dupuy, Victor and April, Alain and Phillips, Michael S. and Tremblay, Johanne and Hamet, Pavel},
doi = {10.1109/CHASE.2016.79},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lauzon et al. - 2016 - Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS).pdf:pdf},
isbn = {9781509009435},
journal = {Proceedings - 2016 IEEE 1st International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2016},
keywords = {Big Data,GWAS health systems provenance,GWAS visualization,dbSNP discrepancies,open-source},
pages = {382--387},
title = {{Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS)}},
year = {2016}
}
@article{Terlizzi2017a,
abstract = {BACKGROUND The effect of complex alleles in cystic fibrosis (CF) is poorly defined for the lack of functional studies. OBJECTIVES To describe the genotype-phenotype correlation and the results of either in vitro and ex vivo studies performed on nasal epithelial cells (NEC) in a cohort of patients with CF carrying cystic fibrosis transmembrane conductance regulator (CFTR) complex alleles. METHODS We studied 70 homozygous, compound heterozygous or heterozygous for CFTR mutations: p.[Arg74Trp;Val201Met;Asp1270Asn], n=8; p.[Ile148Thr;Ile1023_Val1024del], n=5; p.[Arg117Leu;Leu997Phe], n=6; c.[1210-34TG[12];1210-12T[5];2930C>T], n=3; p.[Arg74Trp;Asp1270Asn], n=4; p.Asp1270Asn, n=2; p.Ile148Thr, n=6; p.Leu997Phe, n=36. In 39 patients, we analysed the CFTR gating activity on NEC in comparison with patients with CF (n=8) and carriers (n=4). Finally, we analysed in vitro the p.[Arg74Trp;Val201Met;Asp1270Asn] complex allele. RESULTS The p.[Ile148Thr;Ile1023_Val1024del] caused severe CF in five compound heterozygous with a class I-II mutation. Their CFTR activity on NEC was comparable with patients with two class I-II mutations (mean 7.3% vs 6.9%). The p.[Arg74Trp;Asp1270Asn] and the p.Asp1270Asn have scarce functional effects, while p.[Arg74Trp;Val201Met;Asp1270Asn] caused mild CF in four of five subjects carrying a class I-II mutation in trans, or CFTR-related disorders (CFTR-RD) in three having in trans a class IV-V mutation. The p.[Arg74Trp;Val201Met;Asp1270Asn] causes significantly (p<0.001) higher CFTR activity compared with compound heterozygous for class I-II mutations. Furthermore, five of six compounds heterozygous with the p.[Arg117Leu;Leu997Phe] had mild CF, whereas the p.Leu997Phe, in trans with a class I-II CFTR mutation, caused CFTR-RD or a healthy status (CFTR activity: 21.3-36.9%). Finally, compounds heterozygous for the c.[1210-34TG[12];1210-12T[5];2930C>T] and a class I-II mutation had mild CF or CFTR-RD (gating activity: 18.5-19.0%). CONCLUSIONS The effect of complex alleles partially depends on the mutation in trans. Although larger studies are necessary, the CFTR activity on NEC is a rapid contributory tool to classify patients with CFTR dysfunction.},
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and {Di Lullo}, Antonella Miriam and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Sofia, Valentina Maria and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
issn = {0022-2593},
journal = {Journal of Medical Genetics},
keywords = {[I148T;3199del6bp],[L997F;R117L],[R74W;V201M;D1270N],gating activity,nasal brushing},
month = {apr},
number = {4},
pages = {224--235},
pmid = {27738188},
title = {{Genotype–phenotype correlation and functional studies in patients with cystic fibrosis bearing CFTR complex alleles}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27738188 http://jmg.bmj.com/lookup/doi/10.1136/jmedgenet-2016-103985},
volume = {54},
year = {2017}
}
@article{Wang2017a,
author = {Wang, Fei and Li, Xiao-Li and Wang, Jason T. L. and Ng, See-Kiong},
doi = {10.1109/TCBB.2016.2612558},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {may},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
url = {http://ieeexplore.ieee.org/document/7938559/},
volume = {14},
year = {2017}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Cheng2015,
author = {Cheng, Phil and Levesque, Mitch and Cheng, Phil F and Dummer, Reinhard and Levesque, Mitch P},
doi = {10.4414/smw.2015.14183},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2015 - Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in.pdf:pdf},
keywords = {cancer are somatic mutation,copy number,data mining,data mining, Genomics, transcriptomics, Cancer Gen,gene expres-,genomics,on the tcga data,portal for each,the cancer genome atlas,the data types listed,transcriptomics},
number = {October},
pages = {1--5},
title = {{Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in the era of precision cancer medicine}},
year = {2015}
}
@article{Beltrao2012b,
address = {New York, NY},
author = {Beltrao, Pedro and Ryan, Colm and Krogan, Nevan J and Ryan, C},
doi = {10.1007/978-1-4614-3567-9},
editor = {Soyer, Orkun S.},
file = {::},
isbn = {978-1-4614-3566-2},
publisher = {Springer New York},
series = {Advances in Experimental Medicine and Biology},
title = {{Evolutionary Systems Biology}},
url = {http://www.springerlink.com/index/10.1007/978-1-4614-3567-9},
volume = {751},
year = {2012}
}
@article{Lees2010,
abstract = {Over the last 2 years the Gene3D resource has been significantly improved, and is now more accurate and with a much richer interactive display via the Gene3D website (http://gene3d.biochem.ucl.ac.uk/). Gene3D provides accurate structural domain family assignments for over 1100 genomes and nearly 10,000,000 proteins. A hidden Markov model library, constructed from the manually curated CATH structural domain hierarchy, is used to search UniProt, RefSeq and Ensembl protein sequences. The resulting matches are refined into simple multi-domain architectures using a recently developed in-house algorithm, DomainFinder 3 (available at: ftp://ftp.biochem.ucl.ac.uk/pub/gene3d_data/DomainFinder3/). The domain assignments are integrated with multiple external protein function descriptions (e.g. Gene Ontology and KEGG), structural annotations (e.g. coiled coils, disordered regions and sequence polymorphisms) and family resources (e.g. Pfam and eggNog) and displayed on the Gene3D website. The website allows users to view descriptions for both single proteins and genes and large protein sets, such as superfamilies or genomes. Subsets can then be selected for detailed investigation or associated functions and interactions can be used to expand explorations to new proteins. Gene3D also provides a set of services, including an interactive genome coverage graph visualizer, DAS annotation resources, sequence search facilities and SOAP services.},
author = {Lees, Jonathan and Yeats, Corin and Redfern, Oliver and Clegg, Andrew and Orengo, Christine},
doi = {10.1093/nar/gkp987},
file = {::},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Animals,Computational Biology,Computational Biology: methods,Computational Biology: trends,Databases, Genetic,Databases, Nucleic Acid,Databases, Protein,Genome, Archaeal,Genome, Bacterial,Genome, Viral,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Internet,Markov Chains,Protein Structure, Tertiary,Sequence Analysis, DNA,Software},
month = {jan},
number = {Database issue},
pages = {D296--300},
pmid = {19906693},
title = {{Gene3D: merging structure and function for a Thousand genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2808988&tool=pmcentrez&rendertype=abstract},
volume = {38},
year = {2010}
}
@article{Price2008a,
abstract = {BACKGROUND: All-versus-all BLAST, which searches for homologous pairs of sequences in a database of proteins, is used to identify potential orthologs, to find new protein families, and to provide rapid access to these homology relationships. As DNA sequencing accelerates and data sets grow, all-versus-all BLAST has become computationally demanding. METHODOLOGY/PRINCIPAL FINDINGS: We present FastBLAST, a heuristic replacement for all-versus-all BLAST that relies on alignments of proteins to known families, obtained from tools such as PSI-BLAST and HMMer. FastBLAST avoids most of the work of all-versus-all BLAST by taking advantage of these alignments and by clustering similar sequences. FastBLAST runs in two stages: the first stage identifies additional families and aligns them, and the second stage quickly identifies the homologs of a query sequence, based on the alignments of the families, before generating pairwise alignments. On 6.53 million proteins from the non-redundant Genbank database ("NR"), FastBLAST identifies new families 25 times faster than all-versus-all BLAST. Once the first stage is completed, FastBLAST identifies homologs for the average query in less than 5 seconds (8.6 times faster than BLAST) and gives nearly identical results. For hits above 70 bits, FastBLAST identifies 98% of the top 3,250 hits per query. CONCLUSIONS/SIGNIFICANCE: FastBLAST enables research groups that do not have supercomputers to analyze large protein sequence data sets. FastBLAST is open source software and is available at http://microbesonline.org/fastblast.},
author = {Price, Morgan N and Dehal, Paramvir S and Arkin, Adam P},
doi = {10.1371/journal.pone.0003589},
file = {::},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Animals,Databases, Nucleic Acid,Databases, Protein,Efficiency,Humans,Multigene Family,Proteins,Proteins: analysis,Proteins: chemistry,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Sequence Homology, Amino Acid,Software},
month = {jan},
number = {10},
pages = {e3589},
pmid = {18974889},
title = {{FastBLAST: homology relationships for millions of proteins.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18974889},
volume = {3},
year = {2008}
}
@article{Sjolander2010a,
author = {Sj{\"{o}}lander, Kimmen},
doi = {10.1371/journal.pcbi.1000621},
file = {::},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Genomics,Phylogeny},
month = {jan},
number = {1},
pages = {e1000621},
pmid = {20126522},
title = {{Getting started in structural phylogenomics.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2813252&tool=pmcentrez&rendertype=abstract},
volume = {6},
year = {2010}
}
@article{Yeats2010a,
abstract = {MOTIVATION: Accurate prediction of the domain content and arrangement in multi-domain proteins (which make up >65% of the large-scale protein databases) provides a valuable tool for function prediction, comparative genomics and studies of molecular evolution. However, scanning a multi-domain protein against a database of domain sequence profiles can often produce conflicting and overlapping matches. We have developed a novel method that employs heaviest weighted clique-finding (HCF), which we show significantly outperforms standard published approaches based on successively assigning the best non-overlapping match (Best Match Cascade, BMC). RESULTS: We created benchmark data set of structural domain assignments in the CATH database and a corresponding set of Hidden Markov Model-based domain predictions. Using these, we demonstrate that by considering all possible combinations of matches using the HCF approach, we achieve much higher prediction accuracy than the standard BMC method. We also show that it is essential to allow overlapping domain matches to a query in order to identify correct domain assignments. Furthermore, we introduce a straightforward and effective protocol for resolving any overlapping assignments, and producing a single set of non-overlapping predicted domains. AVAILABILITY AND IMPLEMENTATION: The new approach will be used to determine MDAs for UniProt and Ensembl, and made available via the Gene3D website: http://gene3d.biochem.ucl.ac.uk/Gene3D/. The software has been implemented in C++ and compiled for Linux: source code and binaries can be found at: ftp://ftp.biochem.ucl.ac.uk/pub/gene3d_data/DomainFinder3/ CONTACT: yeats@biochem.ucl.ac.uk SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Yeats, Corin and Redfern, Oliver C and Orengo, Christine},
doi = {10.1093/bioinformatics/btq034},
file = {::},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {mar},
number = {6},
pages = {745--51},
pmid = {20118117},
title = {{A fast and automated solution for accurately resolving protein domain architectures.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20118117},
volume = {26},
year = {2010}
}
@article{Beltrao2012a,
address = {New York, NY},
author = {Beltrao, Pedro and Ryan, Colm and Krogan, Nevan J and Ryan, C},
doi = {10.1007/978-1-4614-3567-9},
editor = {Soyer, Orkun S.},
file = {::},
isbn = {978-1-4614-3566-2},
publisher = {Springer New York},
series = {Advances in Experimental Medicine and Biology},
title = {{Evolutionary Systems Biology}},
url = {http://www.springerlink.com/index/10.1007/978-1-4614-3567-9},
volume = {751},
year = {2012}
}
@article{Ostergard2002a,
author = {{\"{O}}sterg{\aa}rd, Patric R.J.},
doi = {10.1016/S0166-218X(01)00290-6},
file = {::},
issn = {0166218X},
journal = {Discrete Applied Mathematics},
month = {aug},
number = {1-3},
pages = {197--207},
title = {{A fast algorithm for the maximum clique problem☆}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0166218X01002906},
volume = {120},
year = {2002}
}
