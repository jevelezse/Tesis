@article{Yousef2018,
abstract = {Retinoblastoma (RB) is a childhood cancer developing in the retina due to RB1 pathologic variant. Herein we are evaluating the oncogenic mutations in the RB1 gene and the inheritance patterns of RB in the Jordanian patients. In this prospective study, the peripheral blood of 50 retinoblastoma patients was collected, genomic DNA was extracted, mutations were identified using Quantitative multiplex PCR (QM-PCR), Allele-specific PCR, Next Generation Sequencing analysis, and Sanger sequencing. In this cohort of 50 patients, 20(40%) patients had unilateral RB and 30(60%) were males. Overall, 36(72%) patients had germline disease, 17(47%) of whom had the same RB1 pathologic variant detected in one of the parents (inherited disease). In the bilateral group, all (100%) patients had germline disease; 13(43%) of them had inherited mutation. In the unilateral group, 6(30%) had germline disease, 4(20%) of them had inherited mutation. Nonsense mutation generating a stop codon and producing a truncated non-functional protein was the most frequent detected type of mutations (n = 15/36, 42%). Only one (2%) of the patients had mosaic mutation, and of the 17 inherited cases, 16(94%) had an unaffected carrier parent. In conclusion, in addition to all bilateral RB patients in our cohort, 30% of unilateral cases showed germline mutation. Almost half (47%) of germline cases had inherited disease from affected (6%) parent or unaffected carrier (94%). Therefore molecular screening is critical for the genetic counseling regarding the risk for inherited RB in both unilateral and bilateral cases including those with no family history.},
author = {Yousef, Yacoub A. and Tbakhi, Abdelghani and Al-Hussaini, Maysa and AlNawaiseh, Ibrahim and Saab, Ala and Afifi, Amal and Naji, Maysa and Mohammad, Mona and Deebajah, Rasha and Jaradat, Imad and Sultan, Iyad and Mehyar, Mustafa},
doi = {10.1007/s10689-017-0027-5},
file = {:home/jennifer/Descargas/yousef2017.pdf:pdf},
isbn = {1068901700275},
issn = {15737292},
journal = {Familial Cancer},
keywords = {Germline,Mutation,RB1 gene,Retinoblastoma},
number = {2},
pages = {261--268},
pmid = {28803391},
publisher = {Springer Netherlands},
title = {{Mutational analysis of the RB1 gene and the inheritance patterns of retinoblastoma in Jordan}},
volume = {17},
year = {2018}
}
@article{Liu2016,
abstract = {The purpose of the dbNSFP is to provide a one-stop resource for functional predictions and annotations for human nonsynonymous single-nucleotide variants (nsSNVs) and splice-site variants (ssSNVs), and to facilitate the steps of filtering and prioritizing SNVs from a large list of SNVs discovered in an exome-sequencing study. A list of all potential nsSNVs and ssSNVs based on the human reference sequence were created and functional predictions and annotations were curated and compiled for each SNV. Here, we report a recent major update of the database to version 3.0. The SNV list has been rebuilt based on GENCODE 22 and currently the database includes 82,832,027 nsSNVs and ssSNVs. An attached database dbscSNV, which compiled all potential human SNVs within splicing consensus regions and their deleteriousness predictions, add another 15,030,459 potentially functional SNVs. Eleven prediction scores (MetaSVM, MetaLR, CADD, VEST3, PROVEAN, 4× fitCons, fathmm-MKL, and DANN) and allele frequencies from the UK10K cohorts and the Exome Aggregation Consortium (ExAC), among others, have been added. The original seven prediction scores in v2.0 (SIFT, 2× Polyphen2, LRT, MutationTaster, MutationAssessor, and FATHMM) as well as many SNV and gene functional annotations have been updated. dbNSFP v3.0 is freely available at http://sites.google.com/site/jpopgen/dbNSFP.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Liu, Xiaoming and Wu, Chunlei and Li, Chang and Boerwinkle, Eric},
doi = {10.1002/humu.22932},
eprint = {15334406},
file = {:home/jennifer/Descargas/liu2016.pdf:pdf},
isbn = {1098-1004 (Electronic) 1059-7794 (Linking)},
issn = {10981004},
journal = {Human Mutation},
keywords = {Database,DbNSFP,DbscSNV,Functional prediction,Nonsynonymous mutation,Splice-site mutation},
number = {3},
pages = {235--241},
pmid = {26555599},
title = {{dbNSFP v3.0: A One-Stop Database of Functional Predictions and Annotations for Human Nonsynonymous and Splice-Site SNVs}},
volume = {37},
year = {2016}
}
@article{Vasquez2010,
author = {V{\'{a}}squez, Dra Catalina and Aristiz{\'{a}}bal, Ricardo and Daza, Wilson and Madero, Danitza and Medina, Socorro and Parra, William and Pedraza, Angela Mar{\'{i}}a and Ricardo, Jose and Posada, Ricardo and Sara, Marco and Stand, Iv{\'{a}}n},
file = {:home/jennifer/Descargas/publicacion_146.pdf:pdf},
journal = {Neumologia pediatrica http://www.neumologia-pediatrica.cl},
number = {1},
pages = {44--50},
title = {{Fibrosis qu{\'{i}}stica en Colombia}},
year = {2010}
}
@article{Zerbino2018,
abstract = {The Ensembl project has been aggregating, processing, integrating and redistributing genomic datasets since the initial releases of the draft human genome, with the aim of accelerating genomics research through rapid open distribution of public data. Large amounts of raw data are thus transformed into knowledge, which is made available via a multitude of channels, in particular our browser (http://www.ensembl.org). Over time, we have expanded in multiple directions. First, our resources describe multiple fields of genomics, in particular gene annotation, comparative genomics, genetics and epigenomics. Second, we cover a growing number of genome assemblies; Ensembl Release 90 contains exactly 100. Third, our databases feed simultaneously into an array of services designed around different use cases, ranging from quick browsing to genome-wide bioinformatic analysis. We present here the latest developments of the Ensembl project, with a focus on managing an increasing number of assemblies, supporting efforts in genome interpretation and improving our browser.},
author = {Zerbino, Daniel R. and Achuthan, Premanand and Akanni, Wasiu and Amode, M. Ridwan and Barrell, Daniel and Bhai, Jyothish and Billis, Konstantinos and Cummins, Carla and Gall, Astrid and Gir{\'{o}}n, Carlos Garc{\'{i}}a and Gil, Laurent and Gordon, Leo and Haggerty, Leanne and Haskell, Erin and Hourlier, Thibaut and Izuogu, Osagie G. and Janacek, Sophie H. and Juettemann, Thomas and To, Jimmy Kiang and Laird, Matthew R. and Lavidas, Ilias and Liu, Zhicheng and Loveland, Jane E. and Maurel, Thomas and McLaren, William and Moore, Benjamin and Mudge, Jonathan and Murphy, Daniel N. and Newman, Victoria and Nuhn, Michael and Ogeh, Denye and Ong, Chuang Kee and Parker, Anne and Patricio, Mateus and Riat, Harpreet Singh and Schuilenburg, Helen and Sheppard, Dan and Sparrow, Helen and Taylor, Kieron and Thormann, Anja and Vullo, Alessandro and Walts, Brandon and Zadissa, Amonida and Frankish, Adam and Hunt, Sarah E. and Kostadima, Myrto and Langridge, Nicholas and Martin, Fergal J. and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and Staines, Dan M. and Trevanion, Stephen J. and Aken, Bronwen L. and Cunningham, Fiona and Yates, Andrew and Flicek, Paul},
doi = {10.1093/nar/gkx1098},
file = {::},
isbn = {13624962 (Electronic)},
issn = {13624962},
journal = {Nucleic Acids Research},
keywords = {bioinformatics,genome,genome, human,genomics},
month = {jan},
number = {D1},
pages = {D754--D761},
pmid = {29155950},
publisher = {Oxford University Press},
title = {{Ensembl 2018}},
url = {http://academic.oup.com/nar/article/46/D1/D754/4634002},
volume = {46},
year = {2018}
}
@article{Terlizzi2017b,
abstract = {BACKGROUND The effect of complex alleles in cystic fibrosis (CF) is poorly defined for the lack of functional studies. OBJECTIVES To describe the genotype-phenotype correlation and the results of either in vitro and ex vivo studies performed on nasal epithelial cells (NEC) in a cohort of patients with CF carrying cystic fibrosis transmembrane conductance regulator (CFTR) complex alleles. METHODS We studied 70 homozygous, compound heterozygous or heterozygous for CFTR mutations: p.[Arg74Trp;Val201Met;Asp1270Asn], n=8; p.[Ile148Thr;Ile1023_Val1024del], n=5; p.[Arg117Leu;Leu997Phe], n=6; c.[1210-34TG[12];1210-12T[5];2930C>T], n=3; p.[Arg74Trp;Asp1270Asn], n=4; p.Asp1270Asn, n=2; p.Ile148Thr, n=6; p.Leu997Phe, n=36. In 39 patients, we analysed the CFTR gating activity on NEC in comparison with patients with CF (n=8) and carriers (n=4). Finally, we analysed in vitro the p.[Arg74Trp;Val201Met;Asp1270Asn] complex allele. RESULTS The p.[Ile148Thr;Ile1023_Val1024del] caused severe CF in five compound heterozygous with a class I-II mutation. Their CFTR activity on NEC was comparable with patients with two class I-II mutations (mean 7.3% vs 6.9%). The p.[Arg74Trp;Asp1270Asn] and the p.Asp1270Asn have scarce functional effects, while p.[Arg74Trp;Val201Met;Asp1270Asn] caused mild CF in four of five subjects carrying a class I-II mutation in trans, or CFTR-related disorders (CFTR-RD) in three having in trans a class IV-V mutation. The p.[Arg74Trp;Val201Met;Asp1270Asn] causes significantly (p<0.001) higher CFTR activity compared with compound heterozygous for class I-II mutations. Furthermore, five of six compounds heterozygous with the p.[Arg117Leu;Leu997Phe] had mild CF, whereas the p.Leu997Phe, in trans with a class I-II CFTR mutation, caused CFTR-RD or a healthy status (CFTR activity: 21.3-36.9%). Finally, compounds heterozygous for the c.[1210-34TG[12];1210-12T[5];2930C>T] and a class I-II mutation had mild CF or CFTR-RD (gating activity: 18.5-19.0%). CONCLUSIONS The effect of complex alleles partially depends on the mutation in trans. Although larger studies are necessary, the CFTR activity on NEC is a rapid contributory tool to classify patients with CFTR dysfunction.},
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and {Di Lullo}, Antonella Miriam and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Sofia, Valentina Maria and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
file = {:home/jennifer/Descargas/terlizzi2016.pdf:pdf},
issn = {14686244},
journal = {Journal of Medical Genetics},
number = {4},
pages = {224--235},
pmid = {27738188},
title = {{Genotype-phenotype correlation and functional studies in patients with cystic fibrosis bearing CFTR complex alleles}},
volume = {54},
year = {2017}
}
@article{Farrell2016,
abstract = {Objective Cystic fibrosis (CF) can be difficult to diagnose, even when newborn screening (NBS) tests yield positive results. This challenge is exacerbated by the multitude of NBS protocols, misunderstandings about screening vs diagnostic tests, and the lack of guidelines for presumptive diagnoses. There is also confusion regarding the designation of age at diagnosis. Study design To improve diagnosis and achieve standardization in definitions worldwide, the CF Foundation convened a committee of 32 experts with a mission to develop clear and actionable consensus guidelines on diagnosis of CF with an emphasis on screened populations, especially the newborn population. A comprehensive literature review was performed with emphasis on relevant articles published during the past decade. Results After reviewing the common screening protocols and outcome scenarios, 14 of 27 consensus statements were drafted that apply to screened populations. These were approved by 80% or more of the participants. Conclusions It is recommended that all diagnoses be established by demonstrating dysfunction of the CF transmembrane conductance regulator (CFTR) channel, initially with a sweat chloride test and, when needed, potentially with newer methods assessing membrane transport directly, such as intestinal current measurements. Even in babies with 2 CF-causing mutations detected via NBS, diagnosis must be confirmed by demonstrating CFTR dysfunction. The committee also recommends that the latest classifications identified in the Clinical and Functional Translation of CFTR project [http://www.cftr2.org/index.php] should be used to aid with CF diagnosis. Finally, to avoid delays in treatment, we provide guidelines for presumptive diagnoses and recommend how to determine the age of diagnosis.},
author = {Farrell, Philip M. and White, Terry B. and Howenstine, Michelle S. and Munck, Anne and Parad, Richard B. and Rosenfeld, Margaret and Sommerburg, Olaf and Accurso, Frank J. and Davies, Jane C. and Rock, Michael J. and Sanders, Don B. and Wilschanski, Michael and Sermet-Gaudelus, Isabelle and Blau, Hannah and Gartner, Silvia and McColley, Susanna A.},
doi = {10.1016/j.jpeds.2016.09.065},
file = {:home/jennifer/Descargas/PIIS0022347616310496.pdf:pdf},
issn = {10976833},
journal = {Journal of Pediatrics},
keywords = {CF screen positive, inconclusive diagnosis,CFTR-related metabolic syndrome,immunoreactive trypsinogen,intestinal current measurement,nasal potential difference,newborn screening,pancreatitis associated protein,sweat test},
pages = {S33--S44},
pmid = {28129810},
publisher = {Elsevier Inc.},
title = {{Diagnosis of Cystic Fibrosis in Screened Populations}},
url = {http://dx.doi.org/10.1016/j.jpeds.2016.09.065},
volume = {181},
year = {2017}
}
@article{Eilbeck2017,
abstract = {For clinical cases of Mendelian disease that lack a genetic diagnosis, genome and exome sequencing are increasingly used for seeking the genetic cause. This Review discusses the strategies and computational tools for prioritizing the many genetic variants identified in each genome into those that are most likely to be causal for disease. The authors discuss how diverse types of biochemical, evolutionary, pedigree and clinical-phenotype information are used, and they highlight common pitfalls to be aware of for responsible variant prioritization.},
author = {Eilbeck, Karen and Quinlan, Aaron and Yandell, Mark},
doi = {10.1038/nrg.2017.52},
file = {:home/jennifer/Descargas/eilbeck2017.pdf:pdf},
isbn = {1471-0064},
issn = {14710064},
journal = {Nature Reviews Genetics},
number = {10},
pages = {599--612},
pmid = {28804138},
publisher = {Nature Publishing Group},
title = {{Settling the score: Variant prioritization and Mendelian disease}},
url = {http://dx.doi.org/10.1038/nrg.2017.52},
volume = {18},
year = {2017}
}
@article{McCarthy2017,
abstract = {We summarize the remarkable progress that has been made in the identification and functional characterization of DNA sequence variants associated with disease.},
author = {McCarthy, Mark I. and MacArthur, Daniel G.},
doi = {10.1186/s13059-017-1160-z},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McCarthy, MacArthur - 2017 - Human disease genomics from variants to biology.pdf:pdf},
isbn = {1305901711},
issn = {1474760X},
journal = {Genome Biology},
keywords = {Animal Genetics and Genomics,Bioinformatics,Evolutionary Biology,Human Genetics,Microbial Genetics and Genomics,Plant Genetics & Genomics},
month = {dec},
number = {1},
pages = {20},
pmid = {28137298},
publisher = {BioMed Central},
title = {{Human disease genomics: From variants to biology}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1160-z},
volume = {18},
year = {2017}
}
@article{Rishishwar2015a,
abstract = {The human dimension of the Columbian Exchange entailed substantial genetic admixture between ancestral source populations from Africa, the Americas and Europe, which had evolved separately for many thousands of years. We sought to address the implications of the creation of admixed American genomes, containing novel allelic combinations, for human health and fitness via analysis of an admixed Colombian population from Medellin. Colombian genomes from Medellin show a wide range of three-way admixture contributions from ancestral source populations. The primary ancestry component for the population is European (average = 74.6%, range = 45.0%-96.7%), followed by Native American (average = 18.1%, range = 2.1%-33.3%) and African (average = 7.3%, range = 0.2%-38.6%). Locus-specific patterns of ancestry were evaluated to search for genomic regions that are enriched across the population for particular ancestry contributions. Adaptive and innate immune system related genes and pathways are particularly over-represented among ancestry-enriched segments, including genes (HLA-B and MAPK10) that are involved in defense against endemic pathogens such as malaria. Genes that encode functions related to skin pigmentation (SCL4A5) and cutaneous glands (EDAR) are also found in regions with anomalous ancestry patterns. These results suggest the possibility that ancestry-specific loci were differentially retained in the modern admixed Colombian population based on their utility in the New World environment.},
author = {Rishishwar, Lavanya and Conley, Andrew B. and Wigington, Charles H. and Wang, Lu and Valderrama-Aguirre, Augusto and {King Jordan}, I.},
doi = {10.1038/srep12376},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rishishwar et al. - 2015 - Ancestry, admixture and fitness in Colombian genomes(2).pdf:pdf},
isbn = {2045-2322 (ISSNLinking)},
issn = {20452322},
journal = {Scientific Reports},
keywords = {Biological anthropology,Comparative genomics,Genome informatics},
month = {dec},
number = {1},
pages = {12376},
pmid = {26197429},
publisher = {Nature Publishing Group},
title = {{Ancestry, admixture and fitness in Colombian genomes}},
url = {http://www.nature.com/articles/srep12376},
volume = {5},
year = {2015}
}
@misc{GabrielBedoya,
author = {{Gabriel Bedoya}, Maria V. Parra and {Andr{\'{e}}s Ruiz-Linares}},
title = {{NHGRI Collection - 1000 Genomes - Colombian in Medell{\'{i}}n, Colombia}},
url = {https://www.coriell.org/0/Sections/Collections/NHGRI/1000Clm.aspx?PgId=675&coll=HG},
urldate = {2018-05-24}
}
@article{Arias-blanco2015a,
abstract = {<p><p>Objetivo: describir variantes de secuencia en los genes BRCA1 y BRCA2 en una muestra de pacientes colombianas con historia personal o familiar de c{\'{a}}ncer de mama sugestiva de riesgo gen{\'{e}}tico.</p><p>Materiales y m{\'{e}}todos: serie de casos compuesta por 67 pacientes que fueron remitidas para estudio gen{\'{e}}tico por sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama y ovario hereditario (HBOC). De los 67 casos, 42 (62,7 %) cumplieron con los criterios de indicaci{\'{o}}n m{\'{e}}dica de la National Comprehensive Cancer Network (NCCN) del 2013, y en ellos se realiz{\'{o}} secuenciaci{\'{o}}n completa de los genes BRCA1 y BRCA2. Se determin{\'{o}} la frecuencia de mutaci{\'{o}}n, variantes de secuencia y significancia cl{\'{i}}nica de las variantes halladas con base en Breast Cancer Information Core (BIC).</p><p>Resultados: se identificaron mutaciones para el gen BRCA1 en seis pacientes (14,3 %), no se document{\'{o}} mutaci{\'{o}}n para el gen BRCA2, adem{\'{a}}s se detectaron 43 variantes gen{\'{e}}ticas en 27 pacientes (64,2 % de 42 casos). De estas, 21 (48,8 %) fueron identificadas en el gen BRCA1 y 22 (51,2 %) en el gen BRCA2. Dentro de estas variantes, se identificaron 5 mutaciones patog{\'{e}}nicas solo en el gen BRCA1, de las cuales solo una hab{\'{i}}a sido reportada previamente en Colombia.</p><p>Conclusiones: este estudio identifica variantes gen{\'{e}}ticas patog{\'{e}}nicas en el gen BRCA1 no descritas en estudios previos en la poblaci{\'{o}}n colombiana y otras conocidas en diferentes poblaciones; permitiendo de esta forma ampliar el conocimiento sobre las variantes en poblaci{\'{o}}n colombiana de los genes BRCA1 y BRCA2. Sin embargo, se requieren m{\'{a}}s estudios con suficiente poder y calidad metodol{\'{o}}gica para poder estimar la frecuencia de mutaciones y de variantes de secuencia para estos genes en mujeres colombianas con sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama u ovario hereditario.</p></p>},
author = {Arias-Blanco, Juan Felipe and Ospino-Dur{\'{a}}n, Eder Alonso and Restrepo-Fern{\'{a}}ndez, Carlos M. and Guzm{\'{a}}n-AbiSaab, Luis and Fonseca-Mendoza, Dora Janeth and {\'{A}}ngel-Guevara, Diana Isabel and Garz{\'{o}}n-Venegas, Elena Del Pilar and Gamboa-Garay, Oscar and Obreg{\'{o}}n-Tito, Alexandra J. and G{\'{o}}mez-Parrado, Yenny},
doi = {10.18597/rcog.294},
file = {:home/jennifer/Descargas/v66n4a06.pdf:pdf},
issn = {2463-0225},
journal = {Revista Colombiana de Obstetricia y Ginecolog{\'{i}}a},
number = {4},
pages = {287},
title = {{Prevalencia de mutaci{\'{o}}n y de variantes de secuencia para los genes BRCA1 y BRCA2 en una muestra de mujeres colombianas con sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama hereditario: serie de casos}},
url = {http://revista.fecolsog.org/index.php/rcog/article/view/294},
volume = {66},
year = {2015}
}
@article{Consortium2012,
abstract = {By characterizing the geographic and functional spectrum of human genetic variation, the 1000 Genomes Project aims to build a resource to help to understand the genetic contribution to disease. Here we describe the genomes of 1,092 individuals from 14 populations, constructed using a combination of low-coverage whole-genome and exome sequencing. By developing methods to integrate information across several algorithms and diverse data sources, we provide a validated haplotype map of 38 million single nucleotide polymorphisms, 1.4 million short insertions and deletions, and more than 14,000 larger deletions. We show that individuals from different populations carry different profiles of rare and common variants, and that low-frequency variants show substantial geographic differentiation, which is further increased by the action of purifying selection. We show that evolutionary conservation and coding consequence are key determinants of the strength of purifying selection, that rare-variant load varies substantially across biological pathways, and that each individual contains hundreds of rare non-coding variants at conserved sites, such as motif-disrupting changes in transcription-factor-binding sites. This resource, which captures up to 98% of accessible single nucleotide polymorphisms at a frequency of 1% in related populations, enables analysis of common and low-frequency variants in individuals from diverse, including admixed, populations.},
archivePrefix = {arXiv},
arxivId = {Sherry, S. T., Ward, M. H., Kholodov, M., Baker, J., Phan, L., Smigielski, E. M., & Sirotkin, K. (2001). dbSNP: the NCBI database of genetic variation. Nucleic acids research, 29(1), 308-11. Retrieved from http://www.pubmedcentral.nih.gov/articlerender.fc},
author = {Consortium*, The 1000 Genomes Project},
doi = {10.1038/nature11632},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fc},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Consortium - 2012 - An integrated map of genetic variation from 1,092 human genomes.pdf:pdf},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
keywords = {DNA sequencing,Disease genetics,Genetic variation},
month = {nov},
number = {7422},
pages = {56--65},
pmid = {23128226},
primaryClass = {Sherry, S. T., Ward, M. H., Kholodov, M., Baker, J., Phan, L., Smigielski, E. M., & Sirotkin, K. (2001). dbSNP: the NCBI database of genetic variation. Nucleic acids research, 29(1), 308-11. Retrieved from http:},
publisher = {Nature Publishing Group},
title = {{An integrated map of genetic variation from 1,092 human genomes}},
url = {http://www.nature.com/articles/nature11632},
volume = {491},
year = {2012}
}
@article{Compound2012,
abstract = {Filtering for Compound Heterozygous Sequence Variants in Non-Consanguineous Pedigrees The identification of disease-causing mutations in next-generation sequencing (NGS) data requires efficient filtering techniques. For patients with rare recessive diseases, compound heterozygosity of pathogenic mutations is the likeliest inheritance model if the parents are non-consanguineous.},
author = {Compound, Recessive and Filter, Heterozygous},
doi = {www.gene-talk.de},
file = {:home/jennifer/Descargas/GeneTalk_Whitepaper_CompHet.pdf:pdf},
title = {{The Compound-Heterozygous Filter The Compound-Heterozygous Filter}},
year = {2012}
}
@article{Kamphans2013,
abstract = {The identification of disease-causing mutations in next-generation sequencing (NGS) data requires efficient filtering techniques. In patients with rare recessive diseases, compound heterozygosity of pathogenic mutations is the most likely inheritance model if the parents are non-consanguineous. We developed a web-based compound heterozygous filter that is suited for data from NGS projects and that is easy to use for non-bioinformaticians. We analyzed the power of compound heterozygous mutation filtering by deriving background distributions for healthy individuals from different ethnicities and studied the effectiveness in trios as well as more complex pedigree structures. While usually more then 30 genes harbor potential compound heterozygotes in single exomes, this number can be markedly reduced with every additional member of the pedigree that is included in the analysis. In a real data set with exomes of four family members, two sisters affected by Mabry syndrome and their healthy parents, the disease-causing gene PIGO, which harbors the pathogenic compound heterozygous variants, could be readily identified. Compound heterozygous filtering is an efficient means to reduce the number of candidate mutations in studies aiming at identifying recessive disease genes in non-consanguineous families. A web-server is provided to make this filtering strategy available at www.gene-talk.de.},
author = {Kamphans, Tom and Sabri, Peggy and Zhu, Na and Heinrich, Verena and Mundlos, Stefan and Robinson, Peter N. and Parkhomchuk, Dmitri and Krawitz, Peter M.},
doi = {10.1371/journal.pone.0070151},
file = {:home/jennifer/Descargas/pone.0070151.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pages = {1--6},
pmid = {23940540},
title = {{Filtering for Compound Heterozygous Sequence Variants in Non-Consanguineous Pedigrees}},
volume = {8},
year = {2013}
}
@article{Brilliant2017,
author = {Brilliant, Muhamad},
file = {:home/jennifer/Descargas/1018-2601-1-PB.pdf:pdf},
journal = {3rd International Conferences on Information Technology and Business (ICITB)},
keywords = {association rule,data mining,fp-growth},
pages = {177--180},
title = {{Implementation of Data Mining Using Association Rules for Transactional Data Analysis}},
year = {2017}
}
@article{Santhosh2015,
author = {Santhosh, Sangeetha and Francis, Mercelin},
file = {:home/jennifer/Descargas/884662dd950e09992fcf87dfa78a5ffc2c45.pdf:pdf},
keywords = {apriori algorithm,association rule mining,clinical decision support system,clustering algorithm,electronic health record},
pages = {3585--3590},
title = {{Clinic + - A Clinical Decision Support System Using Association Rule Mining}},
year = {2015}
}
@article{Dewey2016,
abstract = {The DiscovEHR collaboration between the Regeneron Genetics Center and Geisinger Health System couples high-throughput sequencing to an integrated health care system using longitudinal electronic health records (EHRs).We sequenced the exomes of 50,726 adult participants in the DiscovEHR study to identify $\sim$4.2 million rare single-nucleotide variants and insertion/deletion events, of which $\sim$176,000 are predicted to result in a loss of gene function. Linking these data to EHR-derived clinical phenotypes, we find clinical associations supporting therapeutic targets, including genes encoding drug targets for lipid lowering, and identify previously unidentified rare alleles associated with lipid levels and other blood level traits. About 3.5% of individuals harbor deleterious variants in 76 clinically actionable genes. The DiscovEHR data set provides a blueprint for large-scale precision},
author = {Dewey, Frederick E. and Murray, Michael F. and Overton, John D. and Habegger, Lukas and Leader, Joseph B. and Fetterolf, Samantha N. and O'Dushlaine, Colm and {Van Hout}, Cristopher V. and Staples, Jeffrey and Gonzaga-Jauregui, Claudia and Metpally, Raghu and Pendergrass, Sarah A. and Giovanni, Monica A. and Kirchner, H. Lester and Balasubramanian, Suganthi and Abul-Husn, Noura S. and Hartzel, Dustin N. and Lavage, Daniel R. and Kost, Korey A. and Packer, Jonathan S. and Lopez, Alexander E. and Penn, John and Mukherjee, Semanti and Gosalia, Nehal and Kanagaraj, Manoj and Li, Alexander H. and Mitnaul, Lyndon J. and Adams, Lance J. and Person, Thomas N. and Praveen, Kavita and Marcketta, Anthony and Lebo, Matthew S. and Austin-Tse, Christina A. and Mason-Suares, Heather M. and Bruse, Shannon and Mellis, Scott and Phillips, Robert and Stahl, Neil and Murphy, Andrew and Economides, Aris and Skelding, Kimberly A. and Still, Christopher D. and Elmore, James R. and Borecki, Ingrid B. and Yancopoulos, George D. and Davis, F. Daniel and Faucett, William A. and Gottesman, Omri and Ritchie, Marylyn D. and Shuldiner, Alan R. and Reid, Jeffrey G. and Ledbetter, David H. and Baras, Aris and Carey, David J.},
doi = {10.1126/science.aaf6814},
file = {:home/jennifer/Descargas/dewey2016.pdf:pdf},
isbn = {10.1126/science.aaf6814},
issn = {10959203},
journal = {Science},
number = {6319},
pmid = {28008009},
title = {{Distribution and clinical impact of functional variants in 50,726 whole-exome sequences from the DiscovEHR study}},
volume = {354},
year = {2016}
}
@article{McCarthy2014,
abstract = {BACKGROUND Variant annotation is a crucial step in the analysis of genome sequencing data. Functional annotation results can have a strong influence on the ultimate conclusions of disease studies. Incorrect or incomplete annotations can cause researchers both to overlook potentially disease-relevant DNA variants and to dilute interesting variants in a pool of false positives. Researchers are aware of these issues in general, but the extent of the dependency of final results on the choice of transcripts and software used for annotation has not been quantified in detail. METHODS This paper quantifies the extent of differences in annotation of 80 million variants from a whole-genome sequencing study. We compare results using the RefSeq and Ensembl transcript sets as the basis for variant annotation with the software Annovar, and also compare the results from two annotation software packages, Annovar and VEP (Ensembl's Variant Effect Predictor), when using Ensembl transcripts. RESULTS We found only 44% agreement in annotations for putative loss-of-function variants when using the RefSeq and Ensembl transcript sets as the basis for annotation with Annovar. The rate of matching annotations for loss-of-function and nonsynonymous variants combined was 79% and for all exonic variants it was 83%. When comparing results from Annovar and VEP using Ensembl transcripts, matching annotations were seen for only 65% of loss-of-function variants and 87% of all exonic variants, with splicing variants revealed as the category with the greatest discrepancy. Using these comparisons, we characterised the types of apparent errors made by Annovar and VEP and discuss their impact on the analysis of DNA variants in genome sequencing studies. CONCLUSIONS Variant annotation is not yet a solved problem. Choice of transcript set can have a large effect on the ultimate variant annotations obtained in a whole-genome sequencing study. Choice of annotation software can also have a substantial effect. The annotation step in the analysis of a genome sequencing study must therefore be considered carefully, and a conscious choice made as to which transcript set and software are used for annotation.},
author = {McCarthy, Davis J. and Humburg, Peter and Kanapin, Alexander and Rivas, Manuel A. and Gaulton, Kyle and Cazier, Jean Baptiste and Donnelly, Peter},
doi = {10.1186/gm543},
file = {:home/jennifer/Descargas/gm543:},
isbn = {1756994X (Linking)},
issn = {1756994X},
journal = {Genome Medicine},
number = {3},
pmid = {24944579},
title = {{Choice of transcripts and software has a large effect on variant annotation}},
volume = {6},
year = {2014}
}
@article{Cassa2017,
abstract = {Shamil Sunyaev, David Beier and colleagues report an analysis of the fitness effects of heterozygous protein-truncating variants from the Exome Aggregation Consortium. They find that high heterozygous selection coefficients are enriched in Mendelian disease-associated genes and essential mouse genes, suggesting that this coefficient can be used to prioritize candidate disease-associated genes from clinical exome-sequencing data.},
author = {Cassa, Christopher A. and Weghorn, Donate and Balick, Daniel J. and Jordan, Daniel M. and Nusinow, David and Samocha, Kaitlin E. and O'Donnell-Luria, Anne and MacArthur, Daniel G. and Daly, Mark J. and Beier, David R. and Sunyaev, Shamil R.},
doi = {10.1038/ng.3831},
file = {:home/jennifer/Descargas/cassa2017.pdf:pdf},
issn = {15461718},
journal = {Nature Genetics},
number = {5},
pages = {806--810},
pmid = {28369035},
publisher = {Nature Publishing Group},
title = {{Estimating the selective effects of heterozygous protein-truncating variants from human exome data}},
url = {http://dx.doi.org/10.1038/ng.3831},
volume = {49},
year = {2017}
}
@article{Fu2013,
abstract = {Establishing the age of each mutation segregating in contemporary human populations is important to fully understand our evolutionary history and will help to facilitate the development of new approaches for disease-gene discovery. Large-scale surveys of human genetic variation have reported signatures of recent explosive population growth, notable for an excess of rare genetic variants, suggesting that many mutations arose recently. To more quantitatively assess the distribution of mutation ages, we resequenced 15,336 genes in 6,515 individuals of European American and African American ancestry and inferred the age of 1,146,401 autosomal single nucleotide variants (SNVs). We estimate that approximately 73% of all protein-coding SNVs and approximately 86% of SNVs predicted to be deleterious arose in the past 5,000-10,000 years. The average age of deleterious SNVs varied significantly across molecular pathways, and disease genes contained a significantly higher proportion of recently arisen deleterious SNVs than other genes. Furthermore, European Americans had an excess of deleterious variants in essential and Mendelian disease genes compared to African Americans, consistent with weaker purifying selection due to the Out-of-Africa dispersal. Our results better delimit the historical details of human protein-coding variation, show the profound effect of recent human history on the burden of deleterious SNVs segregating in contemporary populations, and provide important practical information that can be used to prioritize variants in disease-gene discovery.},
author = {Fu, Wenqing and O'Connor, Timothy D. and Jun, Goo and Kang, Hyun Min and Abecasis, Goncalo and Leal, Suzanne M. and Gabriel, Stacey and Altshuler, David and Shendure, Jay and Nickerson, Deborah A. and Bamshad, Michael J. and Akey, Joshua M.},
doi = {10.1038/nature11690},
file = {:home/jennifer/Descargas/fu2012.pdf:pdf},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {7431},
pages = {216--220},
pmid = {23201682},
title = {{Analysis of 6,515 exomes reveals the recent origin of most human protein-coding variants}},
volume = {493},
year = {2013}
}
@article{Holst-Hansen2017,
abstract = {Allele number, or zygosity, is a clear determinant of gene expression in diploid cells. However, the relationship between the number of copies of a gene and its expression can be hard to anticipate, especially when the gene in question is embedded in a regulatory circuit that contains feedback. Here, we study this question making use of the natural genetic variability of human populations, which allows us to compare the expression profiles of a receptor protein in natural killer cells among donors infected with human cytomegalovirus with one or two copies of the allele. Crucially, the distribution of gene expression in many of the donors is bimodal, which indicates the presence of a positive feedback loop somewhere in the regulatory environment of the gene. Three separate gene-circuit models differing in the location of the positive feedback loop with respect to the gene can all reproduce the homozygous data. However, when the resulting fitted models are applied to the hemizygous donors, one model (the one with the positive feedback located at the level of gene transcription) is superior in describing the experimentally observed gene-expression profile. In that way, our work shows that zygosity can help us relate the structure and function of gene regulatory networks.},
author = {Holst-Hansen, Thomas and Abad, Elena and Muntasell, Aura and L{\'{o}}pez-Botet, Miguel and Jensen, Mogens H. and Trusina, Ala and Garcia-Ojalvo, Jordi},
doi = {10.1016/j.bpj.2017.05.010},
file = {:home/jennifer/Descargas/holsthansen2017.pdf:pdf},
issn = {15420086},
journal = {Biophysical Journal},
number = {1},
pages = {148--156},
pmid = {28700913},
title = {{Impact of Zygosity on Bimodal Phenotype Distributions}},
volume = {113},
year = {2017}
}
@article{Balasubramanian2017,
abstract = {Variants predicted to result in the loss of function of human genes have attracted interest because of their clinical impact and surprising prevalence in healthy individuals. Here, we present ALoFT (annotation of loss-of-function transcripts), a method to annotate and predict the disease-causing potential of loss-of-function variants. Using data from Mendelian disease-gene discovery projects, we show that ALoFT can distinguish between loss-of-function variants that are deleterious as heterozygotes and those causing disease only in the homozygous state. Investigation of variants discovered in healthy populations suggests that each individual carries at least two heterozygous premature stop alleles that could potentially lead to disease if present as homozygotes. When applied to de novo putative loss-of-function variants in autism-affected families, ALoFT distinguishes between deleterious variants in patients and benign variants in unaffected siblings. Finally, analysis of somatic variants in >6500 cancer exomes shows that putative loss-of-function variants predicted to be deleterious by ALoFT are enriched in known driver genes.},
author = {Balasubramanian, Suganthi and Fu, Yao and Pawashe, Mayur and McGillivray, Patrick and Jin, Mike and Liu, Jeremy and Karczewski, Konrad J. and MacArthur, Daniel G. and Gerstein, Mark},
doi = {10.1038/s41467-017-00443-5},
file = {:home/jennifer/Descargas/s41467-017-00443-5.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pmid = {28851873},
publisher = {Springer US},
title = {{Using ALoFT to determine the impact of putative loss-of-function variants in protein-coding genes}},
url = {http://dx.doi.org/10.1038/s41467-017-00443-5},
volume = {8},
year = {2017}
}
@article{Laboratories2015,
author = {Laboratories, Knight Diagnostic and Genetics, Medical and Health, Oregon and Road, Plank and Molecular, Clinical and Children, Nationwide and State, Ohio and Berindan-neagoe, Ioana and Monroig, Paloma and Pasculli, Barbara and George, A and Medicine, Translational and Hatieganu, Pharmacy Iuliu and Juan, San and Rico, Puerto and Sciences, Pharmacological},
doi = {10.1038/gim.2015.30.Standards},
file = {:home/jennifer/Descargas/nihms697486.pdf:pdf},
journal = {CA Cancer J Clin},
number = {5},
pages = {405--424},
title = {{HHS Public Access}},
volume = {17},
year = {2015}
}
@article{Dave2017,
abstract = {The era of huge data is snowballing at frequent swiftness in size (volume) and in different formats (variety). This data which comes from various sources e.g. media, communication devices, internet, business etc. and there are many difficulties and challenges that one faces while handling it. Data mining is a process intended to reconnoiter analytical data (typically business or market associated data - also acknowledged as "Big data"). There are several data mining techniques such as outlier analysis, organization, clustering, prediction and association rule mining. In this paper we have discussed several applications and the importance of clustering. To examine the huge volume of data, clustering algorithms aid in providing a powerful meta-Iearning tool. Numerous clustering techniques (including traditional and the recently developed) in reference to large data sets with their pros & cons are being discussed in this paper.},
author = {Dave, Meenu and Gianey, Hemant},
doi = {10.1109/SYSMART.2016.7894544},
file = {:home/jennifer/Descargas/dave2016.pdf:pdf},
isbn = {9781509035434},
journal = {Proceedings of the 5th International Conference on System Modeling and Advancement in Research Trends, SMART 2016},
keywords = {Clustering,Data Mining (DM),Density Based Methods (DBM),Grid Based Methods (GBM),Hierarchical Methods (HM),Partition Methods (PM)},
pages = {328--333},
title = {{Different clustering algorithms for Big Data analytics: A review}},
year = {2017}
}
@article{Sherry2001,
abstract = {In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.},
archivePrefix = {arXiv},
arxivId = {Sherry, S. T., Ward, M. H., Kholodov, M., Baker, J., Phan, L., Smigielski, E. M., & Sirotkin, K. (2001). dbSNP: the NCBI database of genetic variation. Nucleic acids research, 29(1), 308-11. Retrieved from http://www.pubmedcentral.nih.gov/articlerender.fc},
author = {Sherry, S. T.},
doi = {10.1093/nar/29.1.308},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fc},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sherry et al. - 2001 - dbSNP the NCBI database of genetic variation.pdf:pdf},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Research},
keywords = {biotechnology,chromosome mapping,genbank,genetics,genome,genome, human,national library of medicine (u.s.),patients' rooms,single nucleotide polymorphism,united states national institutes of health},
month = {jan},
number = {1},
pages = {308--311},
pmid = {11125122},
primaryClass = {Sherry, S. T., Ward, M. H., Kholodov, M., Baker, J., Phan, L., Smigielski, E. M., & Sirotkin, K. (2001). dbSNP: the NCBI database of genetic variation. Nucleic acids research, 29(1), 308-11. Retrieved from http:},
publisher = {Oxford University Press},
title = {{dbSNP: the NCBI database of genetic variation}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/29.1.308},
volume = {29},
year = {2001}
}
@article{Triplet2014,
abstract = {To facilitate the integration and querying of genomics data, a number of generic data warehousing frameworks have been developed. They differ in their design and capabilities, as well as their intended audience. We provide a comprehensive and quantitative review of those genomic data warehousing frameworks in the context of large-scale systems biology. We reviewed in detail four genomic data warehouses (BioMart, BioXRT, InterMine and PathwayTools) freely available to the academic community. We quantified 20 aspects of the warehouses, covering the accuracy of their responses, their computational requirements and development efforts. Performance of the warehouses was evaluated under various hardware configurations to help laboratories optimize hardware expenses. Each aspect of the benchmark may be dynamically weighted by scientists using our online tool BenchDW (http://warehousebenchmark.fungalgenomics.ca/benchmark/) to build custom warehouse profiles and tailor our results to their specific needs.},
author = {Triplet, Thomas and Butler, Gregory},
doi = {10.1093/bib/bbt031},
isbn = {1467546314774054},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Benchmark,Data integration,Data warehousing,Large-scale genomics,Systems biology},
month = {jul},
number = {4},
pages = {471--483},
pmid = {23673292},
title = {{A review of genomic data warehousing systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23673292 https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbt031},
volume = {15},
year = {2014}
}
@article{Canuel2015,
abstract = {The rise of personalized medicine and the availability of high-throughput molecular analyses in the context of clinical care have increased the need for adequate tools for translational researchers to manage and explore these data. We reviewed the biomedical literature for translational platforms allowing the management and exploration of clinical and omics data, and identified seven platforms: BRISK, caTRIP, cBio Cancer Portal, G-DOC, iCOD, iDASH and tranSMART. We analyzed these platforms along seven major axes. (1) The community axis regrouped information regarding initiators and funders of the project, as well as availability status and references. (2) We regrouped under the information content axis the nature of the clinical and omics data handled by each system. (3) The privacy management environment axis encompassed functionalities allowing control over data privacy. (4) In the analysis support axis, we detailed the analytical and statistical tools provided by the platforms. We also explored (5) interoperability support and (6) system requirements. The final axis (7) platform support listed the availability of documentation and installation procedures. A large heterogeneity was observed in regard to the capability to manage phenotype information in addition to omics data, their security and interoperability features. The analytical and visualization features strongly depend on the considered platform. Similarly, the availability of the systems is variable. This review aims at providing the reader with the background to choose the platform best suited to their needs. To conclude, we discuss the desiderata for optimal translational research platforms, in terms of privacy, interoperability and technical features.},
author = {Canuel, Vincent and Rance, Bastien and Avillach, Paul and Degoulet, Patrice and Burgun, Anita},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data A review of publicly available solutions.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical research,Clinical data,High-throughput technologies,Information storage and retrieval,Translationalmedical research},
number = {2},
pages = {280--290},
pmid = {24608524},
title = {{Translational research platforms integrating clinical and omics data: A review of publicly available solutions}},
volume = {16},
year = {2015}
}
@article{Wu2017b,
abstract = {Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
isbn = {0018-9294 VO  - 64},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {263--273},
pmid = {28113246},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
url = {http://ieeexplore.ieee.org/document/7587347/},
volume = {64},
year = {2017}
}
@book{Andrews2010a,
abstract = {FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. The main functions of FastQC are Import of data from BAM, SAM or FastQ files (any variant) Providing a quick overview to tell you in which areas there may be problems Summary graphs and tables to quickly assess your data Export of results to an HTML based permanent report Offline operation to allow automated generation of reports without running the interactive application},
author = {Andrews, Simon},
booktitle = {Babraham Bioinformatics},
doi = {citeulike-article-id:11583827},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - FastQC Manual.pdf:pdf},
pages = {http://www.bioinformatics.babraham.ac.uk/projects/},
title = {{FastQC: A quality control tool for high throughput sequence data.}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:FastQC+a+quality+control+tool+for+high+throughput+sequence+data.#0},
year = {2010}
}
@article{Poliakov2015,
author = {Poliakov, Eugenia and Cooper, David N. and Stepchenkova, Elena I. and Rogozin, Igor B.},
doi = {10.1155/2015/364960},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Poliakov et al. - 2015 - Genetics in genomic era.pdf:pdf},
isbn = {2090-3154 (Print)\r2090-3162 (Linking)},
issn = {2090-3154},
journal = {Genetics Research International},
pages = {1--2},
pmid = {25883807},
publisher = {Hindawi},
title = {{Genetics in Genomic Era}},
url = {http://www.hindawi.com/journals/gri/2015/364960/},
volume = {2015},
year = {2015}
}
@article{La2012,
abstract = {Ejercicio de citas en Mendeley},
author = {Notario, Silvia},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
journal = {Seminario de tesis 1},
pages = {1--2},
title = {{Ejercicio de citas con Mendeley}},
year = {2016}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F. and Dolezal, Marlies A. and Koltes, James E. and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J. and Reecy, James M. and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
isbn = {1471-2164},
issn = {14712164},
journal = {BMC Genomics},
keywords = {Next-generation sequencing analysis,Pipeline,Single nucleotide variant identification},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.\n\nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.\n\nDATA SOURCES: Literature review and authors' experience.\n\nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M. and Durtschi, Jacob D. and Margraf, Rebecca L. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)\r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Li2014,
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {22103244},
journal = {Genomics, Proteomics and Bioinformatics},
month = {oct},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Elsevier},
title = {{Big biological data: Challenges and opportunities}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25462151 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4411415},
volume = {12},
year = {2014}
}
@article{Parla,
abstract = {Human exome resequencing using commercial target capture kits has been and is being used for sequencing large numbers of individuals to search for variants associated with various human diseases. We rigorously evaluated the capabilities of two solution exome capture kits. These analyses help clarify the strengths and limitations of those data as well as systematically identify variables that should be considered in the use of those data.},
author = {Parla, Jennifer S and Iossifov, Ivan and Grabill, Ian and Spector, Mona S and Kramer, Melissa and McCombie, W Richard},
doi = {10.1186/gb-2011-12-9-r97},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parla et al. - Unknown - A comparative analysis of exome capture.pdf:pdf},
isbn = {1465-6914 (Electronic)\r1465-6906 (Linking)},
issn = {1465-6906},
journal = {Genome Biology},
number = {9},
pages = {R97},
pmid = {21958622},
title = {{A comparative analysis of exome capture}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/gb-2011-12-9-r97},
volume = {12},
year = {2011}
}
@article{Cleary2014,
abstract = {Abstract The analysis of whole-genome or exome sequencing data from trios and pedigrees has been successfully applied to the identification of disease-causing mutations. However, most methods used to identify and genotype genetic variants from next-generation sequencing data ignore the relationships between samples, resulting in significant Mendelian errors, false positives and negatives. Here we present a Bayesian network framework that jointly analyzes data from all members of a pedigree simultaneously using Mendelian segregation priors, yet providing the ability to detect de novo mutations in offspring, and is scalable to large pedigrees. We evaluated our method by simulations and analysis of whole-genome sequencing (WGS) data from a 17-individual, 3-generation CEPH pedigree sequenced to 50× average depth. Compared with singleton calling, our family caller produced more high-quality variants and eliminated spurious calls as judged by common quality metrics such as Ti/Tv, Het/Hom ratios, and dbSNP/SNP array data concordance, and by comparing to ground truth variant sets available for this sample. We identify all previously validated de novo mutations in NA12878, concurrent with a 7× precision improvement. Our results show that our method is scalable to large genomics and human disease studies.},
author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M.},
doi = {10.1089/cmb.2014.0029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleary et al. - 2014 - Joint Variant and iDe Novoi Mutation Identification on Pedigrees from High-Throughput Sequencing Data.pdf:pdf},
isbn = {1557-8666},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
pmid = {24874280},
title = {{Joint Variant and <i>De Novo</i> Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Reviews3.csv:csv},
title = {{Reviews3}}
}
@article{Wenger2017,
abstract = {Manual review of aligned reads for confirmation and interpretation of variant calls is an important step in many variant calling pipelines for next-generation sequencing (NGS) data. Visual inspection can greatly increase the confidence in calls, reduce the risk of false positives, and help characterize complex events. The Integrative Genomics Viewer (IGV) was one of the first tools to provide NGS data visualization, and it currently provides a rich set of tools for inspection, validation, and interpretation of NGS datasets, as well as other types of genomic data. Here, we present a short overview of IGV's variant review features for both single-nucleotide variants and structural variants, with examples from both cancer and germline datasets. IGV is freely available at https://www.igv.orgCancer Res; 77(21); e31-34. {\textcopyright}2017 AACR.},
author = {Robinson, James T. and Thorvaldsd{\'{o}}ttir, Helga and Wenger, Aaron M. and Zehir, Ahmet and Mesirov, Jill P.},
doi = {10.1158/0008-5472.CAN-17-0337},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wenger et al. - 2017 - Variant Review with the Integrative Genomics Viewer.pdf:pdf},
issn = {15387445},
journal = {Cancer Research},
number = {21},
pages = {e31--e34},
pmid = {29092934},
title = {{Variant review with the integrative genomics viewer}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {77},
year = {2017}
}
@article{Lubin2017,
abstract = {A national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications. This flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants. To provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settings, use of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard to the existing variant file specifications presently used in the clinical setting. Adoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities.},
author = {Lubin, Ira M. and Aziz, Nazneen and Babb, Lawrence J. and Ballinger, Dennis and Bisht, Himani and Church, Deanna M. and Cordes, Shaun and Eilbeck, Karen and Hyland, Fiona and Kalman, Lisa and Landrum, Melissa and Lockhart, Edward R. and Maglott, Donna and Marth, Gabor and Pfeifer, John D. and Rehm, Heidi L. and Roy, Somak and Tezak, Zivana and Truty, Rebecca and Ullman-Cullere, Mollie and Voelkerding, Karl V. and Worthey, Elizabeth A. and Zaranek, Alexander W. and Zook, Justin M.},
doi = {10.1016/j.jmoldx.2016.12.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lubin et al. - 2017 - Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinica.pdf:pdf},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {3},
pages = {417--426},
pmid = {28315672},
publisher = {Elsevier Inc},
title = {{Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinical Settings}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.12.001},
volume = {19},
year = {2017}
}
@article{Fong2017,
abstract = {The availability of whole-exome sequencing has revolutionized the study of genetic disease in recent years, particularly in dermatology, where clinical phenotypes are readily recognized. As this technology becomes increasingly affordable and accessible, questions are emerging regarding the clinical and ethical responsibilities of physicians who determine variants underlying disease, especially with regard to children, for whom treatment may be warranted and clinical course improved based on a known genotype. These responsibilities are accentuated in the developing countries, which harbor most consanguineous populations and thus bear the brunt of monogenic genodermatoses. Although many genetic disorders are identified in these populations, limited educational and clinical infrastructure rarely offers opportunities to improve the course of disease. Here we report a genetic study that illustrates these challenges.},
author = {Fong, Kenneth and Bailey, Celeste V. and Tuttle, Peggy and Cunningham, Bari and McGrath, John A. and Cho, Raymond J.},
doi = {10.1111/pde.13029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong et al. - 2017 - Questioning the Clinical Utility of Exome Sequencing in Developing Countries.pdf:pdf},
issn = {15251470},
journal = {Pediatric Dermatology},
month = {jan},
number = {1},
pages = {e32--e34},
pmid = {27874213},
title = {{Questioning the Clinical Utility of Exome Sequencing in Developing Countries}},
url = {http://doi.wiley.com/10.1111/pde.13029},
volume = {34},
year = {2017}
}
@article{Salter2017,
abstract = {The rise of bioinformatics is a direct response to the political difficulties faced by genomics in its quest to be a new biomedical innovation, and the value of bioinformatics lies in its role as the bridge between the promise of genomics and its realization in the form of health benefits. Western scientific elites are able to use their close relationship with the state to control and facilitate the emergence of new domains compatible with the existing distribution of epistemic power – all within the embrace of public trust. The incorporation of bioinformatics as the saviour of genomics had to be integrated with the operation of two key aspects of governance in this field: the definition and ownership of the new knowledge. This was achieved mainly by the development of common standards and by the promotion of the values of communality, open access and the public ownership of data to legitimize and maintain the governance power of publicly funded genomic science. Opposition from industry advocating the private ownership of knowledge has been largely neutered through the institutions supporting the science-state concordat. However, in order for translation into health benefits to occur and public trust to be assured, genomic and clinical data have to be integrated and knowledge ownership agreed upon across the separate and distinct governance territories of scientist, clinical medicine and society. Tensions abound as science seeks ways of maintaining its control of knowledge production through the negotiation of new forms of governance with the institutions and values of clinicians and patients.},
author = {Salter, Brian and Salter, Charlotte},
doi = {10.1177/0306312716681210},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salter, Salter - 2017 - Controlling new knowledge Genomic science, governance and the politics of bioinformatics.pdf:pdf},
issn = {14603659},
journal = {Social Studies of Science},
keywords = {bioinformatics,genomics,governance,ideology,politics},
month = {apr},
number = {2},
pages = {263--287},
pmid = {28056721},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{Controlling new knowledge: Genomic science, governance and the politics of bioinformatics}},
url = {http://journals.sagepub.com/doi/10.1177/0306312716681210},
volume = {47},
year = {2017}
}
@article{Morimoto2016,
abstract = {We developed a new approach for pairwise kinship analysis in forensic genetics based on chromosomal sharing between two individuals. Here, we defined "index of chromosome sharing" (ICS) calculated using 174,254 single nucleotide polymorphism (SNP) loci typed by SNP microarray and genetic length of the shared segments from the genotypes of two individuals. To investigate the expected ICS distributions from first- to fifth-degree relatives and unrelated pairs, we used computationally generated genotypes to consider the effect of linkage disequilibrium and recombination. The distributions were used for probabilistic evaluation of the pairwise kinship analysis, such as likelihood ratio (LR) or posterior probability, without allele frequencies and haplotype frequencies. Using our method, all actual sample pairs from volunteers showed significantly high LR values (i.e., ≥ 108); therefore, we can distinguish distant relationships (up to the fifth-degree) from unrelated pairs based on LR. Moreover, we can determine accurate degrees of kinship in up to third-degree relationships with a probability of > 80% using the criterion of posterior probability ≥ 0.90, even if the kinship of the pair is totally unpredictable. This approach greatly improves pairwise kinship analysis of distant relationships, specifically in cases involving identification of disaster victims or missing persons.},
author = {Morimoto, Chie and Manabe, Sho and Kawaguchi, Takahisa and Kawai, Chihiro and Fujimoto, Shuntaro and Hamano, Yuya and Yamada, Ryo and Matsuda, Fumihiko and Tamaki, Keiji},
doi = {10.1371/journal.pone.0160287},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morimoto et al. - 2016 - Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.pdf:pdf},
issn = {19326203},
journal = {PloS one},
number = {7},
pages = {e0160287},
pmid = {27472558},
title = {{Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms}},
url = {http://dx.plos.org/10.1371/journal.pone.0160287%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27472558%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4966930},
volume = {11},
year = {2016}
}
@misc{,
annote = {NULL},
pages = {http://www.infobae.com/america/america--latina/2016},
title = {{Colombia se propuso identificar a 16.000 v{\'{i}}ctimas del conflicto armado con las FARC}},
url = {http://www.infobae.com/america/america-latina/2016},
urldate = {2016-11-20}
}
@article{scikit-learn,
author = {Pedregosa, F and Varoquax, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O},
issn = {ISSN 1533-7928},
journal = {J Mach Learn Res},
pages = {2825--2830},
title = {{Scikit-learn: machine learning in Python}},
volume = {12},
year = {2011}
}
@article{Wu2014,
abstract = {Exome sequencing has been widely used in detecting pathogenic nonsynonymous single nucleotide variants (SNVs) for human inherited diseases. However, traditional statistical genetics methods are ineffective in analyzing exome sequencing data, due to such facts as the large number of sequenced variants, the presence of non-negligible fraction of pathogenic rare variants or de novo mutations, and the limited size of affected and normal populations. Indeed, prevalent applications of exome sequencing have been appealing for an effective computational method for identifying causative nonsynonymous SNVs from a large number of sequenced variants. Here, we propose a bioinformatics approach called SPRING (Snv PRioritization via the INtegration of Genomic data) for identifying pathogenic nonsynonymous SNVs for a given query disease. Based on six functional effect scores calculated by existing methods (SIFT, PolyPhen2, LRT, MutationTaster, GERP and PhyloP) and five association scores derived from a variety of genomic data sources (gene ontology, protein-protein interactions, protein sequences, protein domain annotations and gene pathway annotations), SPRING calculates the statistical significance that an SNV is causative for a query disease and hence provides a means of prioritizing candidate SNVs. With a series of comprehensive validation experiments, we demonstrate that SPRING is valid for diseases whose genetic bases are either partly known or completely unknown and effective for diseases with a variety of inheritance styles. In applications of our method to real exome sequencing data sets, we show the capability of SPRING in detecting causative de novo mutations for autism, epileptic encephalopathies and intellectual disability. We further provide an online service, the standalone software and genome-wide predictions of causative SNVs for 5,080 diseases at http://bioinfo.au.tsinghua.edu.cn/spring.},
author = {Wu, Jiaxin and Li, Yanda and Jiang, Rui},
doi = {10.1371/journal.pgen.1004237},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Li, Jiang - 2014 - Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Se.pdf:pdf},
isbn = {1553-7404 (Electronic)\r1553-7390 (Linking)},
issn = {15537404},
journal = {PLoS Genetics},
number = {3},
pmid = {24651380},
title = {{Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Sequencing Studies}},
volume = {10},
year = {2014}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Merelli2014,
abstract = {The explosion of the data both in the biomedical research and in the healthcare systems demands urgent solutions. In particular, the research in omics sciences is moving from a hypothesis-driven to a data-driven approach. Healthcare is additionally always asking for a tighter integration with biomedical data in order to promote personalized medicine and to provide better treatments. Efficient analysis and interpretation of Big Data opens new avenues to explore molecular biology, new questions to ask about physiological and pathological states, and new ways to answer these open issues. Such analyses lead to better understanding of diseases and development of better and personalized diagnostics and therapeutics. However, such progresses are directly related to the availability of new solutions to deal with this huge amount of information. New paradigms are needed to store and access data, for its annotation and integration and finally for inferring knowledge and making it available to researchers. Bioinformatics can be viewed as the "glue" for all these processes. A clear awareness of present high performance computing (HPC) solutions in bioinformatics, Big Data analysis paradigms for computational biology, and the issues that are still open in the biomedical and healthcare fields represent the starting point to win this challenge.},
author = {Merelli, Ivan and P{\'{e}}rez-S{\'{a}}nchez, Horacio and Gesing, Sandra and D'Agostino, Daniele},
doi = {10.1155/2014/134023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Merelli et al. - 2014 - Managing, analysing, and integrating big data in medical bioinformatics open problems and future perspectives.pdf:pdf},
isbn = {23146141 (Electronic)},
issn = {23146141},
journal = {BioMed Research International},
month = {sep},
pages = {134023},
pmid = {25254202},
publisher = {Hindawi},
title = {{Managing, Analysing, and Integrating Big Data in Medical Bioinformatics: Open Problems and Future Perspectives}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25254202 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4165507},
volume = {2014},
year = {2014}
}
@article{Fisch2015a,
abstract = {Motivation: Omics Pipe (http://sulab.scripps.edu/omicspipe) is a computational framework that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole-Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas (TCGA). Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis. The goal of Omics Pipe is to democratize next-generation sequencing analysis by dramatically increasing the accessibility and reproducibility of best practice computational pipelines, which will enable researchers to generate biologically meaningful and interpretable results.\nResults: Using Omics Pipe, we analyzed 100 TCGA breast invasive carcinoma paired tumor-normal datasets based on the latest UCSC hg19 RefSeq annotation. Omics Pipe automatically downloaded and processed the desired TCGA samples on a high throughput compute cluster to produce a results report for each sample. We aggregated the individual sample results and compared them to the analysis in the original publications. This comparison revealed high overlap between the analyses, as well as novel findings due to the use of updated annotations and methods.\nAvailability and implementation: Source code for Omics Pipe is freely available on the web (https://bitbucket.org/sulab/omics_pipe). Omics Pipe is distributed as a standalone Python package for installation (https://pypi.python.org/pypi/omics_pipe) and as an Amazon Machine Image in Amazon Web Services Elastic Compute Cloud that contains all necessary third-party software dependencies and databases (https://pythonhosted.org/omics_pipe/AWS_installation.html).\nContact: asu@scripps.edu or kfisch@ucsd.edu\nSupplementary Information: Supplementary data are available at Bioinformatics online.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Seren2016,
abstract = {Natural genetic variation makes it possible to dis - cover evolutionary changes that have been main - tained in a population because they are advanta - geous . To understand genotype – phenotype relation - ships and to investigate trait architecture , the ex - istence of both high - resolution genotypic and phe - notypic data is necessary . Arabidopsis thaliana is a prime model for these purposes . This herb naturally occurs across much of the Eurasian continent and North America . Thus , it is exposed to a wide range of environmental factors and has been subject to natu - ral selection under distinct conditions . Full genome sequencing data for more than 1000 different natural inbred lines are available , and this has encouraged the distributed generation of many types of pheno - typic data . To leverage these data for meta analyses , AraPheno (https : / / arapheno . 1001genomes . org) pro - vide a central repository of population - scale pheno - types for A . thaliana inbred lines . AraPheno includes various features to easily access , download and vi - sualize the phenotypic data . This will facilitate a com - parative analysis of the many different types of phe - notypic data , which is the base to further enhance our understanding of the genotype – phenotype map .},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4788},
author = {Seren, {\"{U}}mit and Grimm, Dominik and Fitz, Joffrey and Weigel, Detlef and Nordborg, Magnus and Borgwardt, Karsten and Korte, Arthur},
doi = {10.1093/nar/gkw986},
eprint = {arXiv:1212.4788},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seren et al. - 2016 - AraPheno a public database for Arabidopsis thaliana phenotypes.pdf:pdf},
isbn = {0305-1048},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D1054--D1059},
pmid = {27924043},
title = {{AraPheno: A public database for Arabidopsis thaliana phenotypes}},
url = {http://nar.oxfordjournals.org/lookup/doi/10.1093/nar/gkw986},
volume = {45},
year = {2017}
}
@misc{Paez2012,
abstract = {El contexto din{\'{a}}mico y competitivo de la organizaci{\'{o}}n actual exige permanentes soluciones inform{\'{a}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\'{o}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\'{o}}n mayormente, desde el punto de vista tecnol{\'{o}}gico, sin considerar los aspectos organizacionales y metodol{\'{o}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{a}}ez - 2012 - Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio.pdf:pdf},
issn = {0120-341X},
keywords = {Sistemas de informaci{\'{o}}n en administraci{\'{o}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Li2017,
abstract = {In 2015, the American College of Medical Genetics and Genomics (ACMG) and the Association for Molecular Pathology (AMP) published updated standards and guidelines for the clinical interpretation of sequence variants with respect to human diseases on the basis of 28 criteria. However, variability between individual interpreters can be extensive because of reasons such as the different understandings of these guidelines and the lack of standard algorithms for implementing them, yet computational tools for semi-automated variant interpretation are not available. To address these problems, we propose a suite of methods for implementing these criteria and have developed a tool called InterVar to help human reviewers interpret the clinical significance of variants. InterVar can take a pre-annotated or VCF file as input and generate automated interpretation on 18 criteria. Furthermore, we have developed a companion web server, wInterVar, to enable user-friendly variant interpretation with an automated interpretation step and a manual adjustment step. These tools are especially useful for addressing severe congenital or very early-onset developmental disorders with high penetrance. Using results from a few published sequencing studies, we demonstrate the utility of InterVar in significantly reducing the time to interpret the clinical significance of sequence variants.},
author = {Li, Quan and Wang, Kai},
doi = {10.1016/j.ajhg.2017.01.004},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Wang - 2017 - InterVar Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines.pdf:pdf},
isbn = {0002-9297},
issn = {15376605},
journal = {American Journal of Human Genetics},
keywords = {ACMG,ANNOVAR,ClinVar,InterVar,clinical interpretation,genetic diagnosis,variant annotation,variant interpretation},
number = {2},
pages = {267--280},
pmid = {28132688},
publisher = {ElsevierCompany.},
title = {{InterVar: Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines}},
url = {http://dx.doi.org/10.1016/j.ajhg.2017.01.004},
volume = {100},
year = {2017}
}
@article{McKenna2009,
abstract = {The concept of absorptive capacity was introduced by Cohen and Levinthal in 1989. Since then it has been enhanced through reconceptualizations and extended by various empirical studies. Despite the growing interest in absorptive capacity it is unclear what this large stream of papers has collectively accomplished. The used definitions, antecedents, components and outcomes of the construct are extremely heterogeneous. Due to this heterogeneity, the empirical study of the construct remains difficult. There is no standard measure and no standard method of measurement, which can be used in empirical research. To bring more clarity into this research area, this paper provides a critical review of previous empirical treatments of absorptive capacity. For this purpose, different methods of measurement are classified in the following way: within quantitative methods proxy indicators and perceptive instruments are differentiated. Proxy indicators use single firm-level data for measuring absorptive capacity and can be input-oriented (R&D efforts, R&D human capital) or output-oriented (R&D patents, R&D publications). Perceptive instruments imply that researchers develop single questions or a set of questions, which reflect absorptive capacity or parts of it at the operational level. The main weakness of both proxy indicators and perceptive instruments is that they don't meet the complexity and emergence of the construct. Only few qualitative studies have started to adopt a new perspective, recognizing the process and practice-based character of absorptive capacity. In summary, the critical review prints out the necessity of advancing research in this area. For this reason, we set out to develop an alternative approach to capture absorptive capacity. It is a practice-oriented approach that allows studying actual absorptive practices in real world situations and enables researchers to capture the complex, embedded, and context-dependent patterns of acting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schmidt, Stephanie},
doi = {10.1101/gr.107524.110.20},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenna et al. - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
isbn = {1549-5469 (Electronic)\r1088-9051 (Linking)},
issn = {1088-9051},
journal = {Proceedings of the International Conference on Intellectual Capital, Knowledge Management & Organizational Learning},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
pmid = {20644199},
title = {{Measuring absorptive capacity}},
volume = {20},
year = {2009}
}
@article{Eduardoff2015,
abstract = {Abstract Next generation sequencing (NGS) offers the opportunity to analyse forensic DNA samples and obtain massively parallel coverage of targeted short sequences with the variants they carry. We evaluated the levels of sequence coverage, genotyping precision, sensitivity and mixed DNA patterns of a prototype version of the first commercial forensic NGS kit: the HID-Ion AmpliSeq™ Identity Panel with 169-markers designed for the Ion PGM™ system. Evaluations were made between three laboratories following closely matched Ion PGM™ protocols and a simple validation framework of shared DNA controls. The sequence coverage obtained was extensive for the bulk of SNPs targeted by the HID-Ion AmpliSeq™ Identity Panel. Sensitivity studies showed 90-95% of SNP genotypes could be obtained from 25 to 100 pg of input DNA. Genotyping concordance tests included Coriell cell-line control DNA analyses checked against whole-genome sequencing data from 1000 Genomes and Complete Genomics, indicating a very high concordance rate of 99.8%. Discordant genotypes detected in rs1979255, rs1004357, rs938283, rs2032597 and rs2399332 indicate these loci should be excluded from the panel. Therefore, the HID-Ion AmpliSeq™ Identity Panel and Ion PGM™ system provide a sensitive and accurate forensic SNP genotyping assay. However, low-level DNA produced much more varied sequence coverage and in forensic use the Ion PGM™ system will require careful calibration of the total samples loaded per chip to preserve the genotyping reliability seen in routine forensic DNA. Furthermore, assessments of mixed DNA indicate the user's control of sequence analysis parameter settings is necessary to ensure mixtures are detected robustly. Given the sensitivity of Ion PGM™, this aspect of forensic genotyping requires further optimisation before massively parallel sequencing is applied to routine casework.},
author = {Eduardoff, M. and Santos, C. and {De La Puente}, M. and Gross, T. E. and Fondevila, M. and Strobl, C. and Sobrino, B. and Ballard, D. and Schneider, P. M. and Carracedo and Lareu, M. V. and Parson, W. and Phillips, C.},
doi = {10.1016/j.fsigen.2015.04.007},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eduardoff et al. - 2015 - Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the.pdf:pdf},
isbn = {1878-0326 (Electronic)\r1872-4973 (Linking)},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Identification SNPs,Ion PGM™,Ion Torrent,Massively parallel sequencing,Next generation sequencing},
pages = {110--121},
pmid = {25955683},
publisher = {Elsevier Ireland Ltd},
title = {{Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the Ion PGM™}},
url = {http://dx.doi.org/10.1016/j.fsigen.2015.04.007},
volume = {17},
year = {2015}
}
@article{Cornish2015,
abstract = {High-throughput sequencing, especially of exomes, is a popular diagnostic tool, but it is difficult to determine which tools are the best at analyzing this data. In this study, we use the NIST Genome in a Bottle results as a novel resource for validation of our exome analysis pipeline. We use six different aligners and five different variant callers to determine which pipeline, of the 30 total, performs the best on a human exome that was used to help generate the list of variants detected by the Genome in a Bottle Consortium. Of these 30 pipelines, we found that Novoalign in conjunction with GATK UnifiedGenotyper exhibited the highest sensitivity while maintaining a low number of false positives for SNVs. However, it is apparent that indels are still difficult for any pipeline to handle with none of the tools achieving an average sensitivity higher than 33% or a Positive Predictive Value (PPV) higher than 53%. Lastly, as expected, it was found that aligners can play as vital a role in variant detection as variant callers themselves.},
author = {Cornish, Adam and Guda, Chittibabu},
doi = {10.1155/2015/456479},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cornish, Guda - 2015 - A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference.pdf:pdf},
issn = {23146141},
journal = {BioMed Research International},
number = {BioMed Research International},
pages = {11},
pmid = {26539496},
title = {{A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference}},
volume = {2015},
year = {2015}
}
@article{FigueroaFranco2014,
abstract = {: La investigaci{\'{o}}n, recuperaci{\'{o}}n e identificaci{\'{o}}n de cuerpos relacionados con la desaparici{\'{o}}n de personas dentro del marco del conflicto armado en Colombia se ha llevado a cabo por parte del Estado: Fiscal{\'{i}}a General de la Naci{\'{o}}n (FGN), Polic{\'{i}}a Nacional, Departamento Administrativo de Seguridad (DAS) e Instituto Nacional de Medicina Legal y Ciencias Forenses (INMLCF). El an{\'{a}}lisis de ADN corresponde a una etapa del proceso de identificaci{\'{o}}n cuando otros abordajes –como el dactilosc{\'{o}}pico, odontol{\'{o}}gico y/o antropol{\'{o}}gico– no han permitido la identificaci{\'{o}}n fehaciente. Este an{\'{a}}lisis es de uso sistem{\'{a}}tico en laboratorios forenses a nivel mundial para comparar la informaci{\'{o}}n gen{\'{e}}tica de cad{\'{a}}veres en condici{\'{o}}n de no identificados con los posibles familiares de personas reportadas como desaparecidas. El presente estudio revis{\'{o}} la documentaci{\'{o}}n asociada a 154 solicitudes de identificaci{\'{o}}n recibidas durante 2009, analizadas por los peritos del Grupo de Gen{\'{e}}tica del INMLCF, sede Bogot{\'{a}}, D. C., logrando contribuir a la identificaci{\'{o}}n positiva de 95 cuerpos que inicialmente se encontraban en condici{\'{o}}n de no identificados (CNI). La mayor{\'{i}}a de muestras del estudio correspondieron a cuerpos exhumados de la regi{\'{o}}n Caribe colombiana. En el 80% de las solicitudes se realiz{\'{o}} cotejo gen{\'{e}}tico entre perfiles gen{\'{e}}ticos de familiares y de restos humanos; en el 20% los perfiles gen{\'{e}}ticos se almacenaron en la “Base Nacional de Perfiles Gen{\'{e}}ticos de Aplicaci{\'{o}}n Judicial”, conocida como CODIS por sus siglas en ingl{\'{e}}s (Combined DNA Index System). De las muestras {\'{o}}seas analizadas, el f{\'{e}}mur y la tibia arrojaron mejores resultados. El 77% de los cotejos fueron no exclusiones, el 9% resultados negativos o no concluyentes y el 14% exclusiones},
author = {Cabello, Natalia Ramos},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueroa Franco et al. - 2014 - Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lis.pdf:pdf},
issn = {2344-8164},
journal = {Revista Colombiana de medicina Legal y Ciencias Forense},
keywords = {Cromosomas,Mitochondrial ADN},
number = {Revista Colombiana de Medicina Legal y Ciencias Forenses},
pages = {1--6},
title = {{Dentificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lisis Gen{\'{e}}tico De Restos Humanos Durante 2009 En El Inmlcf De Bogot{\'{a}}, D. C.}},
volume = {2},
year = {2012}
}
@article{Field2015,
abstract = {A diversity of tools is available for identification of variants from genome sequence data. Given the current complexity of incorporating external software into a genome analysis infrastructure, a tendency exists to rely on the results from a single tool alone. The quality of the output variant calls is highly variable however, depending on factors such as sequence library quality as well as the choice of short-read aligner, variant caller, and variant caller filtering strategy. Here we present a two-part study first using the high quality 'genome in a bottle' reference set to demonstrate the significant impact the choice of aligner, variant caller, and variant caller filtering strategy has on overall variant call quality and further how certain variant callers outperform others with increased sample contamination, an important consideration when analyzing sequenced cancer samples. This analysis confirms previous work showing that combining variant calls of multiple tools results in the best quality resultant variant set, for either specificity or sensitivity, depending on whether the intersection or union, of all variant calls is used respectively. Second, we analyze a melanoma cell line derived from a control lymphocyte sample to determine whether software choices affect the detection of clinically important melanoma risk-factor variants finding that only one of the three such variants is unanimously detected under all conditions. Finally, we describe a cogent strategy for implementing a clinical variant detection pipeline; a strategy that requires careful software selection, variant caller filtering optimizing, and combined variant calls in order to effectively minimize false negative variants. While implementing such features represents an increase in complexity and computation the results offer indisputable improvements in data quality.},
author = {Field, Matthew A. and Cho, Vicky and Andrews, T. Daniel and Goodnow, Chris C.},
doi = {10.1371/journal.pone.0143199},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Field et al. - 2015 - Reliably detecting clinically important variants requires both combined variant calls and optimized filtering stra.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--19},
pmid = {26600436},
title = {{Reliably detecting clinically important variants requires both combined variant calls and optimized filtering strategies}},
volume = {10},
year = {2015}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Yang2015,
abstract = {Recent developments in sequencing techniques have enabled rapid and high-throughput generation of sequence data, democratizing the ability to compile information on large amounts of genetic variations in individual laboratories. However, there is a growing gap between the generation of raw sequencing data and the extraction of meaningful biological information. Here, we describe a protocol to use the ANNOVAR (ANNOtate VARiation) software to facilitate fast and easy variant annotations, including gene-based, region-based and filter-based annotations on a variant call format (VCF) file generated from human genomes. We further describe a protocol for gene-based annotation of a newly sequenced nonhuman species. Finally, we describe how to use a user-friendly and easily accessible web server called wANNOVAR to prioritize candidate genes for a Mendelian disease. The variant annotation protocols take 5-30 min of computer time, depending on the size of the variant file, and 5-10 min of hands-on time. In summary, through the command-line tool and the web server, these protocols provide a convenient means to analyze genetic variants generated in humans and other species.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Yang, Hui and Wang, Kai},
doi = {10.1038/nprot.2015.105},
eprint = {15334406},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Wang - 2015 - Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR.pdf:pdf},
isbn = {1750-2799 (Electronic)\r1750-2799 (Linking)},
issn = {17502799},
journal = {Nature Protocols},
number = {10},
pages = {1556--1566},
pmid = {26379229},
publisher = {Nature Publishing Group},
title = {{Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26379229%5Cnhttp://www.nature.com/nprot/journal/v10/n10/pdf/nprot.2015.105.pdf},
volume = {10},
year = {2015}
}
@article{Systems2009a,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - Structural variation and Medical Genomics.pdf:pdf},
title = {{Structural variation and Medical Genomics}},
year = {2009}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (2).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (2)}}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.\n\nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.\n\nDATA SOURCES: Literature review and authors' experience.\n\nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M. and Durtschi, Jacob D. and Margraf, Rebecca L. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)\r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Kashyap2015,
abstract = {Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.},
archivePrefix = {arXiv},
arxivId = {1506.05101},
author = {Kashyap, Hirak and Ahmed, Hasin Afzal and Hoque, Nazrul and Roy, Swarup and Bhattacharyya, Dhruba Kumar},
eprint = {1506.05101},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kashyap et al. - 2014 - Big Data Analytics in Bioinformatics A Machine Learning Perspective.pdf:pdf},
journal = {Journal of Latex Class Files},
number = {9},
pages = {1--20},
title = {{Big Data Analytics in Bioinformatics: A Machine Learning Perspective}},
url = {http://arxiv.org/abs/1506.05101},
volume = {13},
year = {2015}
}
@article{Dudley2010,
abstract = {With the continued exponential expansion of publicly available genomic data and access to low-cost, high-throughput molecular technologies for profiling patient populations, computational technologies and informatics are becoming vital considerations in genomic medicine. Although cloud computing technology is being heralded as a key enabling technology for the future of genomic research, available case studies are limited to applications in the domain of high-throughput sequence data analysis. The goal of this study was to evaluate the computational and economic characteristics of cloud computing in performing a large-scale data integration and analysis representative of research problems in genomic medicine. We find that the cloud-based analysis compares favorably in both performance and cost in comparison to a local computational cluster, suggesting that cloud computing technologies might be a viable resource for facilitating large-scale translational research in genomic medicine.},
author = {Dudley, Joel T. and Pouliot, Yannick and Chen, Rong and Morgan, Alexander A. and Butte, Atul J.},
doi = {10.1186/gm172},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudley et al. - 2010 - Translational bioinformatics in the cloud an affordable alternative.pdf:pdf},
isbn = {1756-994X (Electronic)},
issn = {1756994X},
journal = {Genome Medicine},
number = {8},
pages = {51},
pmid = {20691073},
title = {{Translational bioinformatics in the cloud: An affordable alternative}},
url = {http://genomemedicine.com/content/2/8/51},
volume = {2},
year = {2010}
}
@article{Hussain2016,
abstract = {Medications are an important element of medical records but they usually contain significant data errors. This situation may result from haphazardness or possibly careless storage of valuable information. In either case, this misspelled data can cause serious health problems for the patients and can put their life at a major risk. Thus, the correctness of medication data is an important aspect so that potential harms can be identified and steps can be taken to prevent or mitigate them. In this paper, a novel and practical method is proposed for automated detection and correction of spelling errors in electronic medical record (EMR). To realize this technique, major relevant aspects is taken into consideration with the help of PartsofSpeech tagging and Regular Expressions. The paper concludes with recommendations and future work for giving a new direction to the emendation of drug nomenclature.},
author = {Hussain, Faiza and Qamar, Usman},
doi = {10.5220/0005911503330338},
file = {:home/jennifer/Descargas/ICEIS_2016_240.pdf:pdf},
isbn = {978-989-758-187-8},
journal = {Proceedings of the 18th International Conference on Enterprise Information Systems},
keywords = {but they usually contain,electronic medical record,element of medical records,emr,information retrieval,medications are an important,natural language processing,parts-of-speech tagging,post,regular expressions and medical,significant data errors,spelling correction,text mining,text processing},
number = {Iceis},
pages = {333--338},
title = {{Identification and Correction of Misspelled Drugs' Names in Electronic Medical Records (EMR)}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005911503330338},
volume = {2},
year = {2016}
}
@article{Tsai2016,
abstract = {Effective implementation of precision medicine will be enhanced by a thorough understanding of each patient's genetic composition to better treat his or her presenting symptoms or mitigate the onset of disease. This ideally includes the sequence information of a complete genome for each individual. At Partners HealthCare Personalized Medicine, we have developed a clinical process for whole genome sequencing (WGS) with application in both healthy individuals and those with disease. In this manuscript, we will describe our bioinformatics strategy to efficiently process and deliver genomic data to geneticists for clinical interpretation. We describe the handling of data from FASTQ to the final variant list for clinical review for the final report. We will also discuss our methodology for validating this workflow and the cost implications of running WGS.},
author = {Tsai, Ellen A. and Shakbatyan, Rimma and Evans, Jason and Rossetti, Peter and Graham, Chet and Sharma, Himanshu and Lin, Chiao Feng and Lebo, Matthew S.},
doi = {10.3390/jpm6010012},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai et al. - 2016 - Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine.pdf:pdf},
isbn = {2075-4426 (Print)
2075-4426 (Linking)},
issn = {20754426},
journal = {Journal of Personalized Medicine},
keywords = {Bioinformatics,Clinical sequencing,NGS,Next generation sequencing,Precision medicine,Validation,WGS},
number = {1},
pages = {12},
pmid = {26927186},
title = {{Bioinformaticsworkflow for clinical whole genome sequencing at partners healthcare personalized medicine}},
url = {http://www.mdpi.com/2075-4426/6/1/12/htm http://www.mdpi.com/2075-4426/6/1/12},
volume = {6},
year = {2016}
}
@incollection{Kutzera2017a,
author = {Kutzera, Joachim and May, Patrick},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
isbn = {9783319697505},
issn = {16113349},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A tool for efficiently exploring millions of human genetic variants and their annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
volume = {10649 LNBI},
year = {2017}
}
@book{Smith98,
address = {London},
author = {Simon},
edition = {2nd},
isbn = {0802110509},
publisher = {The publishing company},
title = {{The book}},
year = {2017}
}
@book{Klug2013,
abstract = {10a. ed. Acompaña código de acceso de 12 meses a los recursos online. Incluye Pearson eText. Índice analítico. Glosario: p. [892]-917.},
author = {Klug, W. and Cummings, M.},
isbn = {9788415552482},
publisher = {Pearson Educacin{\'{o}}n},
title = {{Conceptos de Gen{\'{e}}tica}},
url = {https://www.casadellibro.com/libro-conceptos-de-genetica/9788415552482/2547204},
year = {1999}
}
@misc{,
title = {{Genes and genetics: the language of scientific discovery | Oxford English Dictionary}},
url = {https://public.oed.com/aspects-of-english/shapers-of-english/genes-and-genetics-the-language-of-scientific-discovery/},
urldate = {2018-05-18}
}
@article{Liu2014,
abstract = {Discriminative pattern mining is one of the most important techniques in data mining. This challenging task is concerned with finding a set of patterns that occur with disproportionate frequency in data sets with various class labels. Such patterns are of great value for group difference detection and classifier construction. Research on finding interesting discriminative patterns in class-labeled data evolves rapidly and lots of algorithms have been proposed to specifically address this problem. Discriminative pattern mining techniques have proven their considerable value in biological data analysis. The archetypical applications in bioinformatics include phosphorylation motif discovery, differentially expressed gene identification, discriminative genotype pattern detection, etc. In this article, we present an overview of discriminative pattern mining and the corresponding effective methods, and subsequently we illustrate their applications to tackling the bioinformatics problems. In the end, we give a general discussion of potential challenges and future work for this task.},
author = {Liu, Xiaoqing and Wu, Jun and Gu, Feiyang and Wang, Jie and He, Zengyou},
doi = {10.1093/bib/bbu042},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Discriminative pattern mining and its applications in bioinformatics.pdf:pdf},
isbn = {1467-5463},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Contrast sets,Discriminative pattern mining,Emerging patterns,Subgroup discovery},
number = {5},
pages = {884--900},
pmid = {25433466},
title = {{Discriminative pattern mining and its applications in bioinformatics}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu042},
volume = {16},
year = {2014}
}
@article{Mayer2017,
abstract = {Sustainable noncommercial bioinformatics infrastructures are a prerequisite to use and take advantage of the potential of big data analysis for research and economy. Consequently, funders, universities and institutes as well as users ask for a transparent value model for the tools and services offered. In this article, a generally applicable lightweight method is described by which bioinformatics infrastructure projects can estimate the value of tools and services offered without determining exactly the total costs of ownership. Five representative scenarios for value estimation from a rough estimation to a detailed breakdown of costs are presented. To account for the diversity in bioinformatics applications and services, the notion of service-specific 'service provision units' is introduced together with the factors influencing them and the main underlying assumptions for these 'value influencing factors'. Special attention is given on how to handle personnel costs and indirect costs such as electricity. Four examples are presented for the calculation of the value of tools and services provided by the German Network for Bioinformatics Infrastructure (de.NBI): one for tool usage, one for (Web-based) database analyses, one for consulting services and one for bioinformatics training events. Finally, from the discussed values, the costs of direct funding and the costs of payment of services by funded projects are calculated and compared.},
author = {Mayer, Gerhard and Quast, Christian and Felden, Janine and Lange, Matthias and Prinz, Manuel and P{\"{u}}hler, Alfred and Lawerenz, Chris and Scholz, Uwe and Gl{\"{o}}ckner, Frank Oliver and M{\"{u}}ller, Wolfgang and Marcus, Katrin and Eisenacher, Martin},
doi = {10.1093/bib/bbx140},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer et al. - 2017 - A generally applicable lightweight method for calculating a value structure for tools and services in bioinformati.pdf:pdf},
isbn = {14774054 (Electronic)},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
month = {oct},
pmid = {29092005},
title = {{A generally applicable lightweight method for calculating a value structure for tools and services in bioinformatics infrastructure projects}},
url = {http://academic.oup.com/bib/article/doi/10.1093/bib/bbx140/4582343},
year = {2017}
}
@article{Rehm2013,
abstract = {Next-generation sequencing technologies have been and continue to be deployed in clinical laboratories, enabling rapid transformations in genomic medicine. These technologies have reduced the cost of large-scale sequencing by several orders of magnitude, and continuous advances are being made. It is now feasible to analyze an individual's near-complete exome or genome to assist in the diagnosis of a wide array of clinical scenarios. Next-generation sequencing technologies are also facilitating further advances in therapeutic decision making and disease prediction for at-risk patients. However, with rapid advances come additional challenges involving the clinical validation and use of these constantly evolving technologies and platforms in clinical laboratories. To assist clinical laboratories with the validation of next-generation sequencing methods and platforms, the ongoing monitoring of next-generation sequencing testing to ensure quality results, and the interpretation and reporting of variants found using these technologies, the American College of Medical Genetics and Genomics has developed the following professional standards and guidelines.},
author = {Rehm, Heidi L. and Bale, Sherri J. and Bayrak-Toydemir, Pinar and Berg, Jonathan S. and Brown, Kerry K. and Deignan, Joshua L. and Friez, Michael J. and Funke, Birgit H. and Hegde, Madhuri R. and Lyon, Elaine},
doi = {10.1038/gim.2013.92},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehm et al. - 2013 - ACMG clinical laboratory standards for next-generation sequencing.pdf:pdf},
isbn = {1530-0366 (Electronic) 1098-3600 (Linking)},
issn = {10983600},
journal = {Genetics in Medicine},
keywords = {ACMG,exome sequencing,genome sequencing,guidelines,next-generation sequencing,standards},
number = {9},
pages = {733--747},
pmid = {23887774},
title = {{ACMG clinical laboratory standards for next-generation sequencing}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4098820&tool=pmcentrez&rendertype=abstract},
volume = {15},
year = {2013}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zaki, Mohammed J. and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for Molecular Biology},
number = {1},
pages = {4},
pmid = {17428327},
title = {{Data mining in bioinformatics (BIOKDD)}},
volume = {2},
year = {2007}
}
@article{Wang2017a,
abstract = {Biologists are stepping up their efforts in understanding the biological
processes that underlie disease pathways in the clinical contexts. This
has resulted in a flood of biological and clinical data-genomic
sequences, DNAmicroarrays, protein interactions, biomedical images,
disease pathways, etc. The rapid adoption of Electronic HealthRecords
(EHRs) across healthcare systems, coupled with the capability of linking
EHRs to research biorepositories, provides a unique opportunity for
conducting large-scale Precision Medicine research. As a result,
datamining techniques, for knowledge discovery and deriving data driven
insights fromvarious data sources, are increasingly important inmodern
biology and healthcare. The purpose of this special section is to bring
together the researchers in bioinformatics, healthcare informatics, and
datamining to share about their current research, and their visions on
future directions.},
author = {Wang, Fei and Li, Xiao-Li and Wang, Jason T. L. and Ng, See-Kiong},
doi = {10.1109/TCBB.2016.2612558},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {may},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
url = {http://ieeexplore.ieee.org/document/7938559/},
volume = {14},
year = {2017}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas}
}
@article{Searls2010,
abstract = {Copyright: {\textcopyright} 2010 David B. Searls. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
author = {Searls, David B.},
doi = {10.1371/journal.pcbi.1000809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Searls - 2010 - The Roots of Bioinformatics.pdf:pdf},
issn = {1553-734X},
journal = {PLoS Computational Biology},
month = {jun},
number = {6},
pages = {7},
pmid = {20589079},
publisher = {Public Library of Science},
title = {{The Roots of Bioinformatics}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1000809},
volume = {6},
year = {2010}
}
@article{Liu2013,
abstract = {Next generation sequencing (NGS) has been leading the genetic study of human disease into an era of unprecedented productivity. Many bioinformatics pipelines have been developed to call variants from NGS data. The performance of these pipelines depends crucially on the variant caller used and on the calling strategies implemented. We studied the performance of four prevailing callers, SAMtools, GATK, glftools and Atlas2, using single-sample and multiple-sample variant-calling strategies. Using the same aligner, BWA, we built four single-sample and three multiple-sample calling pipelines and applied the pipelines to whole exome sequencing data taken from 20 individuals. We obtained genotypes generated by Illumina Infinium HumanExome v1.1 Beadchip for validation analysis and then used Sanger sequencing as a "gold-standard" method to resolve discrepancies for selected regions of high discordance. Finally, we compared the sensitivity of three of the single-sample calling pipelines using known simulated whole genome sequence data as a gold standard. Overall, for single-sample calling, the called variants were highly consistent across callers and the pairwise overlapping rate was about 0.9. Compared with other callers, GATK had the highest rediscovery rate (0.9969) and specificity (0.99996), and the Ti/Tv ratio out of GATK was closest to the expected value of 3.02. Multiple-sample calling increased the sensitivity. Results from the simulated data suggested that GATK outperformed SAMtools and glfSingle in sensitivity, especially for low coverage data. Further, for the selected discrepant regions evaluated by Sanger sequencing, variant genotypes called by exome sequencing versus the exome array were more accurate, although the average variant sensitivity and overall genotype consistency rate were as high as 95.87{%} and 99.82{%}, respectively. In conclusion, GATK showed several advantages over other variant callers for general purpose NGS analyses. The GATK pipelines we developed perform very well.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Liu, Xiangtao and Han, Shizhong and Wang, Zuoheng and Gelernter, Joel and Yang, Bao Zhu},
doi = {10.1371/journal.pone.0075619},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Variant Callers for Next-Generation Sequencing Data A Comparison Study.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--11},
pmid = {24086590},
title = {{Variant Callers for Next-Generation Sequencing Data: A Comparison Study}},
volume = {8},
year = {2013}
}
@article{Santani2017,
abstract = {Context.—The number of targeted next-generation se-quencing (NGS) panels for genetic diseases offered by clinical laboratories is rapidly increasing. Before an NGS-based test is implemented in a clinical laboratory, appropriate validation studies are needed to determine the performance characteristics of the test. Objective.—To provide examples of assay design and validation of targeted NGS gene panels for the detection of germline variants associated with inherited disorders. Data Sources.—The approaches used by 2 clinical laboratories for the development and validation of targeted NGS gene panels are described. Important design and validation considerations are examined. Conclusions.—Clinical laboratories must validate per-formance specifications of each test prior to implementa-tion. Test design specifications and validation data are provided, outlining important steps in validation of targeted NGS panels by clinical diagnostic laboratories.},
author = {Santani, Avni and Murrell, Jill and Funke, Birgit and Yu, Zhenming and Hegde, Madhuri and Mao, Rong and Ferreira-Gonzalez, Andrea and Voelkerding, Karl V. and Weck, Karen E.},
doi = {10.5858/arpa.2016-0517-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santani et al. - 2017 - Development and validation of targeted next-generation sequencing panels for detection of germline variants in i.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {787--797},
pmid = {28322587},
title = {{Development and validation of targeted next-generation sequencing panels for detection of germline variants in inherited diseases}},
volume = {141},
year = {2017}
}
@article{Cook2016,
abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI's vast public data resources.},
author = {Cook, Charles E. and Bergman, Mary Todd and Finn, Robert D. and Cochrane, Guy and Birney, Ewan and Apweiler, Rolf},
doi = {10.1093/nar/gkv1352},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook et al. - 2016 - The European Bioinformatics Institute in 2016 Data growth and integration.pdf:pdf},
isbn = {13624962 (Electronic)},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D20--D26},
pmid = {26673705},
title = {{The European Bioinformatics Institute in 2016: Data growth and integration}},
volume = {44},
year = {2016}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/},
urldate = {2015-11-04}
}
@book{Herraez2012,
address = {Barcelona},
author = {Herr{\'{a}}ez, Angel},
isbn = {978-84-8086-647-7},
pages = {241},
publisher = {Elsevier Ltd},
title = {{Biolog{\'{i}}a Molecular e Ingenier{\'{i}}a Gen{\'{e}}tica. 2{\textordfeminine} ed.}},
year = {2012}
}
@misc{,
title = {{Zarafa Community Hub}},
url = {https://community.zarafa.com/},
urldate = {2017-11-16}
}
@misc{variome2017,
title = {{About the Human Variome Project: what we do and why we do it - Human Variome Project}},
url = {http://www.humanvariomeproject.org/about/about-the-human-variome-project.html},
urldate = {2017-11-19}
}
@article{Tetreault2015,
abstract = {Whole-exome sequencing (WES) represents a significant breakthrough in the field of human genetics. This technology has largely contributed to the identification of new disease-causing genes and is now entering clinical laboratories. WES represents a powerful tool for diagnosis and could reduce the 'diagnostic odyssey' for many patients. In this review, we present a technical overview of WES analysis, variants annotation and interpretation in a clinical setting. We evaluate the usefulness of clinical WES in different clinical indications, such as rare diseases, cancer and complex diseases. Finally, we discuss the efficacy of WES as a diagnostic tool and the impact on patient management.},
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
doi = {10.1586/14737159.2015.1039516},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
isbn = {1473-7159},
issn = {17448352},
journal = {Expert Review of Molecular Diagnostics},
keywords = {Cancer,Whole-exome sequencing,diagnostic,rare diseases,variants detection},
number = {6},
pages = {749--760},
pmid = {25959410},
title = {{Whole-exome sequencing as a diagnostic tool: Current challenges and future opportunities}},
volume = {15},
year = {2015}
}
@article{Canuel2015,
abstract = {The rise of personalized medicine and the availability of high-throughput molecular analyses in the context of clinical care have increased the need for adequate tools for translational researchers to manage and explore these data. We reviewed the biomedical literature for translational platforms allowing the management and exploration of clinical and omics data, and identified seven platforms: BRISK, caTRIP, cBio Cancer Portal, G-DOC, iCOD, iDASH and tranSMART. We analyzed these platforms along seven major axes. (1) The community axis regrouped information regarding initiators and funders of the project, as well as availability status and references. (2) We regrouped under the information content axis the nature of the clinical and omics data handled by each system. (3) The privacy management environment axis encompassed functionalities allowing control over data privacy. (4) In the analysis support axis, we detailed the analytical and statistical tools provided by the platforms. We also explored (5) interoperability support and (6) system requirements. The final axis (7) platform support listed the availability of documentation and installation procedures. A large heterogeneity was observed in regard to the capability to manage phenotype information in addition to omics data, their security and interoperability features. The analytical and visualization features strongly depend on the considered platform. Similarly, the availability of the systems is variable. This review aims at providing the reader with the background to choose the platform best suited to their needs. To conclude, we discuss the desiderata for optimal translational research platforms, in terms of privacy, interoperability and technical features.},
author = {Canuel, Vincent and Rance, Bastien and Avillach, Paul and Degoulet, Patrice and Burgun, Anita},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data a review of publicly available solutions.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Biomedical research,Clinical data,High-throughput technologies,Information storage and retrieval,Translationalmedical research},
number = {2},
pages = {280--290},
pmid = {24608524},
title = {{Translational research platforms integrating clinical and omics data: A review of publicly available solutions}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu006},
volume = {16},
year = {2015}
}
@article{Renganathan2017,
abstract = {With the exponential increase in the number of articles published every year in the biomedical domain, there is a need to build automated systems to extract unknown information from the articles published. Text mining techniques enable the extraction of unknown knowledge from unstructured documents. This paper reviews text mining processes in detail and the software tools available to carry out text mining. It also reviews the roles and applications of text mining in the biomedical domain. Text mining processes, such as search and retrieval of documents, pre-processing of documents, natural language processing, methods for text clustering, and methods for text classification are described in detail. Text mining techniques can facilitate the mining of vast amounts of knowledge on a given topic from published biomedical research articles and draw meaningful conclusions that are not possible otherwise.},
author = {Renganathan, Vinaitheerthan},
doi = {10.4258/hir.2017.23.3.141},
file = {:home/jennifer/Descargas/hir-23-141.pdf:pdf},
isbn = {20933681 (Linking)},
issn = {2093369X},
journal = {Healthcare Informatics Research},
keywords = {Classification,Cluster analysis,Natural language processing,Software,Text mining},
number = {3},
pages = {141--146},
pmid = {28875048},
title = {{Text mining in biomedical domain with emphasis on document clustering}},
volume = {23},
year = {2017}
}
@article{KonstantinosN.LazaridisMD;KimberlyA.SchahlCGC;MargotA.CousinPhD;DusicaBabovic-VuksanovicMD;DouglasL.Riegert-JohnsonMD;RalitzaH.GavrilovaMD;TammyM.McAllisterMA;NoralaneM.LindorMD;RoshiniS.AbrahamPhD;MichaelJ.Ac2016,
author = {et al... Lazaridis, Konstantinos},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
journal = {Mayo Clinic Proceedings},
number = {3},
pages = {29},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience}},
volume = {91},
year = {2016}
}
@article{BernalAcevedo2011,
abstract = {86 osCar bernaL-aCevedo • juan CaMiLo forero-CaMaCHo Rev. Gerenc. Polit. Salud, Bogot{\'{a}} (Colombia), 10 (21): 85-100, julio-diciembre de 2011 Resumen Objetivo: caracterizar y evaluar los sistemas de informaci{\'{o}}n del sector salud en Colombia. Metodolog{\'{i}}a: Se desarroll{\'{o}} un marco conceptual que incluy{\'{o}} contexto legal del pa{\'{i}}s y confor-maci{\'{o}}n de sistemas de informaci{\'{o}}n en otros pa{\'{i}}ses. Posteriormente se caracteriz{\'{o}} el sistema de informaci{\'{o}}n de salud colombiano, a partir de entrevistas con actores relevantes y literatura pertinente. Finalmente, se analiz{\'{o}} la conformaci{\'{o}}n del sistema, el flujo de informaci{\'{o}}n y las fortalezas y debilidades de {\'{e}}ste, para la posterior formulaci{\'{o}}n de recomendaciones. Resultados y conclusiones: el sistema de informaci{\'{o}}n en salud colombiano se encuentra fragmentado y presenta problemas de calidad, situaci{\'{o}}n similar a la de otros pa{\'{i}}ses. Es esencial el desarrollo de una cultura de producci{\'{o}}n, difusi{\'{o}}n y utilizaci{\'{o}}n de la informaci{\'{o}}n. Se debe aprovechar el momento de cambio que sufre el sistema de salud para buscar la mejor{\'{i}}a de la informaci{\'{o}}n. Los mecanismos de captura de la informaci{\'{o}}n requieren una simplificaci{\'{o}}n y estandarizaci{\'{o}}n. Palabras clave autor: servicios de informaci{\'{o}}n, Colombia, comunicaci{\'{o}}n, salud p{\'{u}}blica, informaci{\'{o}}n, administraci{\'{o}}n en salud p{\'{u}}blica.},
author = {Bernal-Acevedo, Oscar and Forero-Camacho, Juan Camilo and Au, Tercer Piso},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernal Acevedo, Forero Camacho - 2011 - Sistemas de informaci{\'{o}}n en el sector salud en Colombia.pdf:pdf},
journal = {Rev. Gerenc. Polit. Salud, Bogot{\'{a}} (Colombia)},
keywords = {de informaci{\'{o}}n en el,sector salud},
number = {21},
pages = {85--100},
title = {{Sistemas de informaci{\'{o}}n en el sector salud en Colombia * Information systems in health sector in Colombia Sistemas de informa{\c{c}}{\~{a}}o no setor sa{\'{u}}de na Col{\^{o}}mbia}},
url = {http://www.scielo.org.co/pdf/rgps/v10n21/v10n21a06.pdf},
volume = {10},
year = {2011}
}
@article{Wu2016a,
abstract = {Association rule mining is an important task in the field of data mining, and many efficient algorithms have been proposed to address this problem. However, a large portion of the rules reported by these algorithms just satisfy the user-defined constraints purely by accident, and those that are not statistically meaningful should be filtered out through statistical significance testing. In the context of association rule discovery, the permutation-based approach can achieve better performance than other competitive methods, although several drawbacks of this effective approach narrow its usability. In this paper, we provide an analysis of these disadvantages and propose an algorithm called Exact Permutation p-values for Association Rules (EPAR) to calculate the exact p-values of all tested rules. Experiments on different types of data sets demonstrate that EPAR can successfully alleviate the disadvantages and outperform the direct permutation-based method over several performance measures.},
author = {Wu, Jun and He, Zengyou and Gu, Feiyang and Liu, Xiaoqing and Zhou, Jianyu and Yang, Can},
doi = {10.1016/j.ins.2016.01.094},
issn = {00200255},
journal = {Information Sciences},
keywords = {Association rule mining,Exact permutation p-value,Permutation testing,Statistical significance testing},
month = {jun},
pages = {146--162},
publisher = {Elsevier},
title = {{Computing exact permutation p-values for association rules}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025516300366},
volume = {346-347},
year = {2016}
}
@article{Lazaridis2016,
abstract = {Objective To describe the experience and outcome of performing whole-exome sequencing (WES) for resolution of patients on a diagnostic odyssey in the first 18 months of an individualized medicine clinic (IMC). Patients and Methods The IMC offered WES to physicians of Mayo Clinic practice for patients with suspected genetic disease. DNA specimens of the proband and relatives were submitted to WES laboratories. We developed the Genomic Odyssey Board with multidisciplinary expertise to determine the appropriateness for IMC services, review WES reports, and make the final decision about whether the exome findings explain the disease. This study took place from September 30, 2012, to March 30, 2014. Results In the first 18 consecutive months, the IMC received 82 consultation requests for patients on a diagnostic odyssey. The Genomic Odyssey Board deferred 7 cases and approved 75 cases to proceed with WES. Seventy-one patients met with an IMC genomic counselor. Fifty-one patients submitted specimens for WES testing, and the results have been received for all. There were 15 cases in which a diagnosis was made on the basis of WES findings; thus, the positive diagnostic yield of this practice was 29%. The mean cost per patient for this service was approximately $8000. Medicaid supported 27% of the patients, and 38% of patients received complete or partial insurance coverage. Conclusion The significant diagnostic yield, moderate cost, and notable health marketplace acceptance for WES compared with conventional genetic testing make the former method a rational diagnostic approach for patients on a diagnostic odyssey.},
author = {Lazaridis, Konstantinos N. and Schahl, Kimberly A. and Cousin, Margot A. and Babovic-Vuksanovic, Dusica and Riegert-Johnson, Douglas L. and Gavrilova, Ralitza H. and McAllister, Tammy M. and Lindor, Noralane M. and Abraham, Roshini S. and Ackerman, Michael J. and Pichurin, Pavel N. and Deyle, David R. and Gavrilov, Dimitar K. and Hand, Jennifer L. and Klee, Eric W. and Stephens, Michael C. and Wick, Myra J. and Atkinson, Elizabeth J. and Linden, David R. and Ferber, Matthew J. and Wieben, Eric D. and Farrugia, Gianrico and Baudhuin, Linnea M. and Beck, Scott A. and Beek, Geoffrey J. and Go, Ronald S. and Guthrie, Kimberly J. and Hovan, Michael J. and Hunt, Katherine S. and Kemppainen, Jennifer L. and Kruisselbrink, Teresa M. and McCormick, Jennifer B. and McLaughlin, Brooke M. and Murphree, Marine I. and Niewold, Timothy B. and Oglesbee, Devin and Reed, Ann and Thibodeau, Stephen N. and Thorland, Erik C.},
doi = {10.1016/j.mayocp.2015.12.018},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
isbn = {1942-5546 (Electronic) 0025-6196 (Linking)},
issn = {19425546},
journal = {Mayo Clinic Proceedings},
language = {English},
month = {mar},
number = {3},
pages = {297--307},
pmid = {26944241},
publisher = {Elsevier},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience}},
url = {http://www.mayoclinicproceedings.org/article/S0025619616000240/fulltext},
volume = {91},
year = {2016}
}
@article{Shendure2016,
abstract = {The exome is the portion of the genome that encodes proteins. Aggregation of 60,706 human exome sequences from 14 studies provides in-depth insight into genetic variation in humans.},
archivePrefix = {arXiv},
arxivId = {20},
author = {Shendure, Jay},
doi = {10.1038/536277a},
eprint = {20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shendure - 2016 - Human genomics A deep dive into genetic variation.pdf:pdf},
isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
keywords = {Genetics,Genomics},
month = {aug},
number = {7616},
pages = {277--278},
pmid = {27535530},
publisher = {Nature Publishing Group},
title = {{Human genomics: A deep dive into genetic variation}},
url = {http://www.nature.com/doifinder/10.1038/536277a},
volume = {536},
year = {2016}
}
@article{Cossio2012,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Munz, Ernst Dietrich},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cossio et al. - 2012 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {07221541},
journal = {Nervenheilkunde},
keywords = {Education reform,Personnel requirements,Psychiatry,Psychotherapist},
number = {10},
pages = {800--805},
pmid = {15991970},
title = {{Psychotherapie in der Psychiatrie}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233&partnerID=tZOtx3y1},
volume = {36},
year = {2017}
}
@article{Lopez2017,
abstract = {High-profile genomic variation projects like the 1000 Genomes project or the Exome Aggregation Consortium, are generating a wealth of human genomic variation knowledge which can be used as an essential reference for identifying disease-causing genotypes. However, accessing these data, contrasting the various studies and integrating those data in downstream analyses remains cumbersome. The Human Genome Variation Archive (HGVA) tackles these challenges and facilitates access to genomic data for key reference projects in a clean, fast and integrated fashion. HGVA provides an efficient and intuitive web-interface for easy data mining, a comprehensive RESTful API and client libraries in Python, Java and JavaScript for fast programmatic access to its knowledge base. HGVA calculates population frequencies for these projects and enriches their data with variant annotation provided by CellBase, a rich and fast annotation solution. HGVA serves as a proof-of-concept of the genome analysis developments being carried out by the University of Cambridge together with UK's 100 000 genomes project and the National Institute for Health Research BioResource Rare-Diseases, in particular, deploying open-source for Computational Biology (OpenCB) software platform for storing and analyzing massive genomic datasets.},
author = {Lopez, Javier and Coll, Jacobo and Haimel, Matthias and Kandasamy, Swaathi and Tarraga, Joaquin and Furio-Tari, Pedro and Bari, Wasim and Bleda, Marta and Rueda, Antonio and Gr{\"{a}}f, Stefan and Rendon, Augusto and Dopazo, Joaquin and Medina, Ignacio},
doi = {10.1093/nar/gkx445},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopez et al. - 2017 - HGVA the Human Genome Variation Archive.pdf:pdf},
isbn = {13624962 (Electronic)},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {W1},
pages = {W189--W194},
pmid = {28535294},
title = {{HGVA: The Human Genome Variation Archive}},
volume = {45},
year = {2017}
}
@article{Lonardi2010,
abstract = {The two papers in this special section are extended papers chosen from nine peer-reviewed papers originally presented at the 2008 International Workshops on Data Mining in Bioinformatics (BIOKDD), held 24-27 August in Las Vegas, NV.},
author = {Lonardi, Stefano and Chen, Jake},
doi = {10.1109/TCBB.2010.28},
issn = {15455963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {apr},
number = {2},
pages = {195--196},
pmid = {20458778},
title = {{Data mining in bioinformatics: Selected papers from BIOKDD}},
url = {http://ieeexplore.ieee.org/document/5460415/},
volume = {7},
year = {2010}
}
@article{Warden2014,
abstract = {The Genome Analysis Toolkit (GATK) is commonly used for variant calling of single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels) from short-read sequencing data aligned against a reference genome. There have been a number of variant calling comparisons against GATK, but an equally comprehensive comparison for VarScan not yet been performed. More specifically, we compare (1) the effects of different pre-processing steps prior to variant calling with both GATK and VarScan, (2) VarScan variants called with increasingly conservative parameters, and (3) filtered and unfiltered GATK variant calls (for both the UnifiedGenotyper and the HaplotypeCaller). Variant calling was performed on three datasets (1 targeted exon dataset and 2 exome datasets), each with approximately a dozen subjects. In most cases, pre-processing steps (e.g., indel realignment and quality score base recalibration using GATK) had only a modest impact on the variant calls, but the importance of the pre-processing steps varied between datasets and variant callers. Based upon concordance statistics presented in this study, we recommend GATK users focus on "high-quality" GATK variants by filtering out variants flagged as low-quality. We also found that running VarScan with a conservative set of parameters (referred to as "VarScan-Cons") resulted in a reproducible list of variants, with high concordance (>97%) to high-quality variants called by the GATK UnifiedGenotyper and HaplotypeCaller. These conservative parameters result in decreased sensitivity, but the VarScan-Cons variant list could still recover 84-88% of the high-quality GATK SNPs in the exome datasets. This study also provides limited evidence that VarScan-Cons has a decreased false positive rate among novel variants (relative to high-quality GATK SNPs) and that the GATK HaplotypeCaller has an increased false positive rate for indels (relative to VarScan-Cons and high-quality GATK UnifiedGenotyper indels). More broadly, we believe the metrics used for comparison in this study can be useful in assessing the quality of variant calls in the context of a specific experimental design. As an example, a limited number of variant calling comparisons are also performed on two additional variant callers.},
author = {Warden, Charles D. and Adamson, Aaron W. and Neuhausen, Susan L. and Wu, Xiwei},
doi = {10.7717/peerj.600},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warden et al. - 2014 - Detailed comparison of two popular variant calling packages for exome and targeted exon studies.pdf:pdf},
isbn = {2167-8359 (Electronic)},
issn = {2167-8359},
journal = {PeerJ},
keywords = {exome,gatk,small indel,snp,targeted sequencing,variant calling,varscan},
pages = {e600},
pmid = {25289185},
title = {{Detailed comparison of two popular variant calling packages for exome and targeted exon studies}},
url = {https://peerj.com/articles/600},
volume = {2},
year = {2014}
}
@article{Buckley1988,
abstract = {-The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term-weighting systems. This article summarizes the insights gained in automatic term weight-ing, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared. 1. AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users' information queries. Typically, certain words extracted from the texts of doc-uments and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections. In either case, the documents would be represented by term vectors of the form},
author = {Salton, Gerard and Buckley, Christopher},
file = {:home/jennifer/Descargas/salton1988.pdf:pdf},
journal = {Information Processing & Management},
number = {5},
pages = {513--523},
title = {{Term-Weighting Approaches in Automatic Text Retrieval}},
volume = {24},
year = {1988}
}
@article{Staccini2014,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - 2013 - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Association rule,Biclustering,Frequent item set,Market basket analysis,Pattern mining},
number = {2},
pages = {216--231},
pmid = {24162173},
title = {{A primer to frequent itemset mining for bioinformatics}},
url = {http://link.springer.com/10.1007/978-2-8178-0478-1_13%5Cnhttp://link.springer.com/chapter/10.1007/978-2-8178-0478-1_13 http://linkinghub.elsevier.com/retrieve/pii/S0957417408000195%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24162173},
volume = {16},
year = {2015}
}
@article{Guo2016,
abstract = {The HID-Ion AmpliSeq™ Identity Panel (the HID Identity Panel) is designed to detect 124-plex single nucleotide polymorphisms (SNPs) with next generation sequencing (NGS) technology on the Ion Torrent PGM™ platform, including 90 individual identification SNPs (IISNPs) on autosomal chromosomes and 34 lineage informative SNPs (LISNPs) on Y chromosome. In this study, we evaluated performance for the HID Identity Panel to provide a reference for NGS-SNP application, focusing on locus strand balance, locus coverage balance, heterozygote balance, and background signals. Besides, several experiments were carried out to find out improvements and limitations of this panel, including studies of species specificity, repeatability and concordance, sensitivity, mixtures, case-type samples and degraded samples, population genetics and pedigrees following the Scientific Working Group on DNA Analysis Methods (SWGDAM) guidelines. In addition, Southern and Northern Chinese Han were investigated to assess applicability of this panel. Results showed this panel led to cross-reactivity with primates to some extent but rarely with non-primate animals. Repeatable and concordant genotypes could be obtained in triplicate with one exception at rs7520386. Full profiles could be obtained from 100 pg input DNA, but the optimal input DNA would be 1 ng–200 pg with 21 initial PCR cycles. A sample with ≥20% minor contributor could be considered as a mixture by the number of homozygotes, and full profiles belonging to minor contributors could be detected between 9:1 and 1:9 mixtures with known reference profiles. Also, this assay could be used for case-type samples and degraded samples. For autosomal SNPs (A-SNPs), FSTacross all 90 loci was not significantly different between Southern and Northern Chinese Han or between male and female samples. All A-SNP loci were independent in Chinese Han population. Except for 18 loci with He<0.4, most of the A-SNPs in the HID Identity Panel presented high polymorphisms. Forensic parameters were calculated as >99.999% for combined discrimination power (CDP), 0.999999724 for combined power of exclusion (CPE), 1.390 × 1011for combined likelihood ratio (CLR) of trios, and 2.361 × 106for CLR of motherless duos. For Y-SNPs, a total of 8 haplotypes were observed with the value of 0.684 for haplotype diversity. As a whole, the HID Identity Panel is a well-performed, robust, reliable and high informative NGS-SNP assay and it can fully meet requirements for individual identification and paternity testing in forensic science.},
author = {Guo, Fei and Zhou, Yishu and Song, He and Zhao, Jinling and Shen, Hongying and Zhao, Bin and Liu, Feng and Jiang, Xianhua},
doi = {10.1016/j.fsigen.2016.07.021},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2016 - Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform.pdf:pdf},
isbn = {1878-0326 (Electronic) 1872-4973 (Linking)},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Evaluation,HID-Ion AmpliSeq™,Identity Panel,Ion Torrent PGM™,Next generation sequencing (NGS),Population genetics,Single nucleotide polymorphism (SNP)},
pages = {73--84},
pmid = {27500651},
publisher = {Elsevier Ireland Ltd},
title = {{Next generation sequencing of SNPs using the HID-Ion AmpliSeq™ Identity Panel on the Ion Torrent PGM™ platform}},
url = {http://dx.doi.org/10.1016/j.fsigen.2016.07.021},
volume = {25},
year = {2016}
}
@article{mckinneypandas,
abstract = {In this paper we will discuss pandas, a Python library of rich data structures and tools for working with structured data sets common to statistics, ﬁnance, social sciences, and many other ﬁelds. The library provides integrated, intuitive routines for performing common data manipulations and analysis on such data sets. It aims to be the foundational layer for the future of statistical computing in Python. It serves as a strong complement to the existing scientiﬁc Python stack while implementing and improving upon the kinds of data manipulation tools found in other statistical programming languages such as R. In addition to detailing its design and features of pandas, we will discuss future avenues of work and growth opportunities for statistics and data analysis applications in the Python language.},
author = {McKinney, Wes},
journal = {Python for High Performance and Scientific Computing},
pages = {1--9},
title = {{pandas: a Foundational Python Library for Data Analysis and Statistics}},
year = {2011}
}
@article{Cheng2015,
abstract = {The Cancer Genome Atlas (TCGA) has given researchers and clinicians unprecedented access to many different cancers through multiple platforms that include exome sequencing, comparative genomic hybridisation (CGH) arrays, DNA methylation arrays, RNA sequencing, reverse protein phase arrays (RPPA), and clinical features. Most data are available to the public in their raw and processed forms; however, analysis and interpretation of these data require specialised training and software. To address this problem, online tools such as cBioportal, canEvolve, GDAC firehose, PROGgeneV2, and UCSC Cancer browser have been developed by various groups to explore and perform analyses on the datasets that are easily understandable by basic researchers and clinicians. In this mini-review, we give an overview of the datasets available from TCGA and the public tools available for integrative analysis of survival with the genomic and transcriptomic datasets, and introduce a tool being developed by our group to analyse the datasets within TCGA.},
author = {Cheng, Phil F. and Dummer, Reinhard and Levesque, Mitch P.},
doi = {10.4414/smw.2015.14183},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2015 - Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in.pdf:pdf},
isbn = {0036-7672},
issn = {14243997},
journal = {Swiss Medical Weekly},
keywords = {Data mining,Genomics,The Cancer Genome Atlas,Transcriptomics},
number = {October},
pages = {1--5},
pmid = {26375999},
title = {{Data mining The Cancer Genome Atlas in the era of precision cancer medicine}},
volume = {145},
year = {2015}
}
@article{Matthijs2015,
abstract = {We present, on behalf of EuroGentest and the European Society of Human Genetics, guidelines for the evaluation and validation of next-generation sequencing (NGS) applications for the diagnosis of genetic disorders. The work was performed by a group of laboratory geneticists and bioinformaticians, and discussed with clinical geneticists, industry and patients' representatives, and other stakeholders in the field of human genetics. The statements that were written during the elaboration of the guidelines are presented here. The background document and full guidelines are available as supplementary material. They include many examples to assist the laboratories in the implementation of NGS and accreditation of this service. The work and ideas presented by others in guidelines that have emerged elsewhere in the course of the past few years were also considered and are acknowledged in the full text. Interestingly, a few new insights that have not been cited before have emerged during the preparation of the guidelines. The most important new feature is the presentation of a 'rating system' for NGS-based diagnostic tests. The guidelines and statements have been applauded by the genetic diagnostic community, and thus seem to be valuable for the harmonization and quality assurance of NGS diagnostics in Europe.European Journal of Human Genetics advance online publication, 28 October 2015; doi:10.1038/ejhg.2015.226.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.5277v2},
author = {Matthijs, Gert and Souche, Erika and Alders, Mari{\"{e}}lle and Corveleyn, Anniek and Eck, Sebastian and Feenstra, Ilse and Race, Val{\'{e}}rie and Sistermans, Erik and Sturm, Marc and Weiss, Marjan and Yntema, Helger and Bakker, Egbert and Scheffer, Hans and Bauer, Peter},
doi = {10.1038/ejhg.2015.226},
eprint = {arXiv:1301.5277v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthijs et al. - 2015 - Guidelines for diagnostic next-generation sequencing.pdf:pdf},
isbn = {1476-5438 (Electronic)\r1018-4813 (Linking)},
issn = {14765438},
journal = {European Journal of Human Genetics},
number = {1},
pages = {2--5},
pmid = {26508566},
title = {{Guidelines for diagnostic next-generation sequencing}},
volume = {24},
year = {2016}
}
@misc{Watson1953,
abstract = {A structure for nucleic acid has already been proposed by Pauling and Corey 1 . They kindly made their manuscript available to us in advance of publication. Their model consists of three intertwined chains, with the phosphates near},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Dobbs, Matthew B.},
booktitle = {Clinical Orthopaedics and Related Research},
doi = {10.1097/BLO.0b013e3181468780},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Crick - 1953 - Molecular structure of nucleic acids.pdf:pdf},
isbn = {0226284158},
issn = {0009921X},
keywords = {nucleic acids},
number = {462},
pages = {2},
pmid = {17804964},
title = {{Genetics in orthopaedics: Editorial comment}},
url = {http://www.nature.com/physics/looking-back/crick/%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/13054692},
volume = {171},
year = {2007}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Williams2011,
abstract = {Editorial Biological data mining is playing an increasingly important role throughout the spectrum of biological and biomedical research with broad implications for the understanding of life science questions such as the tree of life and practical applica-tions of such knowledge to improving human health. Perhaps nowhere is data mining needed more than the emerging discipline of precision medicine. The ability to predict individual risk of presenting with a disease or response to treatment is at the core of the concept of precision medicine, which is gaining ever-increasing levels of traction in the era of technology-driven measurement of biological systems. This has become especially important with the new Presidential initiative on precision medi-cine in the United States [1]. It is obvious to the readers of BioData Mining that this will require careful analyses of large and often complex data sets to best translate information into increasingly individualized risk. Here we ask why improved and appropriate data mining is not only positive but a vast improvement on most current analyses of genomic data. The answer lies to some extent in elucidating the present practice of -omic analyses and how we will need to expand it. Many current -omic approaches rely on univariate and linear analyses that can often miss the underlying architecture of complex traits. For example, univariate analyses of single genetic markers for association with disease risk, prognosis, or drug response that are the analytical standards for genetic analyses of human disease, and have been promoted as a means to develop personalized or more recently precision medicine, make many assumptions about architecture. Given the interest in precision medicine, it is important to ask explicitly what is being assayed in these types of studies that have been argued, incorrectly we believe, as the precursors to precision medicine. Most human geneticists study the association of genetic variants, be they common or rare, assessed across moderate to large samples of cases and controls. The effect of each allelic substitution is then measured as it associates with a particular phenotype. These estimates can provide useful population level risks; however, they are simply the average effect of an allelic substitution across the population, not necessarily predictive of results in an individual or a subgroup. The concept of average allelic effect is one that is well developed in quantitative genetics, but by its very name is suggestive not of precision medicine but of average medicine. Hence, it is possible in a large outbreeding},
author = {Williams, Scott M. and Moore, Jason H.},
doi = {10.1186/s13040-015-0049-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Moore - 2011 - Lumping versus splitting the need for biological data mining in precision medicine.pdf:pdf},
isbn = {1304001500491},
issn = {17560381},
journal = {BioData Mining},
number = {1},
pmid = {26085842},
title = {{Lumping versus splitting: The need for biological data mining in precision medicine}},
volume = {8},
year = {2015}
}
@article{Feero2017,
abstract = {The US health care system has seen significant changes over the last decade resulting from diverse factors including the widespread adoption of electronic health records, the opioid crisis, and the roll-out of the Affordable Care Act. Such changes have buffeted the day-to-day work of physi-cians and other health care practitioners, leaving many finding it challenging to address demands placed on them. Looking forward in 2017, it seems that the future will hold more turbulence. 1 In this milieu, it is easily possible to over-look that medicine has been undergoing a much more gradual and deeper transformation. This shift is inexorably moving medicine from an endeavor in which care for indi-vidual patients is driven by trial and error informed by stud-ies designed to measure population outcomes to one in which care is selected based on a deep understanding of health and disease attributes unique to each individual. Accelerated by the completion of the Human Genome Proj-ect, this transformation has been variably called genomic medicine, genomic health care, personalized medicine, pre-cision medicine, and precision health. The extent to which it ultimately alters medicine remains unclear. 2,3},
author = {Feero, W. Gregory},
doi = {10.1001/jama.2016.20625},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feero - 2017 - Introducing “Genomics and Precision Health”(2).pdf:pdf},
issn = {15383598},
journal = {JAMA - Journal of the American Medical Association},
keywords = {genomics,precision medicine},
month = {may},
number = {18},
pages = {1842--1843},
pmid = {28492888},
publisher = {American Medical Association},
title = {{Introducing "genomics and precision health"}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.20625},
volume = {317},
year = {2017}
}
@article{Uppu2014,
abstract = {There have been many studies that depict genotype- phenotype relationships by identifying genetic variants associated with a specific disease. Researchers focus more attention on interactions between SNPs that are strongly associated with disease in the absence of main effect. In this context, a number of machine learning and data mining tools are applied to identify the combinations of multi-locus SNPs in higher order data. However, none of the current models can identify useful SNP- SNP interactions for high dimensional genome data. Detecting these interactions is challenging due to bio-molecular complexities and computational limitations. The goal of this research was to implement associative classification and study its effectiveness for detecting the epistasis in balanced and imbalanced datasets. The proposed approach was evaluated for two locus epistasis interactions using simulated data. The datasets were generated for 5 different penetrance functions by varying heritability, minor allele frequency and sample size. In total, 23,400 datasets were generated and several experiments are conducted to identify the disease causal SNP interactions. The accuracy of classification by the proposed approach was compared with the previous approaches. Though associative classification showed only relatively small improvement in accuracy for balanced datasets, it outperformed existing approaches in higher order multi-locus interactions in imbalanced datasets. Keywords—},
author = {Uppu, Suneetha and Krishna, Aneesh and Gopalan, Raj P.},
doi = {10.1109/BIBE.2014.29},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uppu, Krishna, Gopalan - 2014 - An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Geno.pdf:pdf},
isbn = {978-1-4799-7502-0},
journal = {2014 IEEE International Conference on Bioinformatics and Bioengineering},
keywords = {3,associative classification,billion-base human genome,epistasis,estimated that about 12,million snps occur along,multi-locus,snp-snp interactions,the 3-,the consequences of snps},
pages = {329--333},
title = {{An Associative Classification Based Approach for Detecting SNP-SNP Interactions in High Dimensional Genome}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7033602},
year = {2014}
}
@article{Wang2010,
abstract = {High-throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes, but it remains a challenge to pinpoint a small subset of functionally important variants. To fill these unmet needs, we developed the ANNOVAR tool to annotate single nucleotide variants (SNVs) and insertions/deletions, such as examining their functional consequence on genes, inferring cytogenetic bands, reporting functional importance scores, finding variants in conserved regions, or identifying variants reported in the 1000 Genomes Project and dbSNP. ANNOVAR can utilize annotation databases from the UCSC Genome Browser or any annotation data set conforming to Generic Feature Format version 3 (GFF3). We also illustrate a 'variants reduction' protocol on 4.7 million SNVs and indels from a human genome, including two causal mutations for Miller syndrome, a rare recessive disease. Through a stepwise procedure, we excluded variants that are unlikely to be causal, and identified 20 candidate genes including the causal gene. Using a desktop computer, ANNOVAR requires ∼4 min to perform gene-based annotation and ∼15 min to perform variants reduction on 4.7 million variants, making it practical to handle hundreds of human genomes in a day. ANNOVAR is freely available at http://www.openbioinformatics.org/annovar/.},
author = {Wang, Kai and Li, Mingyao and Hakonarson, Hakon},
doi = {10.1093/nar/gkq603},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Li, Hakonarson - 2010 - ANNOVAR functional annotation of genetic variants from high-throughput sequencing data.pdf:pdf},
isbn = {0305-1048},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {16},
pages = {e164},
pmid = {20601685},
title = {{ANNOVAR: Functional annotation of genetic variants from high-throughput sequencing data}},
volume = {38},
year = {2010}
}
@article{Li2017a,
abstract = {Widespread clinical laboratory implementation of next-generation sequencing–based cancer testing has highlighted the importance and potential benefits of standardizing the interpretation and reporting of molecular results among laboratories. A multidisciplinary working group tasked to assess the current status of next-generation sequencing–based cancer testing and establish standardized consensus classification, annotation, interpretation, and reporting conventions for somatic sequence variants was convened by the Association for Molecular Pathology with liaison representation from the American College of Medical Genetics and Genomics, American Society of Clinical Oncology, and College of American Pathologists. On the basis of the results of professional surveys, literature review, and the Working Group's subject matter expert consensus, a four-tiered system to categorize somatic sequence variations based on their clinical significances is proposed: tier I, variants with strong clinical significance; tier II, variants with potential clinical significance; tier III, variants of unknown clinical significance; and tier IV, variants deemed benign or likely benign. Cancer genomics is a rapidly evolving field; therefore, the clinical significance of any variant in therapy, diagnosis, or prognosis should be reevaluated on an ongoing basis. Reporting of genomic variants should follow standard nomenclature, with testing method and limitations clearly described. Clinical recommendations should be concise and correlate with histological and clinical findings.},
author = {Li, Marilyn M. and Datto, Michael and Duncavage, Eric J. and Kulkarni, Shashikant and Lindeman, Neal I. and Roy, Somak and Tsimberidou, Apostolia M. and Vnencak-Jones, Cindy L. and Wolff, Daynna J. and Younes, Anas and Nikiforova, Marina N.},
doi = {10.1016/j.jmoldx.2016.10.002},
file = {:home/jennifer/Descargas/main.pdf:pdf},
isbn = {1943-7811 (Electronic)
1525-1578 (Linking)},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {1},
pages = {4--23},
pmid = {27993330},
publisher = {American Society for Investigative Pathology and the Association for Molecular Pathology},
title = {{Standards and Guidelines for the Interpretation and Reporting of Sequence Variants in Cancer: A Joint Consensus Recommendation of the Association for Molecular Pathology, American Society of Clinical Oncology, and College of American Pathologists}},
url = {http://dx.doi.org/10.1016/j.jmoldx.2016.10.002},
volume = {19},
year = {2017}
}
@article{Ghoneim2014,
abstract = {BACKGROUND: Insertions/deletions (indels) are the second most common type of genomic variant and the most common type of structural variant. Identification of indels in next generation sequencing data is a challenge, and algorithms commonly used for indel detection have not been compared on a research cohort of human subject genomic data. Guidelines for the optimal detection of biologically significant indels are limited. We analyzed three sets of human next generation sequencing data (48 samples of a 200 gene target exon sequencing, 45 samples of whole exome sequencing, and 2 samples of whole genome sequencing) using three algorithms for indel detection (Pindel, Genome Analysis Tool Kit's UnifiedGenotyper and HaplotypeCaller).$\$n$\$nRESULTS: We observed variation in indel calls across the three algorithms. The intersection of the three tools comprised only 5.70{%} of targeted exon, 19.52{%} of whole exome, and 14.25{%} of whole genome indel calls. The majority of the discordant indels were of lower read depth and likely to be false positives. When software parameters were kept consistent across the three targets, HaplotypeCaller produced the most reliable results. Pindel results did not validate well without adjustments to parameters to account for varied read depth and number of samples per run. Adjustments to Pindel's M (minimum support for event) parameter improved both concordance and validation rates. Pindel was able to identify large deletions that surpassed the length capabilities of the GATK algorithms.$\$n$\$nCONCLUSIONS: Despite the observed variability in indel identification, we discerned strengths among the individual algorithms on specific data sets. This allowed us to suggest best practices for indel calling. Pindel's low validation rate of indel calls made in targeted exon sequencing suggests that HaplotypeCaller is better suited for short indels and multi-sample runs in targets with very high read depth. Pindel allows for optimization of minimum support for events and is best used for detection of larger indels at lower read depths.},
author = {Ghoneim, Dalia H. and Myers, Jason R. and Tuttle, Emily and Paciorkowski, Alex R.},
doi = {10.1186/1756-0500-7-864},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghoneim et al. - 2014 - Comparison of insertiondeletion calling algorithms on human next-generation sequencing data.pdf:pdf},
isbn = {1756-0500 (Electronic){$}\backslash{\$}r1756-0500 (Linking)},
issn = {17560500},
journal = {BMC Research Notes},
keywords = {Concordance,GATK,Indels,Next generation sequencing,Pindel,Validation},
number = {1},
pages = {864},
pmid = {25435282},
title = {{Comparison of insertion/deletion calling algorithms on human next-generation sequencing data}},
url = {http://www.biomedcentral.com/1756-0500/7/864},
volume = {7},
year = {2014}
}
@incollection{Agrawal2016,
address = {Boston, MA},
author = {Agrawal, Ankit and Choudhary, Alok},
booktitle = {Data and Measures in Health Services Research},
doi = {10.1007/978-1-4899-7673-4_2-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Choudhary - 2016 - Health Services Data Big Data Analytics for Deriving Predictive Healthcare Insights.pdf:pdf},
pages = {1--17},
publisher = {Springer US},
title = {{Health Services Data: Big Data Analytics for Deriving Predictive Healthcare Insights}},
url = {http://link.springer.com/10.1007/978-1-4899-7673-4_2-1},
year = {2016}
}
@misc{Littlefield,
author = {Littlefield, Rayan},
title = {{An introduction into Data Mining in Bioinformatics.}},
url = {https://littlefield.co/an-introduction-into-data-mining-in-bioinformatics-964511e9ea21},
urldate = {2017-11-19}
}
@book{Yunis2002,
author = {{Rodr{\'{i}}guez-Alarc{\'{o}}n G{\'{o}}mez}, J},
booktitle = {An Esp Pediatr},
isbn = {9583504092 9789583504099},
pages = {322--324},
publisher = {Temis S.A.},
title = {{El ADN en la identificaci{\'{o}}n del reci{\'{e}}n nacido}},
volume = {43},
year = {1997}
}
@article{Wang2009,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
doi = {10.1038/nrg2484},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gerstein, Snyder - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {14710056},
journal = {Nature Reviews Genetics},
keywords = {Animals,Base Sequence,Chromosome Mapping,Exons,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Genetic,Molecular Sequence Data,RNA,RNA: analysis,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Transcription, Genetic},
number = {1},
pages = {57--63},
pmid = {19015660},
title = {{RNA-Seq: A revolutionary tool for transcriptomics}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2949280&tool=pmcentrez&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Li2012,
abstract = {Family samples, which can be enriched for rare causal variants by focusing on families with multiple extreme individuals and which facilitate detection of de novo mutation events, provide an attractive resource for next-generation sequencing studies. Here, we describe, implement, and evaluate a likelihood-based framework for analysis of next generation sequence data in family samples. Our framework is able to identify variant sites accurately and to assign individual genotypes, and can handle de novo mutation events, increasing the sensitivity and specificity of variant calling and de novo mutation detection. Through simulations we show explicit modeling of family relationships is especially useful for analyses of low-frequency variants and that genotype accuracy increases with the number of individuals sequenced per family. Compared with the standard approach of ignoring relatedness, our methods identify and accurately genotype more variants, and have high specificity for detecting de novo mutation events. The improvement in accuracy using our methods over the standard approach is particularly pronounced for low-frequency variants. Furthermore the family-aware calling framework dramatically reduces Mendelian inconsistencies and is beneficial for family-based analysis. We hope our framework and software will facilitate continuing efforts to identify genetic factors underlying human diseases.},
author = {Li, Bingshan and Chen, Wei and Zhan, Xiaowei and Busonero, Fabio and Sanna, Serena and Sidore, Carlo and Cucca, Francesco and Kang, Hyun M. and Abecasis, Gon{\c{c}}alo R.},
doi = {10.1371/journal.pgen.1002944},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families.pdf:pdf},
isbn = {1553-7404 (Electronic)\n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {10},
pmid = {23055937},
title = {{A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families}},
volume = {8},
year = {2012}
}
@article{Holzinger2013,
abstract = {BACKGROUND: Professionals in the biomedical domain are confronted with an increasing mass of data. Developing methods to assist professional end users in the field of Knowledge Discovery to identify, extract, visualize and understand useful information from these huge amounts of data is a huge challenge. However, there are so many diverse methods and methodologies available, that for biomedical researchers who are inexperienced in the use of even relatively popular knowledge discovery methods, it can be very difficult to select the most appropriate method for their particular research problem. RESULTS: A web application, called KNODWAT (KNOwledge Discovery With Advanced Techniques) has been developed, using Java on Spring framework 3.1. and following a user-centered approach. The software runs on Java 1.6 and above and requires a web server such as Apache Tomcat and a database server such as the MySQL Server. For frontend functionality and styling, Twitter Bootstrap was used as well as jQuery for interactive user interface operations. CONCLUSIONS: The framework presented is user-centric, highly extensible and flexible. Since it enables methods for testing using existing data to assess suitability and performance, it is especially suitable for inexperienced biomedical researchers, new to the field of knowledge discovery and data mining. For testing purposes two algorithms, CART and C4.5 were implemented using the WEKA data mining framework.},
author = {Holzinger, Andreas and Zupan, Mario},
doi = {10.1186/1471-2105-14-191},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holzinger, Zupan - 2013 - KNODWAT A scientific framework application for testing knowledge discovery methods for the biomedical domain.pdf:pdf},
isbn = {1471-2105 (Linking)},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Data analytics,Knowledge discovery,Methods},
number = {1},
pages = {191},
pmid = {23763826},
publisher = {BioMed Central},
title = {{KNODWAT: A scientific framework application for testing knowledge discovery methods for the biomedical domain}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-191},
volume = {14},
year = {2013}
}
@book{Soames2006,
abstract = {Lasergene's eight modules provide tools that enable users to accomplish each step of sequence analysis, from trimming and assembly of sequence data, to gene discovery, annotation, gene product analysis, sequence similarity searches, sequence alignment, phylogenetic analysis, oligonucleotide primer design, cloning strategies, and publication of the results. The Lasergene software suite provides the functions and customization tools needed so that users can perform analyses the software writers never imagined.},
author = {Burland, Timothy G.},
booktitle = {Bioinformatics Methods and Protocols},
doi = {10.1385/1-59259-192-2:71},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {978-0-89603-732-8},
issn = {1064-3745},
pages = {71--91},
pmid = {10547832},
title = {{DNASTAR's Lasergene Sequence Analysis Software}},
url = {http://link.springer.com/10.1385/1-59259-192-2:71},
volume = {132},
year = {2006}
}
@article{Ignacio2017,
abstract = {Introducci{\'{o}}n : El riesgo de desarrollar c{\'{a}}ncer de seno y c{\'{a}}ncer de ovario puede transmitirse en familias que porten mutaciones en los genes BRCA1 o BRCA2. La detecci{\'{o}}n de estas mutaciones permite tomar decisiones oportunas en el {\'{a}}mbito de la medicina preventiva. Objetivo : Estudiar el espectro de mutaciones en la poblaci{\'{o}}n colombiana y evaluar dos estrategias de detecci{\'{o}}n. Metodos : Se incluyeron en total 853 pacientes con diagn{\'{o}}stico de c{\'{a}}ncer de seno y con solicitud de an{\'{a}}lisis de los genes BRCA1 y BRCA2. Un total de 256 pruebas se analizaron mediante secuencia directa completa de estos genes en Myriad Genetics, y las restantes 597 se estudiaron mediante secuencia parcial basada en mutaciones fundadoras a trav{\'{e}}s de la prueba "Perfil Colombia", implementada por nosotros. Resultados : Se detectaron 107 pacientes portadores de mutaciones en pacientes colombianos, 69 de las cuales estaban localizadas en BRCA1 y 38 en BRCA2. De estas 39 mutaciones son nuevas (22 en BRCA1 y 17 en BRCA2) y solo se hallaron 4 de las 6 mutaciones reportadas previamente como fundadoras en Colombia. En 64/597 pacientes analizados mediante el "Perfil Colombia" se detectaron mutaciones en BRCA1 o BRCA2, as{\'{i}} como en 41/256 pacientes que solicitaron la secuenciaci{\'{o}}n completa de los genes BRCA1 y BRCA2. Conclusiones : El espectro de mutaciones fundadoras en Colombia es m{\'{a}}s amplio que el reportado anteriormente para este pa{\'{i}}s. El "Perfil Colombia" es una prueba que revela a la vez mutaciones fundadoras y mutaciones nuevas, con una tasa de detecci{\'{o}}n del 10.7%. La secuenciaci{\'{o}}n completa presenta una tasa de detecci{\'{o}}n del 16.0% y puede complementar el diagn{\'{o}}stico de la base gen{\'{e}}tica de esta enfermedad.},
author = {Brice{\~{n}}o-Balc{\'{a}}zar, Ignacio and G{\'{o}}mez-Guti{\'{e}}rrez, Alberto and D{\'{i}}az-Duss{\'{a}}n, Natalia Andrea and Noguera-Santamar{\'{i}}a, Mar{\'{i}}a Claudia and D{\'{i}}az-Rinc{\'{o}}n, Diego and Casas-G{\'{o}}mez, Mar{\'{i}}a Consuelo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ignacio et al. - 2017 - Espectro de mutaciones en los genes BRCA1 y BRCA2 asociados a c{\'{a}}ncer de mama en Colombia.pdf:pdf},
issn = {1657-9534},
journal = {Colombia M{\'{e}}dica},
keywords = {BRCA1,BRCA2,Breast cancer,Colombia,diagnostic test,mutation},
month = {jun},
number = {2},
pages = {58--63},
title = {{Colombia médica.}},
url = {http://colombiamedica.univalle.edu.co/index.php/comedica/article/view/1867/3273},
volume = {48},
year = {2017}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas (1)}
}
@article{Stelzer,
abstract = {GeneCards, the human gene compendium, enables researchers to effectively navigate and inter-relate the wide universe of human genes, diseases, variants, proteins, cells, and biological pathways. Our recently launched Version 4 has a revamped infrastructure facilitating faster data updates, better-targeted data queries, and friendlier user experience. It also provides a stronger foundation for the GeneCards suite of companion databases and analysis tools. Improved data unification includes gene-disease links via MalaCards and merged biological pathways via PathCards, as well as drug information and proteome expression. VarElect, another suite member, is a phenotype prioritizer for next-generation sequencing, leveraging the GeneCards and MalaCards knowledgebase. It automatically infers direct and indirect scored associations between hundreds or even thousands of variant-containing genes and disease phenotype terms. VarElect's capabilities, either independently or within TGex, our comprehensive variant analysis pipeline, help prepare for the challenge of clinical projects that involve thousands of exome/genome NGS analyses. {\textcopyright} 2016 by John Wiley & Sons, Inc.},
author = {Stelzer, Gil and Rosen, Naomi and Plaschkes, Inbar and Zimmerman, Shahar and Twik, Michal and Fishilevich, Simon and {Iny Stein}, Tsippi and Nudel, Ron and Lieder, Iris and Mazor, Yaron and Kaplan, Sergey and Dahary, Dvir and Warshawsky, David and Guan-Golan, Yaron and Kohn, Asher and Rappaport, Noa and Safran, Marilyn and Lancet, Doron},
doi = {10.1002/cpbi.5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer et al. - Unknown - The GeneCards Suite From Gene Data Mining to Disease Genome Sequence Analyses.pdf:pdf},
isbn = {9780471250951},
issn = {1934340X},
journal = {Current Protocols in Bioinformatics},
keywords = {Bioinformatics,Biological database,Diseases,Gene prioritization,Genecards,Human genes,Integrated information retrieval,Next generation sequencing,VarElect},
number = {1},
pages = {1.30.1--1.30.33},
pmid = {27322403},
title = {{The GeneCards suite: From gene data mining to disease genome sequence analyses}},
volume = {2016},
year = {2016}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\n\nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\n\nAVAILABILITY: http://maq.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1303.3997},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
eprint = {1303.3997},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Breuer2017,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breuer et al. - 2017 - Genotype-phenotype association mining in bipolar disorder market research meets complex genetics.pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {116624},
publisher = {Cold Spring Harbor Laboratory},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Bao2014,
abstract = {The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigentic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing vaiants and discover gene targets. In this review, we outline the general framework of whole exome processing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis and summarization in the field of bioinformatics.},
author = {Bao, Riyue and Huang, Lei and Andrade, Jorge and Tan, Wei and Kibbe, Warren a and Jiang, Hongmei and Feng, Gang},
doi = {10.4137/CIN.S13779.Received},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bao et al. - 2014 - Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequenci.pdf:pdf},
isbn = {1176-9351 (Electronic)\r1176-9351 (Linking)},
issn = {1176-9351},
journal = {Libertas Academica},
keywords = {10,13,4137,67,82 doi,a,and data management for,and statistical analysis of,applications,bao et al,big data,cancer data,cancer informatics 2014,cin,citation,classification,indel,next generation sequencing,predictive modelling,review of current methods,s13779,s2,sequence alignment,snv,supplement,the bioinformatics analysis of,variant analysis,whole exome sequencing},
pages = {67--82},
pmid = {25288881},
title = {{Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing}},
volume = {13},
year = {2014}
}
@article{Li2014a,
abstract = {Motivation: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods. Results: We made ten SNP and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10-15kb, but the error rate of post-filtered calls is reduced to 1 in 100-200kb without significant compromise on the sensitivity. Availability: BWA-MEM alignment: http://bit.ly/1g8XqRt; Scripts: https://github.com/lh3/varcmp; Additional data: http://figshare.com/account/projects/1013},
archivePrefix = {arXiv},
arxivId = {1404.0929},
author = {Li, Heng and Wren, Jonathan},
doi = {10.1093/bioinformatics/btu356},
eprint = {1404.0929},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2014 - Toward better understanding of artifacts in variant calling from high-coverage samples.pdf:pdf},
isbn = {1367-4803},
issn = {14602059},
journal = {Bioinformatics},
number = {20},
pages = {2843--2851},
pmid = {24974202},
title = {{Toward better understanding of artifacts in variant calling from high-coverage samples}},
volume = {30},
year = {2014}
}
@article{Handl2005,
abstract = {ABSTRACT Motivation: The discovery of novel biological knowledge from the ab initio analysis of post-genomic data relies upon the use of unsupervised processing methods, in particular clustering techniques. Much recent research in bioinformatics has therefore been focused on the transfer of clustering methods introduced in other scientific fields and on the development of novel algorithms specifically designed to tackle the challenges posed by post-genomic data. The partitions returned by a clustering algorithm are commonly validated using visual inspection and concordance with prior biological knowledge-- whether the clusters actually correspond to the real structure in the data is somewhat less frequently considered. Suitable computational cluster validation techniques are available in the general data-mining literature, but have been given only a fraction of the same attention in bioinformatics. Results: This review paper aims to familiarize the reader with the battery of techniques available for the validation of clustering results, with a particular focus on their application to post-genomic data analysis. Synthetic and real biological datasets are used to demonstrate the benefits, and also some of the perils, of analytical cluster validation. Availability: The software used in the experiments is available at http://dbkgroup.org/handl/clustervalidation/ Contact: J.Handl@postgrad.manchester.ac.uk Supplementary information: Enlarged colour plots are provided in the Supplementary Material, which is available at http://dbkgroup.org/ handl/clustervalidation/},
author = {Handl, Julia and Knowles, Joshua and Kell, Douglas B.},
doi = {10.1093/bioinformatics/bti517},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handl, Knowles, Kell - 2005 - Computational cluster validation in post-genomic data analysis.pdf:pdf},
isbn = {1367-4803 (Print)\n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {15},
pages = {3201--3212},
pmid = {15914541},
title = {{Computational cluster validation in post-genomic data analysis}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bti517},
volume = {21},
year = {2005}
}
@article{Palmisano2016,
abstract = {Trials involving genomic-driven treatment selection require the
coordination of many teams interacting with a great variety of
information. The need of better informatics support to manage this
complex set of operations motivated the creation of OpenGeneMed.
OpenGeneMed is a stand-alone and customizable version of GeneMed (Zhao
et al. GeneMed: an informatics hub for the coordination of
next-generation sequencing studies that support precision oncology
clinical trials. Cancer Inform 2015; 14(Suppl 2): 45), a web-based
interface developed for the National Cancer Institute Molecular
Profiling-based Assignment of Cancer Therapy (NCI-MPACT) clinical trial
coordinated by the NIH. OpenGeneMed streamlines clinical trial
management and it can be used by clinicians, lab personnel,
statisticians and researchers as a communication hub. It automates the
annotation of genomic variants identified by sequencing tumor DNA,
classifies the actionable mutations according to customizable rules and
facilitates quality control in reviewing variants. The system generates
summarized reports with detected genomic alterations that a treatment
review team can use for treatment assignment. OpenGeneMed allows
collaboration to happen seamlessly along the clinical pipeline; it helps
reduce errors made transferring data between groups and facilitates
clear documentation along the pipeline. OpenGeneMed is distributed as a
stand-alone virtual machine, ready for deployment and use from a web
browser; its code is customizable to address specific needs of different
clinical trials and research teams. Examples on how to change the code
are provided in the technical documentation distributed with the virtual
machine. In summary, OpenGeneMed offers an initial set of features
inspired by our experience with GeneMed, a system that has been proven
to be efficient and successful for coordinating the application of
next-generation sequencing in the NCI-MPACT trial.},
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/Descargas/palmisano2016.pdf:pdf},
issn = {14774054},
journal = {Briefings in bioinformatics},
keywords = {clinical trial management system,genomics,next-generation sequencing,open source software,precision medicine},
number = {5},
pages = {723--734},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
volume = {18},
year = {2017}
}
@techreport{Henderson2015,
abstract = {Given a collection of m continuous-valued, one-dimensional empirical probability distributions {P1, . . . , Pm}, how can we cluster these distributions efficiently with a nonparamet-ric approach? Such problems arise in many real-world set-tings where keeping the moments of the distribution is not appropriate, because either some of the moments are not defined or the distributions are heavy-tailed or bi-modal. Examples include mining distributions of inter-arrival times and phone-call lengths. We present an efficient algorithm with a non-parametric model for clustering empirical, one-dimensional, continuous probability distributions. Our al-gorithm, called ep-means, is based on the Earth Mover's Distance and k-means clustering. We illustrate the utility of ep-means on various data sets and applications. In par-ticular, we demonstrate that ep-means effectively and ef-ficiently clusters probability distributions of mixed and ar-bitrary shapes, recovering ground-truth clusters exactly in cases where existing methods perform at baseline accuracy. We also demonstrate that ep-means outperforms moment-based classification techniques and discovers useful patterns in a variety of real-world applications.},
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
booktitle = {Sac '15},
doi = {10.1145/2695664.2695860},
isbn = {9781450331968},
keywords = {Empirical Distributions,No,Unsupervised Learning},
pages = {893--900},
title = {{EP-MEANS : An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@inproceedings{mckinney-proc-scipy-2010,
abstract = {In this paper we are concerned with the practical issues of working with data sets common to finance, statistics, and other related fields. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss specific design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
author = {McKinney, Wes},
booktitle = {Proceedings of the 9th Python in Science Conference},
editor = {van der Walt, St{\'{e}}fan and Millman, Jarrod},
isbn = {0440877763224},
issn = {0440877763224},
number = {Scipy},
pages = {51--56},
pmid = {1000224767},
title = {{Data Structures for Statistical Computing in Python}},
url = {http://conference.scipy.org/proceedings/scipy2010/mckinney.html},
volume = {1697900},
year = {2010}
}
@article{Urbanczyk2016,
abstract = {This paper deals with creating the database for the urgent department of the Faculty Hospital of Ostrava. The database is based on the concept of the Internet Of Things. Selected elements are saved to the database using low-cost identifiers that also creates records of use. It is described analysis of the urgent environment and the working process and working environment of the emergency department. It was also necessary to make analysis of each part which will be entered into the process of making electronic registration and medical records as RFID technology, passive tags, WiFi network, etc. It was developed medical database where are stored medical registration and records thanks RFID technology. For the database MySQL database server with client MySQL Workbench 5.2.42.CE application was used. In this paper is also described its implementation and testing's involved.},
author = {Urbanczyk, Tomas and Peter, Lukas},
doi = {10.1016/j.ifacol.2016.12.047},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Urbanczyk, Peter - 2016 - Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Co.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Database,MySQL,RFID,The Urgent department,environment analysis},
number = {25},
pages = {278--283},
publisher = {Elsevier B.V.},
title = {{Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Concept}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2405896316326830},
volume = {49},
year = {2016}
}
@article{Cleary2014,
abstract = {Abstract The analysis of whole-genome or exome sequencing data from trios and pedigrees has been successfully applied to the identification of disease-causing mutations. However, most methods used to identify and genotype genetic variants from next-generation sequencing data ignore the relationships between samples, resulting in significant Mendelian errors, false positives and negatives. Here we present a Bayesian network framework that jointly analyzes data from all members of a pedigree simultaneously using Mendelian segregation priors, yet providing the ability to detect de novo mutations in offspring, and is scalable to large pedigrees. We evaluated our method by simulations and analysis of whole-genome sequencing (WGS) data from a 17-individual, 3-generation CEPH pedigree sequenced to 50× average depth. Compared with singleton calling, our family caller produced more high-quality variants and eliminated spurious calls as judged by common quality metrics such as Ti/Tv, Het/Hom ratios, and dbSNP/SNP array data concordance, and by comparing to ground truth variant sets available for this sample. We identify all previously validated de novo mutations in NA12878, concurrent with a 7× precision improvement. Our results show that our method is scalable to large genomics and human disease studies.},
author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M.},
doi = {10.1089/cmb.2014.0029},
isbn = {1557-8666},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
pmid = {24874280},
title = {{Joint Variant and <i>De Novo</i> Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@article{Lubin2017a,
abstract = {A national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications. This flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants. To provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settings, use of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard to the existing variant file specifications presently used in the clinical setting. Adoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities.},
author = {Lubin, Ira M. and Aziz, Nazneen and Babb, Lawrence J. and Ballinger, Dennis and Bisht, Himani and Church, Deanna M. and Cordes, Shaun and Eilbeck, Karen and Hyland, Fiona and Kalman, Lisa and Landrum, Melissa and Lockhart, Edward R. and Maglott, Donna and Marth, Gabor and Pfeifer, John D. and Rehm, Heidi L. and Roy, Somak and Tezak, Zivana and Truty, Rebecca and Ullman-Cullere, Mollie and Voelkerding, Karl V. and Worthey, Elizabeth A. and Zaranek, Alexander W. and Zook, Justin M.},
doi = {10.1016/j.jmoldx.2016.12.001},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
month = {may},
number = {3},
pages = {417--426},
pmid = {28315672},
publisher = {Elsevier},
title = {{Principles and Recommendations for Standardizing the Use of the Next-Generation Sequencing Variant File in Clinical Settings}},
url = {http://www.sciencedirect.com/science/article/pii/S152515781730106X},
volume = {19},
year = {2017}
}
@article{Kutzera2017,
author = {Kutzera, Joachim and May, Patrick},
doi = {10.1007/978-3-642-15120-0},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Data Integration in the Life Sciences.pdf:pdf},
isbn = {978-3-642-15119-4},
pages = {22--28},
title = {{Data Integration in the Life Sciences}},
url = {http://link.springer.com/10.1007/978-3-642-15120-0},
volume = {6254},
year = {2010}
}
@article{Palmisano2016a,
abstract = {Trials involving genomic-driven treatment selection require the
coordination of many teams interacting with a great variety of
information. The need of better informatics support to manage this
complex set of operations motivated the creation of OpenGeneMed.
OpenGeneMed is a stand-alone and customizable version of GeneMed (Zhao
et al. GeneMed: an informatics hub for the coordination of
next-generation sequencing studies that support precision oncology
clinical trials. Cancer Inform 2015; 14(Suppl 2): 45), a web-based
interface developed for the National Cancer Institute Molecular
Profiling-based Assignment of Cancer Therapy (NCI-MPACT) clinical trial
coordinated by the NIH. OpenGeneMed streamlines clinical trial
management and it can be used by clinicians, lab personnel,
statisticians and researchers as a communication hub. It automates the
annotation of genomic variants identified by sequencing tumor DNA,
classifies the actionable mutations according to customizable rules and
facilitates quality control in reviewing variants. The system generates
summarized reports with detected genomic alterations that a treatment
review team can use for treatment assignment. OpenGeneMed allows
collaboration to happen seamlessly along the clinical pipeline; it helps
reduce errors made transferring data between groups and facilitates
clear documentation along the pipeline. OpenGeneMed is distributed as a
stand-alone virtual machine, ready for deployment and use from a web
browser; its code is customizable to address specific needs of different
clinical trials and research teams. Examples on how to change the code
are provided in the technical documentation distributed with the virtual
machine. In summary, OpenGeneMed offers an initial set of features
inspired by our experience with GeneMed, a system that has been proven
to be efficient and successful for coordinating the application of
next-generation sequencing in the NCI-MPACT trial.},
author = {Palmisano, Alida and Zhao, Yingdong and Li, Ming Chung and Polley, Eric C. and Simon, Richard M.},
doi = {10.1093/bib/bbw059},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmisano et al. - 2016 - OpenGeneMed a portable, flexible and customizable informatics hub for the coordination of next-generation sequ.pdf:pdf},
issn = {14774054},
journal = {Briefings in bioinformatics},
keywords = {clinical trial management system,genomics,next-generation sequencing,open source software,precision medicine},
month = {jul},
number = {5},
pages = {723--734},
publisher = {Oxford University Press},
title = {{OpenGeneMed: a portable, flexible and customizable informatics hub for the coordination of next-generation sequencing studies in support of precision medicine trials}},
url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbw059},
volume = {18},
year = {2017}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
pages = {51--68},
title = {{Salud P{{\'{u}}}blica • Epidemiolog{{\'{i}}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{Jurca2016a,
abstract = {BACKGROUND Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. RESULTS We utilized PubMed for the testing. We investigated gene-gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. CONCLUSIONS Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene-gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
isbn = {1756-0500},
issn = {17560500},
journal = {BMC Research Notes},
keywords = {Breast cancer,Data mining,Network analysis,Text mining},
month = {dec},
number = {1},
pages = {236},
pmid = {27112211},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@article{Davidson2017,
abstract = {Motivation: Ribosomal RNA profiling has become crucial to studying microbial communities, but meaningful taxonomic analysis and inter-comparison of such data are still hampered by technical limitations, between-study design variability and inconsistencies between taxonomies used. Results: Here we present MAPseq, a framework for reference-based rRNA sequence analysis that is up to 30% more accurate (F1/2 score) and up to one hundred times faster than existing solutions, providing in a single run multiple taxonomy classifications and hierarchical OTU mappings, for rRNA sequences in both amplicon and shotgun sequencing strategies, and for datasets of virtually any size. Availability: Source code and binaries are freely available at https://github.com/jfmrod/mapseq Contact:},
archivePrefix = {arXiv},
arxivId = {103549},
author = {{Jo{\~{a}}o F Matias Rodrigues1, Thomas SB Schmidt1}, Janko Tackmann1 and Christian von Mering1},
doi = {10.1093/bioinformatics/xxxxx},
eprint = {103549},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, Oshlack - 2017 - Necklace combining reference and assembled transcriptomes for RNA-Seq analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {May},
pages = {0--0},
pmid = {28407034},
title = {{MAPseq: highly efficient k-mer search with confi- dence estimates, for rRNA sequence analysis}},
url = {https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/PAP/10.1093_bioinformatics_btx400/1/btx400.pdf?Expires=1498600002&Signature=NUtYYcdBpBQNhzgudJGKJAFp6hRQqryxL76tZJuI1v1V7Eh15Dwr9tfCwfud5GIZHEv4qb2hVBrPwLzSGZ8zh2jNkPnFA$\sim$Yd},
year = {2017}
}
@article{DanecekPAutonA2011,
abstract = {SUMMARY: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API. AVAILABILITY: http://vcftools.sourceforge.net},
author = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis a and Banks, Eric and DePristo, Mark a and Handsaker, Robert E and Lunter, Gerton and Marth, Gabor T and Sherry, Stephen T and McVean, Gilean and Durbin, Richard},
doi = {10.1093/bioinformatics/btr330},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danecek et al. - 2011 - The Variant Call Format and VCFtools.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Alleles,Genetic Variation,Genome,Genomics,Genomics: methods,Genotype,Human,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Software},
number = {15},
pages = {2156--8},
pmid = {21653522},
title = {{The variant call format and VCF tools.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3137218&tool=pmcentrez&rendertype=abstract},
volume = {27},
year = {2011}
}
@article{Breuer2018,
author = {Breuer, Ren{\'{e}} and Karypis, George and Murray, Sarah and Berrettini, Wade and Kelsoe, John and Mattheisen, Manuel and Goes, Fernando and Coryell, William and Frank, Josef and Szelinger, Szabolcs and Strohmaier, Jana and Guo, Yirin and M{\"{u}}hleisen, Thomas W. and Krumm, Bertram and Schulze, Thomas G. and Liu, Chunyu and Z{\"{o}}llner, Sebastian and Scheftner, William and Zandi, Peter and Treutlein, Jens and Bloss, Cinnamon and Foroud, Tatiana and Keating, Brendan and Schork, Nicholas and Nwulia, Evaristus and Rietschel, Marcella and Potash, James and Badner, Judith and N{\"{o}}then, Markus M. and Kassem, Layla and Gershon, Elliot and McMahon, Francis J. and Mahon, Pamela and Greenwood, Tiffany and Craig, David and Nievergelt, Caroline and Smith, Erin and Edenberg, Howard and Byerley, William and Koller, Daniel and Degenhardt, Franziska and Zhang, Peng and Lawson, William and Alliey-Rodriguez, Ney and Shekhtman, Tatyana and Herms, Stefan and Cichon, Sven and Hipolito, Maria and Shilling, Paul and Rice, John and Nurnberger, John and McInnis, Melvin},
doi = {10.1186/s40345-018-0132-x},
file = {:home/jennifer/Descargas/40345_2018_Article_132.pdf:pdf},
issn = {2194-7511},
journal = {International Journal of Bipolar Disorders},
keywords = {Bipolar disorder,Subphenotypes,Rule discovery,Data,bipolar disorder,correspondence,data mining,de,genotype,lmu,med,phenotype patterns,rule discovery,subphenotypes,tschulze},
number = {1},
publisher = {Springer Berlin Heidelberg},
title = {{Detecting significant genotype–phenotype association rules in bipolar disorder: market research meets complex genetics}},
url = {https://doi.org/10.1186/s40345-018-0132-x},
volume = {6},
year = {2018}
}
@article{Frazer2009,
abstract = {The last few years have seen extensive efforts to catalogue human genetic variation and correlate it with phenotypic differences. Most common SNPs have now been assessed in genome-wide studies for statistical associations with many complex traits, including many important common diseases. Although these studies have provided new biological insights, only a limited amount of the heritable component of any complex trait has been identified and it remains a challenge to elucidate the functional link between associated variants and phenotypic traits. Technological advances, such as the ability to detect rare and structural variants, and a clear understanding of the challenges in linking different types of variation with phenotype, will be essential for future progress.},
author = {Frazer, Kelly A. and Murray, Sarah S. and Schork, Nicholas J. and Topol, Eric J.},
doi = {10.1038/nrg2554},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frazer et al. - 2009 - Human genetic variation and its contribution to complex traits.pdf:pdf},
isbn = {1471-0064 (Electronic) 1471-0056 (Linking)},
issn = {14710056},
journal = {Nature Reviews Genetics},
number = {4},
pages = {241--251},
pmid = {19293820},
title = {{Human genetic variation and its contribution to complex traits}},
volume = {10},
year = {2009}
}
@article{Bacardit2014,
abstract = {Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented.},
author = {Bacardit, Jaume and Widera, Pawe{\l} and Lazzarini, Nicola and Krasnogor, Natalio},
doi = {10.1089/big.2014.0023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacardit et al. - 2014 - Hard Data Analytics Problems Make for Better Data Analysis Algorithms Bioinformatics as an Example.pdf:pdf},
isbn = {2167-6461},
issn = {2167-6461},
journal = {Big Data},
number = {3},
pages = {164--176},
pmid = {25276500},
title = {{Hard Data Analytics Problems Make for Better Data Analysis Algorithms: Bioinformatics as an Example}},
url = {http://online.liebertpub.com/doi/abs/10.1089/big.2014.0023},
volume = {2},
year = {2014}
}
@article{Fisch2015,
abstract = {Motivation: Omics Pipe (http://sulab.scripps.edu/omicspipe) is a computational framework that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole-Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas (TCGA). Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis. The goal of Omics Pipe is to democratize next-generation sequencing analysis by dramatically increasing the accessibility and reproducibility of best practice computational pipelines, which will enable researchers to generate biologically meaningful and interpretable results.\nResults: Using Omics Pipe, we analyzed 100 TCGA breast invasive carcinoma paired tumor-normal datasets based on the latest UCSC hg19 RefSeq annotation. Omics Pipe automatically downloaded and processed the desired TCGA samples on a high throughput compute cluster to produce a results report for each sample. We aggregated the individual sample results and compared them to the analysis in the original publications. This comparison revealed high overlap between the analyses, as well as novel findings due to the use of updated annotations and methods.\nAvailability and implementation: Source code for Omics Pipe is freely available on the web (https://bitbucket.org/sulab/omics_pipe). Omics Pipe is distributed as a standalone Python package for installation (https://pypi.python.org/pypi/omics_pipe) and as an Amazon Machine Image in Amazon Web Services Elastic Compute Cloud that contains all necessary third-party software dependencies and databases (https://pythonhosted.org/omics_pipe/AWS_installation.html).\nContact: asu@scripps.edu or kfisch@ucsd.edu\nSupplementary Information: Supplementary data are available at Bioinformatics online.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Ren2015,
abstract = {The data explosion in the last decade is revolutionizing diagnostics research and the healthcare industry, offering both opportunities and challenges. These high-throughput "omics" techniques have generated more scientific data in the last few years than in the entire history of mankind. Here we present a brief summary of how "big data" have influenced early diagnosis of complex diseases. We will also review some of the most commonly used "omics" techniques and their applications in diagnostics. Finally, we will discuss the issues brought by these new techniques when translating laboratory discoveries to clinical practice.},
author = {Ren, Guomin and Krawetz, Roman},
doi = {10.3109/1354750X.2015.1105499},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Krawetz - 2015 - Applying computation biology and &quotbig data&quot to develop multiplex diagnostics for complex chronic diseases.pdf:pdf},
issn = {13665804},
journal = {Biomarkers},
keywords = {Chronic disease,Computational biology,Diagnostics,Osteoarthritis},
number = {8},
pages = {533--539},
pmid = {26809774},
publisher = {Taylor & Francis},
title = {{Applying computation biology and "big data" to develop multiplex diagnostics for complex chronic diseases such as osteoarthritis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26809774 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822 http://www.ncbi.nlm.nih.gov/pubmed/26809774%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4819822},
volume = {20},
year = {2015}
}
@article{La2012,
abstract = {Ejercicio de citas en Mendeley},
author = {Notario, Silvia},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
journal = {Seminario de tesis 1},
pages = {1--2},
title = {{Ejercicio de citas con Mendeley}},
year = {2016}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F. and Dolezal, Marlies A. and Koltes, James E. and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J. and Reecy, James M. and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
isbn = {1471-2164},
issn = {14712164},
journal = {BMC Genomics},
keywords = {Next-generation sequencing analysis,Pipeline,Single nucleotide variant identification},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@inproceedings{Hu2011,
abstract = {In this talk, I will discuss some of the latest data mining techniques and methods and their applications in bioinformatics study, focusing on data integration, text mining and graph-based data mining in bioinformatics research. In data integration, I will present a semantic-based approach for multi source bioinformatics data integration. In our approach, a metamodel is utilized to represent the master search schema, and an effective interface extraction algorithm based on the hierarchical structure of the web and pattern is developed to capture the rich semantic relationships of the online bioinformatics data sources. Our final goal is to develop a meta-search interface for biologists as a single point of access to multiple online bioinformatics databases. In text mining, some of the challenging issues in mining and searching the biomedical literature are addressed, and I will present a unified architecture Bio-SET-DM (Biomedical Literature Searching, Extraction and Text Data Mining), discuss some novel algorithms such as semantic-based language model for literature retrieval, semi-supervised pattern learning for information extraction of biological relationships from biomedical literature. In the third part, graph-based data mining, the focus is on graph-based mining in biological networks. I will discuss how to apply graph-based mining techniques and algorithms in the analysis of modular and hierarchical structure of biological networks, how to identify and evaluate the subnetworks from complicated biological networks, and present the experimental results. To put these pieces together, a unified framework is introduced to integrate the three parts (data integration, text mining and graph-based data mining) in the bioinformatics data mining procedure.},
author = {Hu, Xiaohua},
booktitle = {2011 IEEE International Conference on Granular Computing},
doi = {10.1109/GRC.2011.6122559},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu - 2011 - Data mining and its applications in bioinformatics Techniques and methods.pdf:pdf},
isbn = {978-1-4577-0371-3},
month = {nov},
pages = {3--3},
publisher = {IEEE},
title = {{Data mining and its applications in bioinformatics: Techniques and methods}},
url = {http://ieeexplore.ieee.org/document/6122559/},
year = {2011}
}
@article{Li2015,
abstract = {BACKGROUND Conditions associated with sudden cardiac arrest/death (SCA/D) in youth often have a genetic etiology. While SCA/D is uncommon, a pro-active family screening approach may identify these inherited structural and electrical abnormalities prior to symptomatic events and allow appropriate surveillance and treatment. This study investigated the diagnostic utility of exome sequencing (ES) by evaluating the capture and coverage of genes related to SCA/D. METHODS Samples from 102 individuals (13 with known molecular etiologies for SCA/D, 30 individuals without known molecular etiologies for SCA/D and 59 with other conditions) were analyzed following exome capture and sequencing at an average read depth of 100X. Reads were mapped to human genome GRCh37 using Novoalign, and post-processing and analysis was done using Picard and GATK. A total of 103 genes (2,190 exons) related to SCA/D were used as a primary filter. An additional 100 random variants within the targeted genes associated with SCA/D were also selected and evaluated for depth of sequencing and coverage. Although the primary objective was to evaluate the adequacy of depth of sequencing and coverage of targeted SCA/D genes and not for primary diagnosis, all patients who had SCA/D (known or unknown molecular etiologies) were evaluated with the project's variant analysis pipeline to determine if the molecular etiologies could be successfully identified. RESULTS The majority of exons (97.6 %) were captured and fully covered on average at minimum of 20x sequencing depth. The proportion of unique genomic positions reported within poorly covered exons remained small (4 %). Exonic regions with less coverage reflect the need to enrich these areas to improve coverage. Despite limitations in coverage, we identified 100 % of cases with a prior known molecular etiology for SCA/D, and analysis of an additional 30 individuals with SCA/D but no known molecular etiology revealed a diagnostic answer in 5/30 (17 %). We also demonstrated 95 % of 100 randomly selected reported variants within our targeted genes would have been picked up on ES based on our coverage analysis. CONCLUSIONS ES is a helpful clinical diagnostic tool for SCA/D given its potential to successfully identify a molecular diagnosis, but clinicians should be aware of limitations of available platforms from technical and diagnostic perspectives.},
author = {Li, Mindy H. and Abrudan, Jenica L. and Dulik, Matthew C. and Sasson, Ariella and Brunton, Joshua and Jayaraman, Vijayakumar and Dugan, Noreen and Haley, Danielle and Rajagopalan, Ramakrishnan and Biswas, Sawona and Sarmady, Mahdi and DeChene, Elizabeth T. and Deardorff, Matthew A. and Wilkens, Alisha and Noon, Sarah E. and Scarano, Maria I. and Santani, Avni B. and White, Peter S. and Pennington, Jeffrey and Conlin, Laura K. and Spinner, Nancy B. and Krantz, Ian D. and Vetter, Victoria L.},
doi = {10.1186/s40246-015-0038-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sud.pdf:pdf},
issn = {14797364},
journal = {Human genomics},
number = {1},
pages = {15},
pmid = {26187847},
publisher = {Human Genomics},
title = {{Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sudden cardiac arrest/sudden cardiac death}},
url = {http://www.humgenomics.com/content/9/1/15/abstract%5Cnhttp://www.humgenomics.com/content/9/1/15%5Cnhttp://www.humgenomics.com/content/pdf/s40246-015-0038-y.pdf},
volume = {9},
year = {2015}
}
@article{Pietrelli2017,
abstract = {Next-generation sequencing technologies have become the most powerful tool to discover genetic variants associated with human diseases. Although the dramatic reductions in the costs facilitate the use in the wet-lab and clinics, the huge amount of data generated renders their management by non-expert researchers and physicians extremely difficult. Therefore, there is an urgent need of novel approaches and tools aimed at getting the 'end-users' closer to the sequencing data, facilitating the access by non-bioinformaticians, and to speed-up the functional interpretation of genetic variants. We developed myVCF, a standalone, easy-to-use desktop application, which is based on a browser interface and is suitable for Windows, Mac and UNIX systems. myVCF is an efficient platform that is able to manage multiple sequencing projects created from VCF files within the system; stores genetic variants and samples genotypes from an annotated VCF files into a SQLite database; implements a flexible search engine for data exploration, allowing to query for chromosomal region, gene, single variant or dbSNP ID. Besides, myVCF generates a summary statistics report about mutations distribution across samples and across the genome/exome by aggregating the information within the VCF file. In summary, the myVCF platform allows end-users without strong programming and bioinformatics skills to explore, query, visualize and export mutations data in a simple and straightforward way. https://apietrelli.github.io/myVCF/. pietrelli@ingm.org. Supplementary data are available at Bioinformatics online.},
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
pmid = {29036298},
publisher = {Oxford University Press},
title = {{MyVCF: A desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@article{VanderSpoel2015,
abstract = {Kajian perbandingan pengukuran kecekapan pengurusan kewangan institusi Majlis Agama Islam Negeri (MAIN) dengan prestasi agihan kewangan dan bukan kewangan di antara institusi zakat boleh membuka ruang untuk meningkatkan mutu agihan institusi zakat. Pengurusan kewangan sesebuah organisasi adalah penting untuk memastikan institusi tersebut dapat mencapai matlamat yang telah ditentukan. Hal ini kerana masih wujud tanggapan negatif ketidakcekapan agihan institusi zakat dan ia merupakan antara salah satu punca yang telah menjejaskan tahap keyakinan masyarakat Islam untuk menjalankan kewajipan membayar zakat kepada institusi zakat. Ketidakcekapan agihan tersebut boleh dilihat melalui lebihan zakat yang tidak diagihkan dan kegagalan institusi zakat mengagihkan zakat kepada kelapan-lapan golongan asnaf dan tidak mengikut keutamaan asnaf. Persoalannya adakah wujud hubungan antara prestasi pengurusan kewangan dan prestasi pengagihan zakat. Kajian telah mengkategorikan prestasi pengurusan kewangan kepada tiga (3) bahagian iaitu kecairan, kesolvenan dan keuntungan; manakala aspek prestasi kecekapan agihan institusi zakat pula terbahagi kepada dua (2) bahagian iaitu kecekapan agihan berbentuk kewangan (lebihan zakat tahunan) dan bukan kewangan (keutamaan asnaf). Kajian ini juga mengkaji corak agihan zakat kepada setiap asnaf sebagai satu tambahan pengukuran prestasi agihan zakat. Data sekunder terdiri daripada laporan tahunan MAIN telah dianalisis bermula tahun 2000 hingga 2013. Kajian ini mengkaji lima institusi MAIN iaitu Majlis Agama Islam Selangor (MAIS) dan Majlis Agama Islam Negeri Pulau Pinang (MAINPP) yang mewakili institusi yang telah mengkorporatkan kutipan dan agihan zakat; dan Majlis Agama Islam Johor (MAIJ), Majlis Agama Islam dan Adat Melayu Terengganu (MAIDAM) dan Majlis Ugama Islam Sabah (MUIS) yang mewakili negeri yang tidak mengkorporatkan kutipan dan agihan zakat. Hasil kajian mendapati secara umumnya wujud hubungan yang sepadan antara prestasi pengurusan kewangan MAIN dengan prestasi pengagihan zakat dalam aspek agihan kewangan dan bukan kewangan. Hasil kajian juga mendapati corak agihan zakat oleh institusi korporat adalah berbeza dengan corak agihan zakat oleh institusi yang tidak korporat pengurusan zakat. Beberapa cadangan dan implikasi dasar turut dicadangkan dalam kajian ini. ABSTRACT},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wahid, Hairunnizam and Ahmad, Sanep and Nor, Mohd Ali Mohd and Rashid, Maryam Abd},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {01261962},
journal = {Jurnal Ekonomi Malaysia},
keywords = {Financial management efficiency performance,State Islamic Religious Council,Zakat distribution efficiency},
number = {2},
pages = {39--54},
pmid = {25246403},
title = {{Prestasi kecekapan pengurusan kewangan dan agihan zakat: perbandingan antara majlis agama islam negeri di Malaysia}},
volume = {51},
year = {2017}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192%7B&%7Dtool=pmcentrez%7B&%7Drendertype=abstract},
volume = {9},
year = {2014}
}
@article{Farid2016,
abstract = {In this paper, we introduce a new adaptive rule-based classifier for multi-class classification of biological data, where several problems of classifying biological data are addressed: overfitting, noisy instances and class-imbalance data. It is well known that rules are interesting way for representing data in a human interpretable way. The proposed rule-based classifier combines the random subspace and boosting approaches with ensemble of decision trees to construct a set of classification rules without involving global optimisation. The classifier considers random subspace approach to avoid overfitting, boosting approach for classifying noisy instances and ensemble of decision trees to deal with class-imbalance problem. The classifier uses two popular classification techniques: decision tree and k-nearest-neighbor algorithms. Decision trees are used for evolving classification rules from the training data, while k-nearest-neighbor is used for analysing the misclassified instances and removing vagueness between the contradictory rules. It considers a series of k iterations to develop a set of classification rules from the training data and pays more attention to the misclassified instances in the next iteration by giving it a boosting flavour. This paper particularly focuses to come up with an optimal ensemble classifier that will help for improving the prediction accuracy of DNA variant identification and classification task. The performance of proposed classifier is tested with compared to well-approved existing machine learning and data mining algorithms on genomic data (148 Exome data sets) of Brugada syndrome and 10 real benchmark life sciences data sets from the UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed classifier has exemplary classification accuracy on different types of biological data. Overall, the proposed classifier offers good prediction accuracy to new DNA variants classification where noisy and misclassified variants are optimised to increase test performance.},
annote = {NULL},
author = {Farid, Dewan Md and Al-Mamun, Mohammad Abdullah and Manderick, Bernard and Nowe, Ann},
doi = {10.1016/j.eswa.2016.08.008},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farid et al. - 2016 - An adaptive rule-based classifier for mining big biological data.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Brugada syndrome,Classification,Decision tree,Genomic data,Rule-based classifier},
pages = {305--316},
title = {{An adaptive rule-based classifier for mining big biological data}},
volume = {64},
year = {2016}
}
@article{Zhou2013,
abstract = {Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7-8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis. QC-Chain is publicly available at: http://www.computationalbioenergy.org/qc-chain.html.},
author = {Zhou, Qian and Su, Xiaoquan and Wang, Anhui and Xu, Jian and Ning, Kang},
doi = {10.1371/journal.pone.0060234},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2013 - QC-Chain Fast and Holistic Quality Control Method for Next-Generation Sequencing Data.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {23565205},
title = {{QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data}},
volume = {8},
year = {2013}
}
@article{Hannah-Shmouni2015,
abstract = {The causes of heart failure are diverse. Inherited causes represent an important clinical entity and can be divided into 2 major categories: familial and metabolic cardiomyopathies. The distinct features that might be present in early disease states can become broadly overlapping with other diseases, such as in the case of inherited cardiomyopathies (ie, familial hypertrophic cardiomyopathy or mitochondrial diseases). In this review article, we focus on genetic issues related to advanced heart failure. Because of the emerging importance of this topic and its breadth, we sought to focus our discussion on the known genetic forms of heart failure syndromes, genetic testing, and newer data on pharmacogenetics and therapeutics in the treatment of heart failure, to primarily encourage clinicians to place a priority on the diagnosis and treatment of these potentially treatable conditions.},
author = {Hannah-Shmouni, Fady and Seidelmann, Sara B. and Sirrs, Sandra and Mani, Arya and Jacoby, Daniel},
doi = {10.1016/j.cjca.2015.07.735},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannah-Shmouni et al. - 2015 - The Genetic Challenges and Opportunities in Advanced Heart Failure.pdf:pdf},
isbn = {1203785291},
issn = {0828282X},
journal = {Canadian Journal of Cardiology},
number = {11},
pages = {1338--1350},
publisher = {Elsevier Ltd},
title = {{The Genetic Challenges and Opportunities in Advanced Heart Failure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0828282X15013161},
volume = {31},
year = {2015}
}
@article{Yates2016,
abstract = {The Ensembl project (http://www.ensembl.org) is a system for genome annotation, analysis, storage and dissemination designed to facilitate the access of genomic annotation from chordates and key model organisms. It provides access to data from 87 species across our main and early access Pre! websites. This year we introduced three newly annotated species and released numerous updates across our supported species with a concentration on data for the latest genome assemblies of human, mouse, zebrafish and rat. We also provided two data updates for the previous human assembly, GRCh37, through a dedicated website (http://grch37.ensembl.org). Our tools, in particular the VEP, have been improved significantly through integration of additional third party data. REST is now capable of larger-scale analysis and our regulatory data BioMart can deliver faster results. The website is now capable of displaying long-range interactions such as those found in cis-regulated datasets. Finally we have launched a website optimized for mobile devices providing views of genes, variants and phenotypes. Our data is made available without restriction and all code is available from our GitHub organization site (http://github.com/Ensembl) under an Apache 2.0 license.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yates, Andrew and Akanni, Wasiu and Amode, M. Ridwan and Barrell, Daniel and Billis, Konstantinos and Carvalho-Silva, Denise and Cummins, Carla and Clapham, Peter and Fitzgerald, Stephen and Gil, Laurent and Gir{\'{o}}n, Carlos Garc{\'{i}}a and Gordon, Leo and Hourlier, Thibaut and Hunt, Sarah E. and Janacek, Sophie H. and Johnson, Nathan and Juettemann, Thomas and Keenan, Stephen and Lavidas, Ilias and Martin, Fergal J. and Maurel, Thomas and McLaren, William and Murphy, Daniel N. and Nag, Rishi and Nuhn, Michael and Parker, Anne and Patricio, Mateus and Pignatelli, Miguel and Rahtz, Matthew and Riat, Harpreet Singh and Sheppard, Daniel and Taylor, Kieron and Thormann, Anja and Vullo, Alessandro and Wilder, Steven P. and Zadissa, Amonida and Birney, Ewan and Harrow, Jennifer and Muffato, Matthieu and Perry, Emily and Ruffier, Magali and Spudich, Giulietta and Trevanion, Stephen J. and Cunningham, Fiona and Aken, Bronwen L. and Zerbino, Daniel R. and Flicek, Paul},
doi = {10.1093/nar/gkv1157},
eprint = {arXiv:1011.1669v3},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Research},
month = {jan},
number = {D1},
pages = {D710--D716},
pmid = {26687719},
publisher = {Oxford University Press},
title = {{Ensembl 2016}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26687719 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4702834},
volume = {44},
year = {2016}
}
@article{Biesecker2014,
abstract = {S equencing of the genome or exome for clinical applications, hereafter referred to as clinical genome and exome sequencing (CGES), has now entered medical practice. 1 Several thousand CGES tests have already been ordered for patients, with the goal of establishing diagnoses for rare, clini-cally unrecognizable, or puzzling disorders that are suspected to be genetic in ori-gin. We anticipate increases in the use of CGES, the key attribute of which — its breadth — distinguishes it from other forms of laboratory testing. The interroga-tion of variation in about 20,000 genes simultaneously can be a powerful and effec-tive diagnostic method. 2 CGES has been hailed as an important tool in the implementation of predictive and individualized medicine, and there is intense research interest in the clinical benefits and risks of sequencing for screening healthy persons 3 ; however, current practice recommendations 4 do not support the use of sequencing for this purpose, and for that reason we do not further address it here. We have also limited this overview of CGES to the analysis of germline sequence variants for diagnostic purposes and do not discuss the use of CGES to uncover somatic variants in can-cer in order to individualize cancer therapy. Clinicians should understand the diagnostic indications for CGES so that they can effectively deploy it in their practices. Because the success rate of CGES for the identification of a causative variant is approximately 25%, 5 it is important to un-derstand the basis of this testing and how to select the patients most likely to benefit from it. Here, we summarize the technologies underlying CGES and offer our insights into how clinicians should order such testing, interpret the results, and communicate the results to their patients (an interactive graphic giving an overview of the process is available with the full text of this article at NEJM.org). Technic a l Ov erv iew a nd Limitations of CGES},
author = {Biesecker, Leslie G. and Green, Robert C.},
doi = {10.1056/NEJMra1312543},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesecker, Green - 2014 - Diagnostic Clinical Genome and Exome Sequencing.pdf:pdf},
isbn = {1533-4406 (Electronic)\r0028-4793 (Linking)},
issn = {0028-4793},
journal = {New England Journal of Medicine},
keywords = {Exome,Genetic Counseling,Genetic Diseases, Inborn,Genetic Diseases, Inborn: diagnosis,Genetic Diseases, Inborn: genetics,Genetic Testing,Genome, Human,Humans,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {25},
pages = {2418--2425},
pmid = {24941179},
title = {{Diagnostic Clinical Genome and Exome Sequencing}},
url = {http://www.nejm.org/doi/10.1056/NEJMra1312543},
volume = {370},
year = {2014}
}
@article{Trapnell2009,
abstract = {MOTIVATION: A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.\n\nRESULTS: We mapped the RNA-Seq reads from a recent mammalian RNA-Seq experiment and recovered more than 72% of the splice junctions reported by the annotation-based software from that study, along with nearly 20,000 previously unreported junctions. The TopHat pipeline is much faster than previous systems, mapping nearly 2.2 million reads per CPU hour, which is sufficient to process an entire RNA-Seq experiment in less than a day on a standard desktop computer. We describe several challenges unique to ab initio splice site discovery from RNA-Seq reads that will require further algorithm development.\n\nAVAILABILITY: TopHat is free, open-source software available from http://tophat.cbcb.umd.edu.\n\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L.},
doi = {10.1093/bioinformatics/btp120},
eprint = {9605103},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trapnell, Pachter, Salzberg - 2009 - TopHat Discovering splice junctions with RNA-Seq.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {9},
pages = {1105--1111},
pmid = {19289445},
primaryClass = {cs},
title = {{TopHat: Discovering splice junctions with RNA-Seq}},
volume = {25},
year = {2009}
}
@article{Niroula2016,
abstract = {Next generation sequencing (NGS) methods have revolutionized the speed of generating variation information. Sequence data have a plethora of applications and will increasingly be used for disease diagnosis. Interpretation of the identified variants is usually not possible with experimental methods. This has caused a bottleneck that many computational methods aim at addressing. Fast and efficient methods for explaining the significance and mechanisms of detected variants are required for efficient precision/personalized medicine. Computational prediction methods have been developed in three areas to address the issue. There are generic tolerance (pathogenicity) predictors for filtering harmful variants. Gene/protein/disease-specific tools are available for some applications. Mechanism and effect-specific computer programs aim at explaining the consequences of variations. Here, we discuss the different types of predictors and their applications. We review available variation databases and prediction methods useful for variation interpretation. We discuss how the performance of methods is assessed and summarize existing assessment studies. A brief introduction is provided to the principles of the methods developed for variation interpretation as well as guidelines for how to choose the optimal tools and where the field is heading in the future. This article is protected by copyright. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Niroula, Abhishek and Vihinen, Mauno},
doi = {10.1002/humu.22987},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niroula, Vihinen - 2016 - Variation Interpretation Predictors Principles, Types, Performance and Choice.pdf:pdf},
isbn = {9788578110796},
issn = {10981004},
journal = {Human Mutation},
keywords = {Computational tools,Mutation effect prediction,Prediction methods,Variation effect,Variation interpretation,Variation prediction},
month = {mar},
number = {6},
pages = {579--597},
pmid = {26987456},
title = {{Variation Interpretation Predictors: Principles, Types, Performance, and Choice}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26987456},
volume = {37},
year = {2016}
}
@article{Breuer2017b,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breuer et al. - 2017 - Genotype-phenotype association mining in bipolar disorder market research meets complex genetics(2).pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {116624},
publisher = {Cold Spring Harbor Laboratory},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@article{Guo2013,
abstract = {Advances in next-generation sequencing (NGS) technologies have greatly improved our ability to detect genomic variants for biomedical research. In particular, NGS technologies have been recently applied with great success to the discovery of mutations associated with the growth of various tumours and in rare Mendelian diseases. The advance in NGS technologies has also created significant challenges in bioinformatics. One of the major challenges is quality control of the sequencing data. In this review, we discuss the proper quality control procedures and parameters for Illumina technology-based human DNA re-sequencing at three different stages of sequencing: raw data, alignment and variant calling. Monitoring quality control metrics at each of the three stages of NGS data provides unique and independent evaluations of data quality from differing perspectives. Properly conducting quality control protocols at all three stages and correctly interpreting the quality control results are crucial to ensure a successful and meaningful study.},
author = {Guo, Yan and Ye, Fei and Sheng, Quanghu and Clark, Travis and Samuels, David C.},
doi = {10.1093/bib/bbt069},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2013 - Three-stage quality control strategies for DNA re-sequencing data.pdf:pdf},
isbn = {1477-4054 (Electronic)\r1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Alignment,FASTQ,Quality control,Sequencing,Variant calling},
number = {6},
pages = {879--889},
pmid = {24067931},
title = {{Three-stage quality control strategies for DNA re-sequencing data}},
volume = {15},
year = {2013}
}
@article{Lescai2014,
abstract = {The choice of an appropriate variant calling pipeline for exome sequencing data is becoming increasingly more important in translational medicine projects and clinical contexts. Within GOSgene, which facilitates genetic analysis as part of a joint effort of the University College London and the Great Ormond Street Hospital, we aimed to optimize a variant calling pipeline suitable for our clinical context. We implemented the GATK/Queue framework and evaluated the performance of its two callers: the classical UnifiedGenotyper and the new variant discovery tool HaplotypeCaller. We performed an experimental validation of the loss-of-function (LoF) variants called by the two methods using Sequenom technology. UnifiedGenotyper showed a total validation rate of 97.6% for LoF single-nucleotide polymorphisms (SNPs) and 92.0% for insertions or deletions (INDELs), whereas HaplotypeCaller was 91.7% for SNPs and 55.9% for INDELs. We confirm that GATK/Queue is a reliable pipeline in translational medicine and clinical context. We conclude that in our working environment, UnifiedGenotyper is the caller of choice, being an accurate method, with a high validation rate of error-prone calls like LoF variants. We finally highlight the importance of experimental validation, especially for INDELs, as part of a standard pipeline in clinical environments.},
author = {Lescai, Francesco and Marasco, Elena and Bacchelli, Chiara and Stanier, Philip and Mantovani, Vilma and Beales, Philip},
doi = {10.1002/mgg3.42},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lescai et al. - 2014 - Identification and validation of loss of function variants in clinical contexts.pdf:pdf},
issn = {23249269},
journal = {Molecular Genetics & Genomic Medicine},
keywords = {gatk,pipelines,sequencing,variant calling},
number = {1},
pages = {58--63},
pmid = {24498629},
title = {{Identification and validation of loss of function variants in clinical contexts}},
url = {http://doi.wiley.com/10.1002/mgg3.42},
volume = {2},
year = {2014}
}
@article{Bustos2007,
abstract = {El presente art{\'{i}}culo muestra los resultados de la investigaci{\'{o}}n en la cual se aplic{\'{o}} la metodolog{\'{i}}a Shainin del dise{\~{n}}o experimental en la planta de producci{\'{o}}n de un ingenio azucarero del Valle del Cauca. Este trabajo destaca la importancia que tiene el Dise{\~{n}}o Experimental como herramienta estad{\'{i}}stica para el mejoramiento de procesos productivos, que va m{\'{a}}s all{\'{a}} del simple monitoreo impuesto por las t{\'{e}}cnicas de control estad{\'{i}}stico de procesos, sin desmeritarlas como herramientas {\'{u}}tiles para controlar su rendimiento},
author = {Bustos, Ligia and Moreno, Ricardo and Duque, Nestor},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/SPARC (Organization), Universidad Tecnológica de Pereira., Duque - 1995 - Scientia et technica.pdf:pdf},
issn = {0122-1701},
journal = {Scientia},
keywords = {catequina,condensados,fenoles,florotaninos,s polifenoles,taninos,taninos hidrolizables,{\'{a}}cido el{\'{a}}gico,{\'{a}}cido g{\'{a}}lico},
number = {037},
pages = {13--18},
publisher = {Universidad Tecnológica de Pereira},
title = {{Modelo de una bodega de datos para el soporte a la investigaci{\'{o}}n bioinform{\'{a}}tica}},
url = {http://www.redalyc.org/resumen.oa?id=84922625025%0Ahttp://revistas.utp.edu.co/index.php/revistaciencia/article/view/4125/2181%0Ahttp://redalyc.uaemex.mx/src/inicio/ArtPdfRed.jsp?iCve=84903790},
volume = {XIII},
year = {2007}
}
@article{Terlizzi2017a,
abstract = {BACKGROUND The effect of complex alleles in cystic fibrosis (CF) is poorly defined for the lack of functional studies. OBJECTIVES To describe the genotype-phenotype correlation and the results of either in vitro and ex vivo studies performed on nasal epithelial cells (NEC) in a cohort of patients with CF carrying cystic fibrosis transmembrane conductance regulator (CFTR) complex alleles. METHODS We studied 70 homozygous, compound heterozygous or heterozygous for CFTR mutations: p.[Arg74Trp;Val201Met;Asp1270Asn], n=8; p.[Ile148Thr;Ile1023_Val1024del], n=5; p.[Arg117Leu;Leu997Phe], n=6; c.[1210-34TG[12];1210-12T[5];2930C>T], n=3; p.[Arg74Trp;Asp1270Asn], n=4; p.Asp1270Asn, n=2; p.Ile148Thr, n=6; p.Leu997Phe, n=36. In 39 patients, we analysed the CFTR gating activity on NEC in comparison with patients with CF (n=8) and carriers (n=4). Finally, we analysed in vitro the p.[Arg74Trp;Val201Met;Asp1270Asn] complex allele. RESULTS The p.[Ile148Thr;Ile1023_Val1024del] caused severe CF in five compound heterozygous with a class I-II mutation. Their CFTR activity on NEC was comparable with patients with two class I-II mutations (mean 7.3% vs 6.9%). The p.[Arg74Trp;Asp1270Asn] and the p.Asp1270Asn have scarce functional effects, while p.[Arg74Trp;Val201Met;Asp1270Asn] caused mild CF in four of five subjects carrying a class I-II mutation in trans, or CFTR-related disorders (CFTR-RD) in three having in trans a class IV-V mutation. The p.[Arg74Trp;Val201Met;Asp1270Asn] causes significantly (p<0.001) higher CFTR activity compared with compound heterozygous for class I-II mutations. Furthermore, five of six compounds heterozygous with the p.[Arg117Leu;Leu997Phe] had mild CF, whereas the p.Leu997Phe, in trans with a class I-II CFTR mutation, caused CFTR-RD or a healthy status (CFTR activity: 21.3-36.9%). Finally, compounds heterozygous for the c.[1210-34TG[12];1210-12T[5];2930C>T] and a class I-II mutation had mild CF or CFTR-RD (gating activity: 18.5-19.0%). CONCLUSIONS The effect of complex alleles partially depends on the mutation in trans. Although larger studies are necessary, the CFTR activity on NEC is a rapid contributory tool to classify patients with CFTR dysfunction.},
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and {Di Lullo}, Antonella Miriam and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Sofia, Valentina Maria and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
issn = {14686244},
journal = {Journal of Medical Genetics},
keywords = {[I148T;3199del6bp],[L997F;R117L],[R74W;V201M;D1270N],gating activity,nasal brushing},
month = {apr},
number = {4},
pages = {224--235},
pmid = {27738188},
title = {{Genotype-phenotype correlation and functional studies in patients with cystic fibrosis bearing CFTR complex alleles}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27738188 http://jmg.bmj.com/lookup/doi/10.1136/jmedgenet-2016-103985},
volume = {54},
year = {2017}
}
@article{Higasa2016,
abstract = {Whole-genome and -exome resequencing using next-generation sequencers is a powerful approach for identifying genomic variations that are associated with diseases. However, systematic strategies for prioritizing causative variants from many candidates to explain the disease phenotype are still far from being established, because the population-specific frequency spectrum of genetic variation has not been characterized. Here, we have collected exomic genetic variation from 1208 Japanese individuals through a collaborative effort, and aggregated the data into a prevailing catalog. In total, we identified 156 622 previously unreported variants. The allele frequencies for the majority (88.8%) were lower than 0.5% in allele frequency and predicted to be functionally deleterious. In addition, we have constructed a Japanese-specific major allele reference genome by which the number of unique mapping of the short reads in our data has increased 0.045% on average. Our results illustrate the importance of constructing an ethnicity-specific reference genome for identifying rare variants. All the collected data were centralized to a newly developed database to serve as useful resources for exploring pathogenic variations. Public access to the database is available at INTRODUCTION Next-generation sequencing technologies are revolutionizing the approach in identifying genetic variants that are associated with diseases. A current promising strategy focuses on rare variants that},
author = {Higasa, Koichiro and Miyake, Noriko and Yoshimura, Jun and Okamura, Kohji and Niihori, Tetsuya and Saitsu, Hirotomo and Doi, Koichiro and Shimizu, Masakazu and Nakabayashi, Kazuhiko and Aoki, Yoko and Tsurusaki, Yoshinori and Morishita, Shinichi and Kawaguchi, Takahisa and Migita, Osuke and Nakayama, Keiko and Nakashima, Mitsuko and Mitsui, Jun and Narahara, Maiko and Hayashi, Keiko and Funayama, Ryo and Yamaguchi, Daisuke and Ishiura, Hiroyuki and Ko, Wen Ya and Hata, Kenichiro and Nagashima, Takeshi and Yamada, Ryo and Matsubara, Yoichi and Umezawa, Akihiro and Tsuji, Shoji and Matsumoto, Naomichi and Matsuda, Fumihiko},
doi = {10.1038/jhg.2016.12},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Higasa et al. - 2016 - Human genetic variation database, a reference database of genetic variations in the Japanese population.pdf:pdf},
issn = {1435232X},
journal = {Journal of Human Genetics},
number = {6},
pages = {547--553},
pmid = {26911352},
publisher = {Nature Publishing Group},
title = {{Human genetic variation database, a reference database of genetic variations in the Japanese population}},
url = {http://www.nature.com/doifinder/10.1038/jhg.2016.12},
volume = {61},
year = {2016}
}
@techreport{Henderson2015,
abstract = {Given a collection of m continuous-valued, one-dimensional empirical probability distributions {P1, . . . , Pm}, how can we cluster these distributions efficiently with a nonparamet-ric approach? Such problems arise in many real-world set-tings where keeping the moments of the distribution is not appropriate, because either some of the moments are not defined or the distributions are heavy-tailed or bi-modal. Examples include mining distributions of inter-arrival times and phone-call lengths. We present an efficient algorithm with a non-parametric model for clustering empirical, one-dimensional, continuous probability distributions. Our al-gorithm, called ep-means, is based on the Earth Mover's Distance and k-means clustering. We illustrate the utility of ep-means on various data sets and applications. In par-ticular, we demonstrate that ep-means effectively and ef-ficiently clusters probability distributions of mixed and ar-bitrary shapes, recovering ground-truth clusters exactly in cases where existing methods perform at baseline accuracy. We also demonstrate that ep-means outperforms moment-based classification techniques and discovers useful patterns in a variety of real-world applications.},
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
booktitle = {Sac '15},
doi = {10.1145/2695664.2695860},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
keywords = {Empirical Distributions,No,Unsupervised Learning},
pages = {893--900},
title = {{EP-MEANS : An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{Wang2017,
author = {Wang, Fei and Li, Xiao-li and Wang, Jason T L and Ng, See-kiong},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Guest Editorial Special Section on Biological Data Mining and Its Applications in Healthcare.pdf:pdf},
number = {3},
pages = {501--502},
title = {{Guest Editorial: Special Section on Biological Data Mining and Its Applications in Healthcare}},
volume = {14},
year = {2017}
}
@article{Pandey2016,
abstract = {BACKGROUND: Traditional Sanger sequencing has been used as a gold standard method for genetic testing in clinic to perform single gene test, which has been a cumbersome and expensive method to test several genes in heterogeneous disease such as cancer. With the advent of Next Generation Sequencing technologies, which produce data on unprecedented speed in a cost effective manner have overcome the limitation of Sanger sequencing. Therefore, for the efficient and affordable genetic testing, Next Generation Sequencing has been used as a complementary method with Sanger sequencing for disease causing mutation identification and confirmation in clinical research. However, in order to identify the potential disease causing mutations with great sensitivity and specificity it is essential to ensure high quality sequencing data. Therefore, integrated software tools are lacking which can analyze Sanger and NGS data together and eliminate platform specific sequencing errors, low quality reads and support the analysis of several sample/patients data set in a single run.\n\nRESULTS: We have developed ClinQC, a flexible and user-friendly pipeline for format conversion, quality control, trimming and filtering of raw sequencing data generated from Sanger sequencing and three NGS sequencing platforms including Illumina, 454 and Ion Torrent. First, ClinQC convert input read files from their native formats to a common FASTQ format and remove adapters, and PCR primers. Next, it split bar-coded samples, filter duplicates, contamination and low quality sequences and generates a QC report. ClinQC output high quality reads in FASTQ format with Sanger quality encoding, which can be directly used in down-stream analysis. It can analyze hundreds of sample/patients data in a single run and generate unified output files for both Sanger and NGS sequencing data. Our tool is expected to be very useful for quality control and format conversion of Sanger and NGS data to facilitate improved downstream analysis and mutation screening.\n\nCONCLUSIONS: ClinQC is a powerful and easy to handle pipeline for quality control and trimming in clinical research. ClinQC is written in Python with multiprocessing capability, run on all major operating systems and is available at https://sourceforge.net/projects/clinqc .},
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1186/s12859-016-0915-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - ClinQC a tool for quality control and cleaning of Sanger and NGS data in clinical research.pdf:pdf},
isbn = {14712105 (Electronic)},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Molecular diagnostic testing,Next generation sequencing,Quality control,Sanger sequencing},
number = {1},
pages = {56},
pmid = {26830926},
publisher = {BMC Bioinformatics},
title = {{ClinQC: A tool for quality control and cleaning of Sanger and NGS data in clinical research}},
url = {http://www.biomedcentral.com/1471-2105/17/56},
volume = {17},
year = {2016}
}
@misc{Xuan2013,
abstract = {The advent of next generation sequencing (NGS) technologies has revolutionized the field of genomics, enabling fast and cost-effective generation of genome-scale sequence data with exquisite resolution and accuracy. Over the past years, rapid technological advances led by academic institutions and companies have continued to broaden NGS applications from research to the clinic. A recent crop of discoveries have highlighted the medical impact of NGS technologies on Mendelian and complex diseases, particularly cancer. However, the ever-increasing pace of NGS adoption presents enormous challenges in terms of data processing, storage, management and interpretation as well as sequencing quality control, which hinder the translation from sequence data into clinical practice. In this review, we first summarize the technical characteristics and performance of current NGS platforms. We further highlight advances in the applications of NGS technologies towards the development of clinical diagnostics and therapeutics. Common issues in NGS workflows are also discussed to guide the selection of NGS platforms and pipelines for specific research purposes. {\textcopyright} 2012.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Xuan, Jiekun and Yu, Ying and Qing, Tao and Guo, Lei and Shi, Leming},
booktitle = {Cancer Letters},
doi = {10.1016/j.canlet.2012.11.025},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xuan et al. - 2013 - Next-generation sequencing in the clinic Promises and challenges.pdf:pdf},
isbn = {1872-7980 (Electronic)\r0304-3835 (Linking)},
issn = {03043835},
keywords = {Bioinformatics,Clinical applications,Exome sequencing,FFPE,RNA-Seq,Tumor heterogeneity,Whole-genome sequencing},
number = {2},
pages = {284--295},
pmid = {23174106},
title = {{Next-generation sequencing in the clinic: Promises and challenges}},
volume = {340},
year = {2013}
}
@book{,
issn = {1473-6691},
number = {2},
pmid = {24171086},
title = {{FastQC Manual}},
url = {http://www.bioinformatics.babraham.ac.uk/projects/fastqc/},
volume = {7}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Del et al. - 2014 - Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology.pdf:pdf},
pages = {51--68},
title = {{Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{Mckenna2010,
abstract = {The concept of absorptive capacity was introduced by Cohen and Levinthal in 1989. Since then it has been enhanced through reconceptualizations and extended by various empirical studies. Despite the growing interest in absorptive capacity it is unclear what this large stream of papers has collectively accomplished. The used definitions, antecedents, components and outcomes of the construct are extremely heterogeneous. Due to this heterogeneity, the empirical study of the construct remains difficult. There is no standard measure and no standard method of measurement, which can be used in empirical research. To bring more clarity into this research area, this paper provides a critical review of previous empirical treatments of absorptive capacity. For this purpose, different methods of measurement are classified in the following way: within quantitative methods proxy indicators and perceptive instruments are differentiated. Proxy indicators use single firm-level data for measuring absorptive capacity and can be input-oriented (R&D efforts, R&D human capital) or output-oriented (R&D patents, R&D publications). Perceptive instruments imply that researchers develop single questions or a set of questions, which reflect absorptive capacity or parts of it at the operational level. The main weakness of both proxy indicators and perceptive instruments is that they don't meet the complexity and emergence of the construct. Only few qualitative studies have started to adopt a new perspective, recognizing the process and practice-based character of absorptive capacity. In summary, the critical review prints out the necessity of advancing research in this area. For this reason, we set out to develop an alternative approach to capture absorptive capacity. It is a practice-oriented approach that allows studying actual absorptive practices in real world situations and enables researchers to capture the complex, embedded, and context-dependent patterns of acting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schmidt, Stephanie},
doi = {10.1101/gr.107524.110.20},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
isbn = {1549-5469 (Electronic)\r1088-9051 (Linking)},
issn = {1088-9051},
journal = {Proceedings of the International Conference on Intellectual Capital, Knowledge Management & Organizational Learning},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
pmid = {20644199},
title = {{Measuring absorptive capacity}},
volume = {20},
year = {2009}
}
@article{Paila2013,
abstract = {注释你 Mutation\r\n可以研究家系遗传病},
archivePrefix = {arXiv},
arxivId = {1304.4860},
author = {Paila, Umadevi and Chapman, Brad A. and Kirchner, Rory and Quinlan, Aaron R.},
doi = {10.1371/journal.pcbi.1003153},
eprint = {1304.4860},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paila et al. - 2013 - GEMINI Integrative Exploration of Genetic Variation and Genome Annotations.pdf:pdf},
isbn = {10.1371/journal.pcbi.1003153},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {7},
pmid = {23874191},
title = {{GEMINI: Integrative Exploration of Genetic Variation and Genome Annotations}},
volume = {9},
year = {2013}
}
@article{Systems2009,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - FASTQ format and data quality.pdf:pdf},
title = {{FASTQ format and data quality}},
year = {2009}
}
@inproceedings{Moore99,
abstract = {The worthiness of comparability as a goal for financial reporting information is addressed. It has been traditionally assumed that useful information helps users compare competing investment opportunities. Uniformity brings about comparability if and only if the uniformly applied rule actually produces relevant and reliable information. Modern financial analysis demands financial reports that support investors' search for intrinsic values. Rather than trying to force everyone to adopt the same practices, whatever those practices turn out to be, this better role for financial reporting demands that policy makers provide managers with latitude to produce statements that transparently describe their economic circumstances to best support users' assessments of their securities' intrinsic values. Chief executives and chief financial officers can innovate without violating GAAP because of SFAS 159, which is exactly the kind of standard that FASB needs to issue over and over again. By making innovation voluntary, the board encourages progress without compromise and without compulsion. We hope this experiment is a success and that it's followed by many more.},
author = {Moore, R. and Lopes, J.},
booktitle = {Accounting Today},
number = {9},
pages = {15},
publisher = {SCITEPRESS},
title = {{Paper templates}},
url = {http://proquest.umi.com/pqdweb?did=1668897271&Fmt=7&clientId=15443&RQT=309&VName=PQD%5Cnhttp://proquest.umi.com/pqdweb?did=1484623471&Fmt=7&clientId=15443&RQT=309&VName=PQD},
volume = {22},
year = {1999}
}
@article{Wu2017a,
abstract = {Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/Descargas/wu2016.pdf:pdf},
isbn = {0018-9294 VO  - 64},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {263--273},
pmid = {28113246},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
url = {http://ieeexplore.ieee.org/document/7587347/},
volume = {64},
year = {2017}
}
@article{Buchard2016,
abstract = {The HID-Ion AmpliSeqTM Identity Panel is a next-generation sequencing assay with 90 autosomal and 34 Y-chromosome SNPs that are amplified in one PCR step and subse- quently sequenced using the Ion Personal Genome Machine (Ion PGMTM) System. This assay was validated for relationship testing in our ISO 17025 accredited laboratory in 2015. Here, the essential parts of the validation report submitted to the Danish Accreditation Fund are presented. A total of 100 unrelated Danes were typed in duplicates and the locus balance, heterozygote balance (Hb) and noise levels were analysed in detail. Two loci were disregarded for casework because genotyping was uncertain. Hb for rs7520386 was skewed and high levels of noise were observed in rs576261. Three general acceptance criteria for analysis of single-source samples were defined: (i) sequencing depth ? 200 reads, (ii) noise level ? 3% and (iii) Hb ? 0.3. A Python script named SNPonPGM was developed to assist the analyst by highlighting loci that do not fulfil the general acceptance criteria. Furthermore, SNPonPGM has functions that reduce the hands-on time of the reporting officer to a few minutes per case.Mixtures with DNA from two individuals in a 1:24 ratio were readily identified using the three criteria and the SNPonPGM script.},
author = {Buchard, Anders and Kampmann, Marie Louise and Poulsen, Lena and B{\o}rsting, Claus and Morling, Niels},
doi = {10.1002/elps.201600269},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchard et al. - 2016 - ISO 17025 validation of a next-generation sequencing assay for relationship testing(3).pdf:pdf},
issn = {15222683},
journal = {Electrophoresis},
keywords = {Forensic genetics,ISO 17025 accreditation,Kinship testing,Next-generation sequencing,SNPs},
number = {21},
pages = {2822--2831},
pmid = {27709635},
title = {{ISO 17025 validation of a next-generation sequencing assay for relationship testing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27709635},
volume = {37},
year = {2016}
}
@book{Auwera2014,
author = {Auwera, Geraldine A Van Der and Carneiro, Mauricio O and Hartl, Chris and Poplin, Ryan and Levy-moonshine, Ami and Jordan, Tadeusz and Shakir, Khalid and Roazen, David and Thibault, Joel and Banks, Eric and Garimella, Kiran V and Altshuler, David and Gabriel, Stacey and Depristo, Mark A},
booktitle = {Curr Protoc Bioinformatics},
doi = {10.1002/0471250953.bi1110s43.From},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auwera et al. - 2014 - From FastQ data to high confidence varant calls the Genonme Analysis Toolkit best practices pipeline.pdf:pdf},
isbn = {0471250953},
keywords = {exome,genotyping,ngs,variant detection,wgs},
number = {1110},
title = {{From FastQ data to high confidence varant calls: the Genome Analysis Toolkit best practices pipeline}},
volume = {11},
year = {2014}
}
@article{Weegar2015,
abstract = {Detection of early symptoms in cervical cancer is crucial for early treatment and survival. To find symptoms of cervical cancer in clinical text, Named Entity Recognition is needed. In this paper the Clinical Entity Finder, a machine-learning tool trained on annotated clinical text from a Swedish internal medicine emergency unit, is evaluated on cervical cancer records. The Clinical Entity Finder identifies entities of the types body part, finding and disorder and is extended with negation detection using the rule-based tool NegEx, to distinguish between negated and non-negated entities. To measure the performance of the tools on this new domain, two physicians annotated a set of clinical notes from the health records of cervical cancer patients. The inter-annotator agreement for finding, disorder and body part obtained an average F-score of 0.677 and the Clinical Entity Finder extended with NegEx had an average F-score of 0.667.},
author = {Weegar, R. and Kvist, M. and Sundstr{\"{o}}m, K. and Brunak, S. and Dalianis, H.},
file = {:home/jennifer/Descargas/2245432.pdf:pdf},
issn = {1942597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
pages = {1296--1305},
title = {{Finding Cervical Cancer Symptoms in Swedish Clinical Text using a Machine Learning Approach and NegEx}},
volume = {2015},
year = {2015}
}
@article{Ketchen1996,
author = {Ketchen, David J and Shook, L},
file = {:home/jennifer/Descargas/10.2307@2486927.pdf:pdf},
journal = {Strategic Management Journal},
number = {6},
pages = {441--458},
title = {{The Application of Cluster Analysis in Strategic Management Research: An Analysis and Critique}},
volume = {17},
year = {1996}
}
@article{Xu2016,
abstract = {A high-performance computing environment, also known as a supercomputing environment, e-Science environment or cyberinfrastructure, is a crucial system that connects users' applications to supercomputers, and provides usability, efficiency, sharing, and collaboration capabilities. This review presents important lessons drawn from China's nationwide efforts to build and use a high-performance computing environment over the past 20 years (1995–2015), including three observations and two open problems. We present evidence that such an environment helps to grow China's nationwide supercomputing ecosystem by orders of magnitude, where a loosely coupled architecture accommodates diversity. An important open problem is why technology for global networked supercomputing has not yet become as widespread as the Internet or Web. In the next 20 years, high-performance computing environments will need to provide zettaflops computing capability and 10 000 times better energy efficiency, and support seamless human-cyber-physical ternary computing.},
author = {Xu, Zhiwei and Chi, Xuebin and Xiao, Nong},
doi = {10.1093/nsr/nww001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Chi, Xiao - 2016 - High-Performance Computing Environment A Review of Twenty Years of Experiments in China.pdf:pdf},
issn = {2053714X},
journal = {National Science Review},
keywords = {Cyberinfrastructure,E-Science environment,Middleware,Supercomputingcover-date},
number = {1},
pages = {36--48},
title = {{High-performance computing environment: A review of twenty years of experiments in China}},
volume = {3},
year = {2016}
}
@misc{Paez2012,
abstract = {El contexto din{\'{a}}mico y competitivo de la organizaci{\'{o}}n actual exige permanentes soluciones inform{\'{a}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\'{o}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\'{o}}n mayormente, desde el punto de vista tecnol{\'{o}}gico, sin considerar los aspectos organizacionales y metodol{\'{o}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
issn = {0120-341X},
keywords = {Sistemas de informaci{\'{o}}n en administraci{\'{o}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Fisch2015,
abstract = {Motivation: Omics Pipe (http://sulab.scripps.edu/omicspipe) is a computational framework that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole-Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas (TCGA). Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis. The goal of Omics Pipe is to democratize next-generation sequencing analysis by dramatically increasing the accessibility and reproducibility of best practice computational pipelines, which will enable researchers to generate biologically meaningful and interpretable results.\nResults: Using Omics Pipe, we analyzed 100 TCGA breast invasive carcinoma paired tumor-normal datasets based on the latest UCSC hg19 RefSeq annotation. Omics Pipe automatically downloaded and processed the desired TCGA samples on a high throughput compute cluster to produce a results report for each sample. We aggregated the individual sample results and compared them to the analysis in the original publications. This comparison revealed high overlap between the analyses, as well as novel findings due to the use of updated annotations and methods.\nAvailability and implementation: Source code for Omics Pipe is freely available on the web (https://bitbucket.org/sulab/omics_pipe). Omics Pipe is distributed as a standalone Python package for installation (https://pypi.python.org/pypi/omics_pipe) and as an Amazon Machine Image in Amazon Web Services Elastic Compute Cloud that contains all necessary third-party software dependencies and databases (https://pythonhosted.org/omics_pipe/AWS_installation.html).\nContact: asu@scripps.edu or kfisch@ucsd.edu\nSupplementary Information: Supplementary data are available at Bioinformatics online.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Allahyari2017,
abstract = {The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.},
archivePrefix = {arXiv},
arxivId = {1707.02919},
author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saied and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
eprint = {1707.02919},
file = {:home/jennifer/Descargas/1707.02919.pdf:pdf},
keywords = {classification,clustering,infor-,information retrieval,text mining},
title = {{A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques}},
url = {http://arxiv.org/abs/1707.02919},
year = {2017}
}
@article{Arias-blanco2015,
abstract = {<p><p>Objetivo: describir variantes de secuencia en los genes BRCA1 y BRCA2 en una muestra de pacientes colombianas con historia personal o familiar de c{\'{a}}ncer de mama sugestiva de riesgo gen{\'{e}}tico.</p><p>Materiales y m{\'{e}}todos: serie de casos compuesta por 67 pacientes que fueron remitidas para estudio gen{\'{e}}tico por sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama y ovario hereditario (HBOC). De los 67 casos, 42 (62,7 %) cumplieron con los criterios de indicaci{\'{o}}n m{\'{e}}dica de la National Comprehensive Cancer Network (NCCN) del 2013, y en ellos se realiz{\'{o}} secuenciaci{\'{o}}n completa de los genes BRCA1 y BRCA2. Se determin{\'{o}} la frecuencia de mutaci{\'{o}}n, variantes de secuencia y significancia cl{\'{i}}nica de las variantes halladas con base en Breast Cancer Information Core (BIC).</p><p>Resultados: se identificaron mutaciones para el gen BRCA1 en seis pacientes (14,3 %), no se document{\'{o}} mutaci{\'{o}}n para el gen BRCA2, adem{\'{a}}s se detectaron 43 variantes gen{\'{e}}ticas en 27 pacientes (64,2 % de 42 casos). De estas, 21 (48,8 %) fueron identificadas en el gen BRCA1 y 22 (51,2 %) en el gen BRCA2. Dentro de estas variantes, se identificaron 5 mutaciones patog{\'{e}}nicas solo en el gen BRCA1, de las cuales solo una hab{\'{i}}a sido reportada previamente en Colombia.</p><p>Conclusiones: este estudio identifica variantes gen{\'{e}}ticas patog{\'{e}}nicas en el gen BRCA1 no descritas en estudios previos en la poblaci{\'{o}}n colombiana y otras conocidas en diferentes poblaciones; permitiendo de esta forma ampliar el conocimiento sobre las variantes en poblaci{\'{o}}n colombiana de los genes BRCA1 y BRCA2. Sin embargo, se requieren m{\'{a}}s estudios con suficiente poder y calidad metodol{\'{o}}gica para poder estimar la frecuencia de mutaciones y de variantes de secuencia para estos genes en mujeres colombianas con sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama u ovario hereditario.</p></p>},
author = {Arias-Blanco, Juan Felipe and Ospino-Dur{\'{a}}n, Eder Alonso and Restrepo-Fern{\'{a}}ndez, Carlos M. and Guzm{\'{a}}n-AbiSaab, Luis and Fonseca-Mendoza, Dora Janeth and {\'{A}}ngel-Guevara, Diana Isabel and Garz{\'{o}}n-Venegas, Elena Del Pilar and Gamboa-Garay, Oscar and Obreg{\'{o}}n-Tito, Alexandra J. and G{\'{o}}mez-Parrado, Yenny},
doi = {10.18597/rcog.294},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arias-blanco, Fonseca-mendoza, Gamboa-garay - 2015 - FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2.pdf:pdf},
issn = {2463-0225},
journal = {Revista Colombiana de Obstetricia y Ginecolog{\'{i}}a},
number = {4},
pages = {287},
title = {{Prevalencia de mutaci{\'{o}}n y de variantes de secuencia para los genes BRCA1 y BRCA2 en una muestra de mujeres colombianas con sospecha de s{\'{i}}ndrome de c{\'{a}}ncer de mama hereditario: serie de casos}},
url = {http://revista.fecolsog.org/index.php/rcog/article/view/294},
volume = {66},
year = {2015}
}
@article{Luo2017,
abstract = {—Disease gene prediction is a challenging task that has a variety of applications such as early diagnosis and drug development. The existing machine learning methods suffer from the imbalanced sample issue because the number of known disease genes (positive samples) is much less than that of unknown genes which are typically considered to be negative samples. In addition, most methods have not utilized clinical data from patients with a specific disease to predict disease genes. In this study, we propose a disease gene prediction algorithm (called dgSeq) by combining protein-protein interaction (PPI) network, clinical RNA-Seq data, and Online Mendelian Inheritance in Man (OMIN) data. Our dgSeq constructs differential networks based on rewiring information calculated from clinical RNA-Seq data. To select balanced sets of non-disease genes (negative samples), a disease-gene network is also constructed from OMIM data. After features are extracted from the PPI networks and differential networks, the logistic regression classifiers are trained. Our dgSeq obtains AUC values of 0.88, 0.83 and 0.80 for identifying breast cancer genes, thyroid cancer genes and Alzheimer's disease genes, respectively, which indicates its superiority to other three competing methods. Both gene set enrichment analysis and predicted results demonstrate that dgSeq can effectively predict new disease genes.},
author = {Luo, Ping and Tian, Li Ping and Ruan, Jishou and Wu, Fangxiang},
doi = {10.1109/TCBB.2017.2770120},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Ruan, Member - 2017 - Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data.pdf:pdf},
issn = {15455963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
keywords = {Diseases,Feature extraction,Gene expression,Logistics,Machine learning algorithms,OMIM data,Prediction algorithms,Training,clinical RNA-Seq data,disease gene,protein-protein interaction network,rewiring information},
number = {306},
pages = {1--11},
title = {{Disease Gene Prediction by Integrating PPI Networks, Clinical RNA-Seq Data and OMIM Data}},
volume = {1},
year = {2017}
}
@article{Li2014,
abstract = {In ''Omics'' era of the life sciences, data is presented in many forms, which represent the information at various levels of bio-logical systems, including data about genome, transcriptome, epigenome, proteome, metabolome, molecular imaging, molec-ular pathways, different population of people and clinical/med-ical records. The biological data is big, and its scale has already been well beyond petabyte (PB) even exabyte (EB). Nobody doubts that the biological data will create huge amount of val-ues, if scientists can overcome many challenges, e.g., how to handle the complexity of information, how to integrate the data from very heterogeneous resources, what kind of principles or standards to be adopted when facing with the big data. Tools and techniques for analyzing big biological data enable us to translate massive amount of information into a better under-standing of the basic biomedical mechanisms, which can be fur-ther applied to translational or personalized medicine. Today, big data is one of the hottest topics in information science, but its concept can be misleading or confusing. The name itself suggests huge amount of data, which, however, represents only one aspect. In general, big data has four impor-tant features, so called four V's: volume of data, velocity of processing the data, variability of data sources, and veracity of the data quality. These four hallmarks of big data require to be characterized by special theory and technology; however, currently there is no satisfactory solution. Now, more biolo-gists are involved with the big data due to the rapid advance of high-throughput biotechnologies. As an example, the Human Genome Project utilized the expertise, infrastructure, and people from 20 institutions and took 13 years of work with over $3 billion to determine the whole genome structure of approximately three billion nucleotides. But now we can sequence a whole human genome for $1000 and within three days. We have spent decades struggling to collect enough bio-logical and biomedical data, but when big data overwhelms us, are we ready to face the challenge? The new bottleneck to this problem in biology is how to reveal the essential mechanisms of biological systems by understanding the big noisy data. Life sciences today need more robust, expressive, computable, quantitative, accurate and precise ways to handle the big data. As a matter of fact, recent works in this area have already brought remarkable advantage and opportunities, which implies the central roles of bioinformatics and bioinformati-cians in the future research of the biological and biomedical fields. In the following text, we describe several aspects of big biological data based on our recent studies.},
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big biological data challenges and opportunities.pdf:pdf},
issn = {22103244},
journal = {Genomics, Proteomics and Bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Data Mining,Data Mining: methods,Gene Expression Profiling,High-Throughput Screening Assays,Humans,Software},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China},
title = {{Big biological data: Challenges and opportunities}},
url = {http://www.sciencedirect.com/science/article/pii/S1672022914001041},
volume = {12},
year = {2014}
}
@misc{CoriellInstitute,
author = {Stephenson, Joan},
booktitle = {Jama},
doi = {10.1001/jama.299.7.755-d},
isbn = {1546-1696 (Electronic)
1087-0156 (Linking)},
issn = {0098-7484},
number = {7},
pages = {755},
pmid = {18327223},
title = {{1000 Genomes Project}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.299.7.755-d},
volume = {299},
year = {2008}
}
@article{Pei,
abstract = {OBJECTIVES To determine the prevalence of anti-myeloperoxidase (MPO) antibodies of IgA (IgA anti-MPO) isotype in patients with eosinophilic granulomatosis with polyangiitis (EGPA), and the association of the IgA antibodies with IgG anti-MPO and with disease activity. METHODS Serum samples from patients with EGPA followed in a multi-center longitudinal cohort were tested by ELISA for the presence of IgA anti-MPO and IgG anti-MPO antibodies. Sera from 87 healthy controls were used to define a positive test. Sera from 168 patients with EGPA (298 samples) were tested. Frequencies of positive testing for IgA anti-MPO were compared between patients with active EGPA, patients in remission, and controls. RESULTS IgA anti-MPO was detected in 10 of 168 (6%) patients with EGPA (11 of 298 serum samples) compared to 1 of 87 (1%) healthy controls (p=0.10). All 11 samples testing positive for IgA anti-MPO also tested positive for IgG anti-MPO. Ninety samples tested positive for IgG anti-MPO but negative for IgA. Samples taken during active EGPA were positive for IgA anti-MPO in 6/72 cases (8%), compared to 5/226 (2%) during remission (p=0.03). Among samples taken during moderate or high disease activity, 5/41 were positive (12%, p=0.01 compared to remission). CONCLUSIONS Although IgA anti-MPO antibodies are detectable in some patients with EGPA and may be detectable more frequently during active disease, their presence seems unlikely to provide information beyond what is obtained from conventional IgG anti-MPO.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Oommen, Esha and Hummel, Amber and Allmannsberger, Lisa and Cuthbertson, David and Carette, Simon and Pagnoux, Christian and Hoffman, Gary S. and Jenne, Dieter E. and Khalidi, Nader A. and Koening, Curry L. and Langford, Carol A. and McAlear, Carol A. and Moreland, Larry and Seo, Philip and Sreih, Antoine and Ytterberg, Steven R. and Merkel, Peter A. and Specks, Ulrich and Monach, Paul A.},
doi = {10.1007/128},
eprint = {15334406},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pei Hui - 2012 - Next Generation Sequencing Chemistry, Technology and Applications.pdf:pdf},
isbn = {0002-9297},
issn = {0392856X},
journal = {Clinical and experimental rheumatology},
keywords = {genomic medicine,next generation sequencing {\'{a}}},
number = {1},
pages = {98--101},
pmid = {28642624},
title = {{IgA antibodies to myeloperoxidase in patients with eosinophilic granulomatosis with polyangiitis (Churg-Strauss)}},
volume = {35},
year = {2017}
}
@misc{Tetreault2015a,
abstract = {Whole-exome sequencing (WES) represents a significant breakthrough in the field of human genetics. This technology has largely contributed to the identification of new disease-causing genes and is now entering clinical laboratories. WES represents a powerful tool for diagnosis and could reduce the 'diagnostic odyssey' for many patients. In this review, we present a technical overview of WES analysis, variants annotation and interpretation in a clinical setting. We evaluate the usefulness of clinical WES in different clinical indications, such as rare diseases, cancer and complex diseases. Finally, we discuss the efficacy of WES as a diagnostic tool and the impact on patient management.},
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
booktitle = {Expert Review of Molecular Diagnostics},
doi = {10.1586/14737159.2015.1039516},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
isbn = {1473-7159},
issn = {17448352},
keywords = {Cancer,Whole-exome sequencing,diagnostic,rare diseases,variants detection},
language = {en},
month = {may},
number = {6},
pages = {749--760},
pmid = {25959410},
publisher = {Informa Healthcare},
title = {{Whole-exome sequencing as a diagnostic tool: Current challenges and future opportunities}},
url = {http://www.tandfonline.com/doi/abs/10.1586/14737159.2015.1039516?journalCode=iero20#.VxmiWVg_zcU.mendeley},
volume = {15},
year = {2015}
}
@techreport{Chu2011,
abstract = {The advent of high-throughput sequencing (HTS) methods has enabled direct approaches to quantitatively profile small RNA populations. However, these methods have been limited by several factors, including representational artifacts and lack of established statistical methods of analysis. Furthermore, massive HTS data sets present new problems related to data processing and mapping to a reference genome. Here, we show that cluster-based sequencing-by-synthesis technology is highly reproducible as a quantitative profiling tool for several classes of small RNA from Arabidopsis thaliana. We introduce the use of synthetic RNA oligoribonucleotide standards to facilitate objective normalization between HTS data sets, and adapt microarray-type methods for statistical analysis of multiple samples. These methods were tested successfully using mutants with small RNA biogenesis (miRNA-defective dcl1 mutant and siRNA-defective dcl2 dcl3 dcl4 triple mutant) or effector protein (ago1 mutant) deficiencies. Computational methods were also developed to rapidly and accurately parse, quantify, and map small RNA data.},
author = {Fahlgren, N. and Sullivan, C. M. and Kasschau, K. D. and Chapman, E. J. and Cumbie, J. S. and Montgomery, T. A. and Gilbert, S. D. and Dasenko, M. and Backman, T. W.H. and Givan, S. A. and Carrington, J. C.},
booktitle = {Rna},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
isbn = {1469-9001 (Electronic)\r1355-8382 (Linking)},
issn = {1355-8382},
keywords = {gene expression,microarrays,significance analysi},
number = {5},
pages = {992--1002},
pmid = {19307293},
title = {{Computational and analytical framework for small RNA profiling by high-throughput sequencing}},
url = {http://rnajournal.cshlp.org/cgi/doi/10.1261/rna.1473809},
volume = {15},
year = {2009}
}
@techreport{Henderson2015,
abstract = {Given a collection of m continuous-valued, one-dimensional empirical probability distributions {P1, . . . , Pm}, how can we cluster these distributions efficiently with a nonparamet-ric approach? Such problems arise in many real-world set-tings where keeping the moments of the distribution is not appropriate, because either some of the moments are not defined or the distributions are heavy-tailed or bi-modal. Examples include mining distributions of inter-arrival times and phone-call lengths. We present an efficient algorithm with a non-parametric model for clustering empirical, one-dimensional, continuous probability distributions. Our al-gorithm, called ep-means, is based on the Earth Mover's Distance and k-means clustering. We illustrate the utility of ep-means on various data sets and applications. In par-ticular, we demonstrate that ep-means effectively and ef-ficiently clusters probability distributions of mixed and ar-bitrary shapes, recovering ground-truth clusters exactly in cases where existing methods perform at baseline accuracy. We also demonstrate that ep-means outperforms moment-based classification techniques and discovers useful patterns in a variety of real-world applications.},
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
booktitle = {Sac '15},
doi = {10.1145/2695664.2695860},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
keywords = {Empirical Distributions,No,Unsupervised Learning},
pages = {893--900},
title = {{EP-MEANS : An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@misc{,
annote = {NULL},
booktitle = {La Patria},
pages = {http://www.lapatria.com/nacional/adn--clave--para--id},
title = {{ADN, clave para identificar 311 cuerpos}},
url = {http://www.lapatria.com/nacional/adn-clave-para-id},
urldate = {2016-11-20}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - scatter-mode (1).png:png},
title = {{Scatter-Mode (1)}}
}
@article{La2012,
abstract = {Ejercicio de citas en Mendeley},
author = {Notario, Silvia},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
journal = {Seminario de tesis 1},
pages = {1--2},
title = {{Ejercicio de citas con Mendeley}},
year = {2016}
}
@article{Li2009a,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803\r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Mulder2017,
abstract = {Background Although pockets of bioinformatics excellence have developed in Africa, generally, large-scale genomic data analysis has been limited by the availability of expertise and infrastructure. H3ABioNet, a pan-African bioinformatics network, was established to build capacity specifically to enable H3Africa (Human Heredity and Health in Africa) researchers to analyze their data in Africa. Since the inception of the H3Africa initiative, H3ABioNet's role has evolved in response to changing needs from the consortium and the African bioinformatics community. Objectives H3ABioNet set out to develop core bioinformatics infrastructure and capacity for genomics research in various aspects of data collection, transfer, storage, and analysis. Methods and Results Various resources have been developed to address genomic data management and analysis needs of H3Africa researchers and other scientific communities on the continent. NetMap was developed and used to build an accurate picture of network performance within Africa and between Africa and the rest of the world, and Globus Online has been rolled out to facilitate data transfer. A participant recruitment database was developed to monitor participant enrollment, and data is being harmonized through the use of ontologies and controlled vocabularies. The standardized metadata will be integrated to provide a search facility for H3Africa data and biospecimens. Because H3Africa projects are generating large-scale genomic data, facilities for analysis and interpretation are critical. H3ABioNet is implementing several data analysis platforms that provide a large range of bioinformatics tools or workflows, such as Galaxy, the Job Management System, and eBiokits. A set of reproducible, portable, and cloud-scalable pipelines to support the multiple H3Africa data types are also being developed and dockerized to enable execution on multiple computing infrastructures. In addition, new tools have been developed for analysis of the uniquely divergent African data and for downstream interpretation of prioritized variants. To provide support for these and other bioinformatics queries, an online bioinformatics helpdesk backed by broad consortium expertise has been established. Further support is provided by means of various modes of bioinformatics training. Conclusions For the past 4 years, the development of infrastructure support and human capacity through H3ABioNet, have significantly contributed to the establishment of African scientific networks, data analysis facilities, and training programs. Here, we describe the infrastructure and how it has affected genomics and bioinformatics research in Africa.},
author = {Mulder, Nicola J. and Adebiyi, Ezekiel and Adebiyi, Marion and Adeyemi, Seun and Ahmed, Azza and Ahmed, Rehab and Akanle, Bola and Alibi, Mohamed and Armstrong, Don L. and Aron, Shaun and Ashano, Efejiro and Baichoo, Shakuntala and Benkahla, Alia and Brown, David K. and Chimusa, Emile R. and Fadlelmola, Faisal M. and Falola, Dare and Fatumo, Segun and Ghedira, Kais and Ghouila, Amel and Hazelhurst, Scott and Isewon, Itunuoluwa and Jung, Segun and Kassim, Samar Kamal and Kayondo, Jonathan K. and Mbiyavanga, Mamana and Meintjes, Ayton and Mohammed, Somia and Mosaku, Abayomi and Moussa, Ahmed and Muhammd, Mustafa and Mungloo-Dilmohamud, Zahra and Nashiru, Oyekanmi and Odia, Trust and Okafor, Adaobi and Oladipo, Olaleye and Osamor, Victor and Oyelade, Jellili and Sadki, Khalid and Salifu, Samson Pandam and Soyemi, Jumoke and Panji, Sumir and Radouani, Fouzia and Souiai, Oussama and {Tastan Bishop}, {\"{O}}zlem},
doi = {10.1016/j.gheart.2017.01.005},
issn = {22118179},
journal = {Global Heart},
month = {jun},
number = {2},
pages = {91--98},
pmid = {28302555},
publisher = {Elsevier},
title = {{Development of Bioinformatics Infrastructure for Genomics Research}},
url = {http://www.sciencedirect.com/science/article/pii/S2211816017300054},
volume = {12},
year = {2017}
}
@article{Sims2014,
abstract = {Sequencing technologies have placed a wide range of genomic analyses within the capabilities of many laboratories. However, sequencing costs often set limits to the amount of sequences that can be generated and, consequently, the biological outcomes that can be achieved from an experimental design. In this Review, we discuss the issue of sequencing depth in the design of next-generation sequencing experiments. We review current guidelines and precedents on the issue of coverage, as well as their underlying considerations, for four major study designs, which include de novo genome sequencing, genome resequencing, transcriptome sequencing and genomic location analyses (for example, chromatin immunoprecipitation followed by sequencing (ChIP-seq) and chromosome conformation capture (3C)).},
author = {Sims, David and Sudbery, Ian and Ilott, Nicholas E. and Heger, Andreas and Ponting, Chris P.},
doi = {10.1038/nrg3642},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sims et al. - 2014 - Genomics is extending its reach into diverse fields of biomedical research from agriculture to clinical diag- nosti.pdf:pdf},
isbn = {1471-0056},
issn = {14710056},
journal = {Nature Reviews Genetics},
number = {2},
pages = {121--132},
pmid = {24434847},
title = {{Sequencing depth and coverage: Key considerations in genomic analyses}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24434847},
volume = {15},
year = {2014}
}
@article{Wei2009,
abstract = {There are several mining algorithms of association rules. One of the most popular algorithms is Apriori that is used to extract frequent itemsets from large database and getting the association rule for discovering the knowledge. Based on this algorithm, this paper indicates the limitation of the original Apriori algorithm of wasting time for scanning the whole database searching on the frequent itemsets, and presents an improvement on Apriori by reducing that wasted time depending on scanning only some transactions. The paper shows by experimental results with several groups of transactions, and with several values of minimum support that applied on the original Apriori and our implemented improved Apriori that our improved Apriori reduces the time consumed by 67.38% in comparison with the original Apriori, and makes the Apriori algorithm more efficient and less time consuming},
archivePrefix = {arXiv},
arxivId = {1403.3948},
author = {Wei, Yong Qing and Yang, Ren Hua and Liu, Pei Yu},
doi = {10.1109/ITIME.2009.5236211},
eprint = {1403.3948},
file = {:home/jennifer/Descargas/1403.3948.pdf:pdf},
isbn = {9781424439294},
issn = {23194111},
journal = {ITME2009 - Proceedings 2009 IEEE International Symposium on IT in Medicine and Education},
number = {1},
pages = {942--946},
title = {{An improved apriori algorithm for association rules of mining}},
volume = {3},
year = {2009}
}
@misc{scipy,
abstract = {By itself, Python is an excellent "steering" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
annote = {[Online; accessed ]},
author = {Oliphant, Travis E},
booktitle = {Computing in Science and Engineering},
doi = {10.1109/MCSE.2007.58},
isbn = {9781584889298},
issn = {1521-9615},
pages = {10--20},
title = {{SciPy: Open source scientific tools for Python}},
url = {http://www.scipy.org/},
volume = {9},
year = {2007}
}
@article{Breuer2017a,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breuer et al. - 2017 - Genotype-phenotype association mining in bipolar disorder market research meets complex genetics(2).pdf:pdf},
journal = {bioRxiv},
month = {mar},
pages = {116624},
publisher = {Cold Spring Harbor Laboratory},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@misc{Illumina2017,
author = {Illumina},
title = {{Whole Exome Sequencing | Detect exonic variants}},
url = {http://www.illumina.com/techniques/sequencing/dna-sequencing/targeted-resequencing/exome-sequencing.html},
urldate = {2017-11-15},
year = {2017}
}
@article{Hehir-Kwa2016,
abstract = {Structural variation (SV) represents a major source of differences between individual human genomes and has been linked to disease phenotypes. However, the majority of studies provide neither a global view of the full spectrum of these variants nor integrate them into reference panels of genetic variation. Here, we analyse whole genome sequencing data of 769 individuals from 250 Dutch families, and provide a haplotype-resolved map of 1.9 million genome variants across 9 different variant classes, including novel forms of complex indels, and retrotransposition-mediated insertions of mobile elements and processed RNAs. A large proportion are previously under reported variants sized between 21 and 100 bp. We detect 4 megabases of novel sequence, encoding 11 new transcripts. Finally, we show 191 known, trait-associated SNPs to be in strong linkage disequilibrium with SVs and demonstrate that our panel facilitates accurate imputation of SVs in unrelated individuals.},
author = {Hehir-Kwa, Jayne Y. and Marschall, Tobias and Kloosterman, Wigard P. and Francioli, Laurent C. and Baaijens, Jasmijn A. and Dijkstra, Louis J. and Abdellaoui, Abdel and Koval, Vyacheslav and Thung, Djie Tjwan and Wardenaar, Ren{\'{e}} and Renkens, Ivo and Coe, Bradley P. and Deelen, Patrick and {De Ligt}, Joep and Lameijer, Eric Wubbo and {Van Dijk}, Freerk and Hormozdiari, Fereydoun and Uitterlinden, Andr{\'{e}} G. and {Van Duijn}, Cornelia M. and Eichler, Evan E. and {De Bakker}, Paul I.W. and Swertz, Morris A. and Wijmenga, Cisca and {Van Ommen}, Gert Jan B. and Slagboom, P. Eline and Boomsma, Dorret I. and Sch{\"{o}}nhuth, Alexander and Ye, Kai and Guryev, Victor and Bovenberg, Jasper A. and {De Craen}, Anton J.M. and Beekman, Marian and Hofman, Albert and Willemsen, Gonneke and Wolffenbuttel, Bruce and Platteel, Mathieu and Du, Yuanping and Chen, Ruoyan and Cao, Hongzhi and Cao, Rui and Sun, Yushen and Cao, Jeremy Sujie and Neerincx, Pieter B.T. and Dijkstra, Martijn and Byelas, George and Kanterakis, Alexandros and Bot, Jan and Vermaat, Martijn and Laros, Jeroen F.J. and {Den Dunnen}, Johan T. and {De Knijff}, Peter and Karssen, Lennart C. and {Van Leeuwen}, Elisa M. and Amin, Najaf and Rivadeneira, Fernando and Estrada, Karol and Hottenga, Jouke Jan and Kattenberg, V. Mathijs and {Van Enckevort}, David and Mei, Hailiang and Santcroos, Mark and {Van Schaik}, Barbera D.C. and Handsaker, Robert E. and McCarroll, Steven A. and Ko, Arthur and Sudmant, Peter and Nijman, Isaac J.},
doi = {10.1038/ncomms12989},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hehir-Kwa et al. - 2016 - ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural var.pdf:pdf},
isbn = {2041-1723 (Electronic)\r2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
number = {11},
pmid = {27708267},
title = {{A high-quality human reference panel reveals the complexity and distribution of genomic structural variants}},
volume = {7},
year = {2016}
}
@article{Maharjan2011,
author = {Maharjan, Merina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maharjan - 2011 - Genome Analysis with MapReduce.pdf:pdf},
pages = {1--23},
title = {{Genome Analysis with MapReduce}},
year = {2011}
}
@article{Ramasamy2017,
author = {Ramasamy, S. and Nirmala, K.},
doi = {10.1080/1206212X.2017.1396415},
file = {:home/jennifer/Descargas/dies.pdf:pdf},
issn = {1206-212X},
journal = {International Journal of Computers and Applications},
keywords = {Data mining,association,association rule,classification,data mining,keyword-based clustering},
number = {December},
pages = {1--8},
publisher = {Taylor & Francis},
title = {{Disease prediction in data mining using association rule mining and keyword based clustering algorithms}},
url = {https://www.tandfonline.com/doi/full/10.1080/1206212X.2017.1396415},
volume = {7074},
year = {2017}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (1).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (2)}}
}
@article{Pirooznia2014,
abstract = {The processing and analysis of the large scale data generated by next-generation sequencing (NGS) experiments is challenging and is a burgeoning area of new methods development. Several new bioinformatics tools have been developed for calling sequence variants from NGS data. Here, we validate the variant calling of these tools and compare their relative accuracy to determine which data processing pipeline is optimal. We developed a unified pipeline for processing NGS data that encompasses four modules: mapping, filtering, realignment and recalibration, and variant calling. We processed 130 subjects from an ongoing whole exome sequencing study through this pipeline. To evaluate the accuracy of each module, we conducted a series of comparisons between the single nucleotide variant (SNV) calls from the NGS data and either gold-standard Sanger sequencing on a total of 700 variants or array genotyping data on a total of 9,935 single-nucleotide polymorphisms. A head to head comparison showed that Genome Analysis Toolkit (GATK) provided more accurate calls than SAMtools (positive predictive value of 92.55% vs. 80.35%, respectively). Realignment of mapped reads and recalibration of base quality scores before SNV calling proved to be crucial to accurate variant calling. GATK HaplotypeCaller algorithm for variant calling outperformed the UnifiedGenotype algorithm. We also showed a relationship between mapping quality, read depth and allele balance, and SNV call accuracy. However, if best practices are used in data processing, then additional filtering based on these metrics provides little gains and accuracies of >99% are achievable. Our findings will help to determine the best approach for processing NGS data to confidently call variants for downstream analyses. To enable others to implement and replicate our results, all of our codes are freely available at \r\n                    http://metamoodics.org/wes\r\n                    \r\n                  .},
author = {Pirooznia, Mehdi and Kramer, Melissa and Parla, Jennifer and Goes, Fernando S. and Potash, James B. and McCombie, W. Richard and Zandi, Peter P.},
doi = {10.1186/1479-7364-8-14},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pirooznia et al. - 2014 - Validation and assessment of variant calling pipelines for next-generation sequencing.pdf:pdf},
isbn = {1479-7364 (Electronic)\r1473-9542 (Linking)},
issn = {14797364},
journal = {Human genomics},
keywords = {Bipolar Disorder,Bipolar Disorder: genetics,DNA,Data Interpretation,Exome,High-Throughput Nucleotide Sequencing,Humans,Polymorphism,Sequence Analysis,Single Nucleotide,Software,Statistical},
number = {1},
pages = {14},
pmid = {25078893},
title = {{Validation and assessment of variant calling pipelines for next-generation sequencing}},
url = {http://www.humgenomics.com/content/8/1/14},
volume = {8},
year = {2014}
}
@article{VishalGupta2009,
abstract = {Text Mining has become an important research area. Text Mining is the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources. In this paper, a Survey of Text Mining techniques and applications have been s presented.},
author = {{Vishal Gupta}, Gurpreet S. Lehal},
doi = {10.4304/jetwi.1.1.60-76},
file = {:home/jennifer/Descargas/20141230112729939.pdf:pdf},
isbn = {0954405021},
issn = {17980461},
journal = {journal of Emerging Technologies in web Intelligence},
number = {1},
pages = {17},
title = {{A Survey of Text Mining Techniques and Applications}},
url = {http://www.academypublisher.com/jetwi/vol01/no1/jetwi01016076.pdf},
volume = {1},
year = {2009}
}
@article{Jiang2004,
abstract = { DNA microarray technology has now made it possible to simultaneously monitor the expression levels of thousands of genes during important biological processes and across collections of related samples. Elucidating the patterns hidden in gene expression data offers a tremendous opportunity for an enhanced understanding of functional genomics. However, the large number of genes and the complexity of biological networks greatly increases the challenges of comprehending and interpreting the resulting mass of data, which often consists of millions of measurements. A first step toward addressing this challenge is the use of clustering techniques, which is essential in the data mining process to reveal natural structures and identify interesting patterns in the underlying data. Cluster analysis seeks to partition a given data set into groups based on specified features so that the data points within a group are more similar to each other than the points in different groups. A very rich literature on cluster analysis has developed over the past three decades. Many conventional clustering algorithms have been adapted or directly applied to gene expression data, and also new algorithms have recently been proposed specifically aiming at gene expression data. These clustering algorithms have been proven useful for identifying biologically relevant groups of genes and samples. In this paper, we first briefly introduce the concepts of microarray technology and discuss the basic elements of clustering on gene expression data. In particular, we divide cluster analysis for gene expression data into three categories. Then, we present specific challenges pertinent to each clustering category and introduce several representative approaches. We also discuss the problem of cluster validation in three aspects and review various methods to assess the quality and reliability of clustering results. Finally, we conclude this paper and suggest the promising trends in this field.},
author = {Jiang, Daxin Jiang Daxin and Tang, Chun Tang Chun and Zhang, Aidong Zhang Aidong},
doi = {10.1109/TKDE.2004.68},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
isbn = {1041-4347 VO  - 16},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {65,Index Terms- Microarray technology,clustering.,gene expression data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: a survey}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey#0},
volume = {16},
year = {2004}
}
@article{Kumari2014,
author = {Kumari, D Aruna and Bhavana, D Poojitha and Aditya, V Venkata Sai},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumari, Bhavana, Aditya - 2014 - Data Mining in Biodata Analysis.pdf:pdf},
number = {9},
pages = {1--3},
title = {{Data Mining in Biodata Analysis}},
volume = {14},
year = {2014}
}
@article{Weitschek2014,
abstract = {Specific fragments, coming from short portions of DNA (e.g., mitochondrial, nuclear, and plastid sequences), have been defined as DNA Barcode and can be used as markers for organisms of the main life kingdoms. Species classification with DNA Barcode sequences has been proven effective on different organisms. Indeed, specific gene regions have been identified as Barcode: COI in animals, rbcL and matK in plants, and ITS in fungi. The classification problem assigns an unknown specimen to a known species by analyzing its Barcode. This task has to be supported with reliable methods and algorithms. In this work the efficacy of supervised machine learning methods to classify species with DNA Barcode sequences is shown. The Weka software suite, which includes a collection of supervised classification methods, is adopted to address the task of DNA Barcode analysis. Classifier families are tested on synthetic and empirical datasets belonging to the animal, fungus, and plant kingdoms. In particular, the function-based method Support Vector Machines (SVM), the rule-based RIPPER, the decision tree C4.5, and the Na{\"{i}}ve Bayes method are considered. Additionally, the classification results are compared with respect to ad-hoc and well-established DNA Barcode classification methods. A software that converts the DNA Barcode FASTA sequences to the Weka format is released, to adapt different input formats and to allow the execution of the classification procedure. The analysis of results on synthetic and real datasets shows that SVM and Na{\"{i}}ve Bayes outperform on average the other considered classifiers, although they do not provide a human interpretable classification model. Rule-based methods have slightly inferior classification performances, but deliver the species specific positions and nucleotide assignments. On synthetic data the supervised machine learning methods obtain superior classification performances with respect to the traditional DNA Barcode classification methods. On empirical data their classification performances are at a comparable level to the other methods. The classification analysis shows that supervised machine learning methods are promising candidates for handling with success the DNA Barcoding species classification problem, obtaining excellent performances. To conclude, a powerful tool to perform species identification is now available to the DNA Barcoding community.},
author = {Weitschek, Emanuel and Fiscon, Giulia and Felici, Giovanni},
doi = {10.1186/1756-0381-7-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitschek, Fiscon, Felici - 2014 - Supervised DNA Barcodes species classification analysis, comparisons and results.pdf:pdf},
isbn = {1756-0381},
issn = {17560381},
journal = {BioData Mining},
keywords = {DNA Barcoding,Species identification,Supervised classification methods},
number = {1},
pages = {4},
pmid = {24721333},
publisher = {BioData Mining},
title = {{Supervised DNA Barcodes species classification: Analysis, comparisons and results}},
url = {http://www.biodatamining.org/content/7/1/4},
volume = {7},
year = {2014}
}
@article{Lauzon2016,
abstract = {Effective genome wide association studies (GWAS) presents new Big Data challenges for health researchers: data processing delays, data provenance and efficient real-time visualization. This paper presents two recent open source initiatives that, used together, aim at solving these issues. First, an introduction to GWAS is presented followed by a description of the issues faced by the bioinformatics staff at this small health research lab. We then introduce two open source project we initiated: a query engine (QnGene) and a genetic output analysis tool (GOAT) to address these issues and give an overview of their internal architecture and our current experimentation and validation plan. {\textcopyright} 2016 IEEE.},
author = {Lauzon, David and Kanzki, Beatriz and Dupuy, Victor and April, Alain and Phillips, Michael S. and Tremblay, Johanne and Hamet, Pavel},
doi = {10.1109/CHASE.2016.79},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lauzon et al. - 2016 - Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS).pdf:pdf},
isbn = {9781509009435},
journal = {Proceedings - 2016 IEEE 1st International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2016},
keywords = {Big Data,GWAS health systems provenance,GWAS visualization,dbSNP discrepancies,open-source},
pages = {382--387},
title = {{Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS)}},
year = {2016}
}
@article{Rosenberg2007,
abstract = {We present V-measure, an external entropy- based cluster evaluation measure. V- measure provides an elegant solution to many problems that affect previously de- fined cluster evaluation measures includ- ing 1) dependence on clustering algorithm or data set, 2) the problem of matching, where the clustering of only a portion of data points are evaluated and 3) accurate evalu- ation and combination of two desirable as- pects of clustering, homogeneity and com- pleteness. We compare V-measure to a num- ber of popular cluster evaluation measures and demonstrate that it satisfies several de- sirable properties of clustering solutions, us- ing simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.},
author = {Rosenberg, Andrew and Hirschberg, Julia},
doi = {10.7916/D80V8N84},
file = {:home/jennifer/Descargas/D07-1043.pdf:pdf},
journal = {Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language (EMNLP-CoNLL'07)},
number = {June},
pages = {410--420},
title = {{V-measure: A conditional entropy-based external cluster evaluation measure}},
url = {http://acl.ldc.upenn.edu/D/D07/D07-1043.pdf},
volume = {1},
year = {2007}
}
@article{Hegde2017,
abstract = {Context.—With the decrease in the cost of sequencing, the clinical testing paradigm has shifted from single gene to gene panel and now whole-exome and whole-genome sequencing. Clinical laboratories are rapidly implementing next-generation sequencing–based whole-exome and whole-genome sequencing. Because a large number of targets are covered by whole-exome and whole-genome sequencing, it is critical that a laboratory perform appropriate validation studies, develop a quality assurance and quality control program, and participate in proficiency testing. Objective.—To provide recommendations for whole-exome and whole-genome sequencing assay design, validation, and implementation for the detection of germ-line variants associated in inherited disorders. Data Sources.—An example of trio sequencing, filtration and annotation of variants, and phenotypic consideration to arrive at clinical diagnosis is discussed. Conclusions.—It is critical that clinical laboratories planning to implement whole-exome and whole-genome sequencing design and validate the assay to specifications and ensure adequate performance prior to implementa-tion. Test design specifications, including variant filtering and annotation, phenotypic consideration, guidance on consenting options, and reporting of incidental findings, are provided. These are important steps a laboratory must take to validate and implement whole-exome and whole-genome sequencing in a clinical setting for germline variants in inherited disorders.},
author = {Hegde, Madhuri and Santani, Avni and Mao, Rong and Ferreira-Gonzalez, Andrea and Weck, Karen E. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2016-0622-RA},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hegde et al. - 2017 - Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {798--805},
pmid = {28362156},
title = {{Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants in inherited disease}},
volume = {141},
year = {2017}
}
@misc{Littlefielda,
author = {Littlefield, Rayan},
title = {{An introduction into Data Mining in Bioinformatics.}},
url = {https://littlefield.co/an-introduction-into-data-mining-in-bioinformatics-964511e9ea21},
urldate = {2017-11-19}
}
@article{Medina2016,
abstract = {As sequencing technologies progress, the amount of data produced grows exponentially, shifting the bottleneck of discovery towards the data analysis phase. In particular, currently available mapping solutions for RNA-seq leave room for improvement in terms of sensitivity and performance, hindering an efficient analysis of transcriptomes by massive sequencing. Here, we present an innovative approach that combines re-engineering, optimization and parallelization. This solution results in a significant increase of mapping sensitivity over a wide range of read lengths and substantial shorter runtimes when compared with current RNA-seq mapping methods available.},
author = {Medina, I. and T{\'{a}}rraga, J. and Mart{\'{i}}nez, H. and Barrachina, S. and Castillo, M. I. and Paschall, J. and Salavert-Torres, J. and Blanquer-Espert, I. and Hern{\'{a}}ndez-Garc{\'{i}}a, V. and Quintana-Ort{\'{i}}, E. S. and Dopazo, J.},
doi = {10.1093/dnares/dsv039},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medina et al. - 2016 - Highly sensitive and ultrafast read mapping for RNA-seq analysis.pdf:pdf},
issn = {17561663},
journal = {DNA Research},
keywords = {Burrows-Wheeler Transform,RNA-seq,high-performance computing,mapping},
number = {2},
pages = {93--100},
pmid = {26740642},
title = {{Highly sensitive and ultrafast read mapping for RNA-seq analysis}},
url = {http://dnaresearch.oxfordjournals.org/content/early/2016/01/05/dnares.dsv039.full},
volume = {23},
year = {2016}
}
@article{ORawe2013,
abstract = {Background: To facilitate the clinical implementation of genomic medicine by next-generation sequencing, it will be critically important to obtain accurate and consistent variant calls on personal genomes. Multiple software tools for variant calling are available, but it is unclear how comparable these tools are or what their relative merits in real-world scenarios might be. Methods: We sequenced 15 exomes from four families using commercial kits (Illumina HiSeq 2000 platform and Agilent SureSelect version 2 capture kit), with approximately 120X mean coverage. We analyzed the raw data using near-default parameters with five different alignment and variant-calling pipelines (SOAP, BWA-GATK, BWA-SNVer, GNUMAP, and BWA-SAMtools). We additionally sequenced a single whole genome using the sequencing and analysis pipeline from Complete Genomics (CG), with 95% of the exome region being covered by 20 or more reads per base. Finally, we validated 919 single-nucleotide variations (SNVs) and 841 insertions and deletions (indels), including similar fractions of GATK-only, SOAP-only, and shared calls, on the MiSeq platform by amplicon sequencing with approximately 5000X mean coverage. Results: SNV concordance between five Illumina pipelines across all 15 exomes was 57.4%, while 0.5 to 5.1% of variants were called as unique to each pipeline. Indel concordance was only 26.8% between three indel-calling pipelines, even after left-normalizing and intervalizing genomic coordinates by 20 base pairs. There were 11% of CG variants falling within targeted regions in exome sequencing that were not called by any of the Illumina-based exome analysis pipelines. Based on targeted amplicon sequencing on the MiSeq platform, 97.1%, 60.2%, and 99.1% of the GATK-only, SOAP-only and shared SNVs could be validated, but only 54.0%, 44.6%, and 78.1% of the GATK-only, SOAP-only and shared indels could be validated. Additionally, our analysis of two families (one with four individuals and the other with seven), demonstrated additional accuracy gained in variant discovery by having access to genetic data from a multi-generational family. Conclusions: Our results suggest that more caution should be exercised in genomic medicine settings when analyzing individual genomes, including interpreting positive and negative findings with scrutiny, especially for indels. We advocate for renewed collection and sequencing of multi-generational families to increase the overall accuracy of whole genomes.},
author = {O'Rawe, Jason and Jiang, Tao and Sun, Guangqing and Wu, Yiyang and Wang, Wei and Hu, Jingchu and Bodily, Paul and Tian, Lifeng and Hakonarson, Hakon and Johnson, W. Evan and Wei, Zhi and Wang, Kai and Lyon, Gholson J.},
doi = {10.1186/gm432},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Rawe et al. - 2013 - Low concordance of multiple variant-calling pipelines practical implications for exome and genome sequencing.pdf:pdf},
isbn = {1756-994X (Print)},
issn = {1756994X},
journal = {Genome Medicine},
number = {3},
pages = {28},
pmid = {23537139},
title = {{Low concordance of multiple variant-calling pipelines: Practical implications for exome and genome sequencing}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3706896&tool=pmcentrez&rendertype=abstract},
volume = {5},
year = {2013}
}
@article{Oellrich2014,
abstract = {MOTIVATION: Large-scale phenotyping projects such as the Sanger Mouse Genetics project are ongoing efforts to help identify the influences of genes and their modification on phenotypes. Gene-phenotype relations are crucial to the improvement of our understanding of human heritable diseases as well as the development of drugs. However, given that there are ∼: 20 000 genes in higher vertebrate genomes and the experimental verification of gene-phenotype relations requires a lot of resources, methods are needed that determine good candidates for testing. RESULTS: In this study, we applied an association rule mining approach to the identification of promising secondary phenotype candidates. The predictions rely on a large gene-phenotype annotation set that is used to find occurrence patterns of phenotypes. Applying an association rule mining approach, we could identify 1967 secondary phenotype hypotheses that cover 244 genes and 136 phenotypes. Using two automated and one manual evaluation strategies, we demonstrate that the secondary phenotype candidates possess biological relevance to the genes they are predicted for. From the results we conclude that the predicted secondary phenotypes constitute good candidates to be experimentally tested and confirmed. AVAILABILITY: The secondary phenotype candidates can be browsed through at http://www.sanger.ac.uk/resources/databases/phenodigm/gene/secondaryphenotype/list. CONTACT: ao5@sanger.ac.uk or ds5@sanger.ac.uk SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Oellrich, Anika and Jacobsen, Julius and Papatheodorou, Irene and Smedley, Damian},
doi = {10.1093/bioinformatics/btu260},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oellrich et al. - 2014 - Using association rule mining to determine promising secondary phenotyping hypotheses.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
month = {jun},
number = {12},
pages = {i52--i59},
pmid = {24932005},
publisher = {Oxford University Press},
title = {{Using association rule mining to determine promising secondary phenotyping hypotheses}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btu260},
volume = {30},
year = {2014}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zaki, Mohammed J. and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD)(2).pdf:pdf},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for Molecular Biology},
month = {apr},
number = {1},
pages = {4},
pmid = {17428327},
publisher = {BioMed Central},
title = {{Data mining in bioinformatics (BIOKDD)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17428327 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1852315},
volume = {2},
year = {2007}
}
@article{Robinson2017,
abstract = {Manual review of aligned reads for confirmation and interpretation of variant calls is an important step in many variant calling pipelines for next-generation sequencing (NGS) data. Visual inspection can greatly increase the confidence in calls, reduce the risk of false positives, and help characterize complex events. The Integrative Genomics Viewer (IGV) was one of the first tools to provide NGS data visualization, and it currently provides a rich set of tools for inspection, validation, and interpretation of NGS datasets, as well as other types of genomic data. Here, we present a short overview of IGV's variant review features for both single-nucleotide variants and structural variants, with examples from both cancer and germline datasets. IGV is freely available at https://www.igv.orgCancer Res; 77(21); e31-34. {\textcopyright}2017 AACR.},
author = {Robinson, James T. and Thorvaldsd{\'{o}}ttir, Helga and Wenger, Aaron M. and Zehir, Ahmet and Mesirov, Jill P.},
doi = {10.1158/0008-5472.CAN-17-0337},
issn = {15387445},
journal = {Cancer Research},
month = {nov},
number = {21},
pages = {e31--e34},
pmid = {29092934},
title = {{Variant review with the integrative genomics viewer}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29092934 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5678989 http://cancerres.aacrjournals.org/lookup/doi/10.1158/0008-5472.CAN-17-0337},
volume = {77},
year = {2017}
}
@article{Moore2010,
abstract = {MOTIVATION The sequencing of the human genome has made it possible to identify an informative set of >1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs). The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing. This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies. However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing. One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology. Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context. There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype-phenotype relationship that is characterized by significant heterogeneity and gene-gene and gene-environment interaction. We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases. The goal of this review is to identify and discuss those GWAS challenges that will require computational methods.},
author = {Moore, Jason H. and Asselbergs, Folkert W. and Williams, Scott M.},
doi = {10.1093/bioinformatics/btp713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Asselbergs, Williams - 2010 - Bioinformatics challenges for genome-wide association studies.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pages = {445--455},
pmid = {20053841},
title = {{Bioinformatics challenges for genome-wide association studies}},
volume = {26},
year = {2010}
}
@article{Bash2015,
abstract = {Kajian perbandingan pengukuran kecekapan pengurusan kewangan institusi Majlis Agama Islam Negeri (MAIN) dengan prestasi agihan kewangan dan bukan kewangan di antara institusi zakat boleh membuka ruang untuk meningkatkan mutu agihan institusi zakat. Pengurusan kewangan sesebuah organisasi adalah penting untuk memastikan institusi tersebut dapat mencapai matlamat yang telah ditentukan. Hal ini kerana masih wujud tanggapan negatif ketidakcekapan agihan institusi zakat dan ia merupakan antara salah satu punca yang telah menjejaskan tahap keyakinan masyarakat Islam untuk menjalankan kewajipan membayar zakat kepada institusi zakat. Ketidakcekapan agihan tersebut boleh dilihat melalui lebihan zakat yang tidak diagihkan dan kegagalan institusi zakat mengagihkan zakat kepada kelapan-lapan golongan asnaf dan tidak mengikut keutamaan asnaf. Persoalannya adakah wujud hubungan antara prestasi pengurusan kewangan dan prestasi pengagihan zakat. Kajian telah mengkategorikan prestasi pengurusan kewangan kepada tiga (3) bahagian iaitu kecairan, kesolvenan dan keuntungan; manakala aspek prestasi kecekapan agihan institusi zakat pula terbahagi kepada dua (2) bahagian iaitu kecekapan agihan berbentuk kewangan (lebihan zakat tahunan) dan bukan kewangan (keutamaan asnaf). Kajian ini juga mengkaji corak agihan zakat kepada setiap asnaf sebagai satu tambahan pengukuran prestasi agihan zakat. Data sekunder terdiri daripada laporan tahunan MAIN telah dianalisis bermula tahun 2000 hingga 2013. Kajian ini mengkaji lima institusi MAIN iaitu Majlis Agama Islam Selangor (MAIS) dan Majlis Agama Islam Negeri Pulau Pinang (MAINPP) yang mewakili institusi yang telah mengkorporatkan kutipan dan agihan zakat; dan Majlis Agama Islam Johor (MAIJ), Majlis Agama Islam dan Adat Melayu Terengganu (MAIDAM) dan Majlis Ugama Islam Sabah (MUIS) yang mewakili negeri yang tidak mengkorporatkan kutipan dan agihan zakat. Hasil kajian mendapati secara umumnya wujud hubungan yang sepadan antara prestasi pengurusan kewangan MAIN dengan prestasi pengagihan zakat dalam aspek agihan kewangan dan bukan kewangan. Hasil kajian juga mendapati corak agihan zakat oleh institusi korporat adalah berbeza dengan corak agihan zakat oleh institusi yang tidak korporat pengurusan zakat. Beberapa cadangan dan implikasi dasar turut dicadangkan dalam kajian ini. ABSTRACT},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wahid, Hairunnizam and Ahmad, Sanep and Nor, Mohd Ali Mohd and Rashid, Maryam Abd},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bash - 2015 - VARIATION INTERPRETATION PREDICTORS PRINCIPLES, TYPES, PERFORMANCE AND CHOICE.pdf:pdf},
isbn = {9788578110796},
issn = {01261962},
journal = {Jurnal Ekonomi Malaysia},
keywords = {Financial management efficiency performance,State Islamic Religious Council,Zakat distribution efficiency},
number = {2},
pages = {39--54},
pmid = {25246403},
title = {{Prestasi kecekapan pengurusan kewangan dan agihan zakat: perbandingan antara majlis agama islam negeri di Malaysia}},
volume = {51},
year = {2017}
}
@article{Bajcsy2005,
abstract = {Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.},
author = {Bajcsy, Peter and Han, Jiawei and Liu, Lei and Yang, Jiong},
doi = {10.1007/1-84628-059-1_2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bajcsy et al. - 2005 - Survey of Biodata Analysis from a Data Mining Perspective.pdf:pdf},
isbn = {1852336714},
journal = {Data Mining in Bioinformatics},
pages = {9--39},
title = {{Survey of Biodata Analysis from a Data Mining Perspective}},
url = {http://dx.doi.org/10.1007/1-84628-059-1_2},
year = {2005}
}
@book{Quinlan2014,
abstract = {Technological advances have enabled the use of DNA sequencing as a flexible tool to characterize genetic variation and to measure the activity of diverse cellular phenomena such as gene isoform expression and transcription factor binding. Extracting biological insight from the experiments enabled by these advances demands the analysis of large, multi-dimensional datasets. This unit describes the use of the BEDTools toolkit for the exploration of high-throughput genomics datasets. Several protocols are presented for common genomic analyses, demonstrating how simple BEDTools operations may be combined to create bespoke pipelines addressing complex questions. Curr. Protoc. Bioinform. 47:11.12.1-11.12.34. {\textcopyright} 2014 by John Wiley & Sons, Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Quinlan, Aaron R.},
booktitle = {Current Protocols in Bioinformatics},
doi = {10.1002/0471250953.bi1112s47},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan - 2014 - BEDTools The Swiss-Army tool for genome feature analysis.pdf:pdf},
isbn = {0471250953},
issn = {1934340X},
keywords = {Bioinformatics,Genome analysis,Genome features,Genome intervals,Genomics},
pages = {11.12.1--11.12.34},
pmid = {25199790},
title = {{BEDTools: The Swiss-Army tool for genome feature analysis}},
volume = {2014},
year = {2014}
}
@article{Hubbard2002,
abstract = {The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.},
author = {Hubbard, T.},
doi = {10.1093/nar/30.1.38},
isbn = {1362-4962 (Electronic)},
issn = {13624962},
journal = {Nucleic Acids Research},
month = {jan},
number = {1},
pages = {38--41},
pmid = {11752248},
publisher = {Oxford University Press},
title = {{The Ensembl genome database project}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/30.1.38},
volume = {30},
year = {2002}
}
@misc{FBI,
author = {{U.S. Government}},
number = {20 June, 2010},
pages = {https://www.fbi.gov/services/laboratory/biometric--},
title = {{Combined DNA Index System}},
url = {http://www.dna.gov/dna-databases/codis},
volume = {2010},
year = {2010}
}
@article{Mohammed2014,
abstract = {The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis. This so called “big data” challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields. Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability. It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structured/unstructured data. The MapReduce programming framework uses two tasks common in functional programming: Map and Reduce. MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters. Compared with existing parallel processing paradigms (e.g. grid computing and graphical processing unit (GPU)), MapReduce and Hadoop have two advantages: 1) fault-tolerant storage resulting in reliable data processing by replicating the computing tasks, and cloning the data chunks on different computing nodes across the computing cluster; 2) high-throughput data processing via a batch processing framework and the Hadoop distributed file system (HDFS). Data are stored in the HDFS and made available to the slave nodes for computation. In this paper, we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields. The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization, and opens up new opportunities in the emerging era of big data analytics. The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools. This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields.},
author = {Mohammed, Emad A. and Far, Behrouz H. and Naugler, Christopher},
doi = {10.1186/1756-0381-7-22},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammed, Far, Naugler - 2014 - Applications of the MapReduce programming framework to clinical big data analysis current landscape and.pdf:pdf},
isbn = {1756-0381 (Linking)},
issn = {17560381},
journal = {BioData Mining},
keywords = {Big data,Bioinformatics,Clinical big data analysis,Clinical data analysis,Distributed programming,Hadoop,MapReduce},
number = {1},
pages = {1--23},
pmid = {25383096},
title = {{Applications of the MapReduce programming framework to clinical big data analysis: Current landscape and future trends}},
url = {http://link.springer.com/article/10.1186/1756-0381-7-22%5Cnhttp://link.springer.com/content/pdf/10.1186/1756-0381-7-22.pdf},
volume = {7},
year = {2014}
}
@article{Parnell2014,
abstract = {BACKGROUND: Genetic understanding of complex traits has developed immensely over the past decade but remains hampered by incomplete descriptions of contribution to phenotypic variance. Gene-environment (GxE) interactions are one of these contributors and in the guise of diet and physical activity are important modulators of cardiometabolic phenotypes and ensuing diseases.\n\nRESULTS: We mined the scientific literature to collect GxE interactions from 386 publications for blood lipids, glycemic traits, obesity anthropometrics, vascular measures, inflammation and metabolic syndrome, and introduce CardioGxE, a gene-environment interaction resource. We then analyzed the genes and SNPs supporting cardiometabolic GxEs in order to demonstrate utility of GxE SNPs and to discern characteristics of these important genetic variants. We were able to draw many observations from our extensive analysis of GxEs. 1) The CardioGxE SNPs showed little overlap with variants identified by main effect GWAS, indicating the importance of environmental interactions with genetic factors on cardiometabolic traits. 2) These GxE SNPs were enriched in adaptation to climatic and geographical features, with implications on energy homeostasis and response to physical activity. 3) Comparison to gene networks responding to plasma cholesterol-lowering or regression of atherosclerotic plaques showed that GxE genes have a greater role in those responses, particularly through high-energy diets and fat intake, than do GWAS-identified genes for the same traits. Other aspects of the CardioGxE dataset were explored.\n\nCONCLUSIONS: Overall, we demonstrate that SNPs supporting cardiometabolic GxE interactions often exhibit transcriptional effects or are under positive selection. Still, not all such SNPs can be assigned potential functional or regulatory roles often because data are lacking in specific cell types or from treatments that approximate the environmental factor of the GxE. With research on metabolic related complex disease risk embarking on genome-wide GxE interaction tests, CardioGxE will be a useful resource.},
author = {Parnell, Laurence D. and Blokker, Britt A. and Dashti, Hassan S. and Nesbeth, Paula Dene and Cooper, Brittany Elle and Ma, Yiyi and Lee, Yu Chi and Hou, Ruixue and Lai, Chao Qiang and Richardson, Kris and Ordov{\'{a}}s, Jos{\'{e}} M.},
doi = {10.1186/1756-0381-7-21},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnell et al. - 2014 - CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.pdf:pdf},
isbn = {1759-5029},
issn = {17560381},
journal = {BioData Mining},
keywords = {Cardiovascular diseases,Diet,Gene-environment interaction,Genetic variants,Phenotypic variance,Physical activity,Type 2 diabetes},
number = {1},
pages = {21},
pmid = {25368670},
title = {{CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4217104&tool=pmcentrez&rendertype=abstract},
volume = {7},
year = {2014}
}
@article{Pietrelli2017a,
abstract = {Next-generation sequencing technologies have become the most powerful tool to discover genetic variants associated with human diseases. Although the dramatic reductions in the costs facilitate the use in the wet-lab and clinics, the huge amount of data generated renders their management by non-expert researchers and physicians extremely difficult. Therefore, there is an urgent need of novel approaches and tools aimed at getting the 'end-users' closer to the sequencing data, facilitating the access by non-bioinformaticians, and to speed-up the functional interpretation of genetic variants. We developed myVCF, a standalone, easy-to-use desktop application, which is based on a browser interface and is suitable for Windows, Mac and UNIX systems. myVCF is an efficient platform that is able to manage multiple sequencing projects created from VCF files within the system; stores genetic variants and samples genotypes from an annotated VCF files into a SQLite database; implements a flexible search engine for data exploration, allowing to query for chromosomal region, gene, single variant or dbSNP ID. Besides, myVCF generates a summary statistics report about mutations distribution across samples and across the genome/exome by aggregating the information within the VCF file. In summary, the myVCF platform allows end-users without strong programming and bioinformatics skills to explore, query, visualize and export mutations data in a simple and straightforward way. https://apietrelli.github.io/myVCF/. pietrelli@ingm.org. Supplementary data are available at Bioinformatics online.},
author = {Pietrelli, Alessandro and Valenti, Luca},
doi = {10.1093/bioinformatics/btx475},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pietrelli, Valenti - 2017 - myVCF a desktop application for high-throughput mutations data management.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {3676--3678},
pmid = {29036298},
publisher = {Oxford University Press},
title = {{MyVCF: A desktop application for high-throughput mutations data management}},
url = {http://academic.oup.com/bioinformatics/article/33/22/3676/4004873},
volume = {33},
year = {2017}
}
@article{Naulaerts,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - Unknown - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Association rule,Biclustering,Frequent item set,Market basket analysis,Pattern mining},
number = {2},
pages = {216--231},
pmid = {24162173},
title = {{A primer to frequent itemset mining for bioinformatics}},
volume = {16},
year = {2015}
}
@article{Hwang2015,
abstract = {The success of clinical genomics using next generation sequencing (NGS) requires the accurate and consistent identification of personal genome variants. Assorted variant calling methods have been developed, which show low concordance between their calls. Hence, a systematic comparison of the variant callers could give important guidance to NGS-based clinical genomics. Recently, a set of high-confident variant calls for one individual (NA12878) has been published by the Genome in a Bottle (GIAB) consortium, enabling performance benchmarking of different variant calling pipelines. Based on the gold standard reference variant calls from GIAB, we compared the performance of thirteen variant calling pipelines, testing combinations of three read aligners-BWA-MEM, Bowtie2, and Novoalign-and four variant callers-Genome Analysis Tool Kit HaplotypeCaller (GATK-HC), Samtools mpileup, Freebayes and Ion Proton Variant Caller (TVC), for twelve data sets for the NA12878 genome sequenced by different platforms including Illumina2000, Illumina2500, and Ion Proton, with various exome capture systems and exome coverage. We observed different biases toward specific types of SNP genotyping errors by the different variant callers. The results of our study provide useful guidelines for reliable variant identification from deep sequencing of personal genomes.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Hwang, Sohyun and Kim, Eiru and Lee, Insuk and Marcotte, Edward M.},
doi = {10.1038/srep17875},
eprint = {1512.00567},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang et al. - 2015 - Systematic comparison of variant calling pipelines using gold standard personal exome variants.pdf:pdf},
isbn = {8010628980},
issn = {20452322},
journal = {Scientific Reports},
number = {December},
pages = {17875},
pmid = {26639839},
publisher = {Nature Publishing Group},
title = {{Systematic comparison of variant calling pipelines using gold standard personal exome variants}},
url = {http://www.nature.com/articles/srep17875%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26639839%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4671096},
volume = {5},
year = {2015}
}
@article{Bamshad2011,
abstract = {Nature Reviews Genetics 12, 745 (2011). doi:10.1038/nrg3031},
archivePrefix = {arXiv},
arxivId = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
author = {Bamshad, Michael J. and Ng, Sarah B. and Bigham, Abigail W. and Tabor, Holly K. and Emond, Mary J. and Nickerson, Deborah A. and Shendure, Jay},
doi = {10.1038/nrg3031},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164&tool=pmcentrez&rendertype=abstract.},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bamshad et al. - 2011 - Exome sequencing as a tool for Mendelian disease gene discovery.pdf:pdf},
isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
issn = {14710056},
journal = {Nature Reviews Genetics},
keywords = {Alleles,Base Sequence,Exome,Exome: genetics,Genetic Predisposition to Disease,Genome, Human,Genome-Wide Association Study,Humans,Molecular Sequence Data,Pedigree,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {11},
pages = {745--755},
pmid = {21946919},
primaryClass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:},
publisher = {Nature Publishing Group},
title = {{Exome sequencing as a tool for Mendelian disease gene discovery}},
url = {http://dx.doi.org/10.1038/nrg3031%5Cnpapers2://publication/doi/10.1038/nrg3031},
volume = {12},
year = {2011}
}
@article{Conesa2016,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Michal Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
doi = {10.1186/s13059-016-0881-8},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conesa et al. - 2016 - A survey of best practices for RNA-seq data analysis.pdf:pdf},
isbn = {1474-760X (Electronic)\r1474-7596 (Linking)},
issn = {1474760X},
journal = {Genome Biology},
keywords = {Animal Genetics and Genomics,Bioinformatics,Evolutionary Biology,Human Genetics,Microbial Genetics and Genomics,Plant Genetics & Genomics},
number = {1},
pages = {13},
pmid = {26813401},
title = {{A survey of best practices for RNA-seq data analysis}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0881-8},
volume = {17},
year = {2016}
}
@article{Jiang2004,
abstract = { DNA microarray technology has now made it possible to simultaneously monitor the expression levels of thousands of genes during important biological processes and across collections of related samples. Elucidating the patterns hidden in gene expression data offers a tremendous opportunity for an enhanced understanding of functional genomics. However, the large number of genes and the complexity of biological networks greatly increases the challenges of comprehending and interpreting the resulting mass of data, which often consists of millions of measurements. A first step toward addressing this challenge is the use of clustering techniques, which is essential in the data mining process to reveal natural structures and identify interesting patterns in the underlying data. Cluster analysis seeks to partition a given data set into groups based on specified features so that the data points within a group are more similar to each other than the points in different groups. A very rich literature on cluster analysis has developed over the past three decades. Many conventional clustering algorithms have been adapted or directly applied to gene expression data, and also new algorithms have recently been proposed specifically aiming at gene expression data. These clustering algorithms have been proven useful for identifying biologically relevant groups of genes and samples. In this paper, we first briefly introduce the concepts of microarray technology and discuss the basic elements of clustering on gene expression data. In particular, we divide cluster analysis for gene expression data into three categories. Then, we present specific challenges pertinent to each clustering category and introduce several representative approaches. We also discuss the problem of cluster validation in three aspects and review various methods to assess the quality and reliability of clustering results. Finally, we conclude this paper and suggest the promising trends in this field.},
author = {Jiang, Daxin Jiang Daxin and Tang, Chun Tang Chun and Zhang, Aidong Zhang Aidong},
doi = {10.1109/TKDE.2004.68},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
isbn = {1041-4347 VO  - 16},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {65,Index Terms- Microarray technology,clustering.,gene expression data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: a survey}},
url = {http://scholar.google.com/scholar?hl=en%7B&%7DbtnG=Search%7B&%7Dq=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey%7B#%7D0},
volume = {16},
year = {2004}
}
@article{Rishishwar2015,
abstract = {The human dimension of the Columbian Exchange entailed substantial genetic admixture between ancestral source populations from Africa, the Americas and Europe, which had evolved separately for many thousands of years. We sought to address the implications of the creation of admixed American genomes, containing novel allelic combinations, for human health and fitness via analysis of an admixed Colombian population from Medellin. Colombian genomes from Medellin show a wide range of three-way admixture contributions from ancestral source populations. The primary ancestry component for the population is European (average = 74.6%, range = 45.0%-96.7%), followed by Native American (average = 18.1%, range = 2.1%-33.3%) and African (average = 7.3%, range = 0.2%-38.6%). Locus-specific patterns of ancestry were evaluated to search for genomic regions that are enriched across the population for particular ancestry contributions. Adaptive and innate immune system related genes and pathways are particularly over-represented among ancestry-enriched segments, including genes (HLA-B and MAPK10) that are involved in defense against endemic pathogens such as malaria. Genes that encode functions related to skin pigmentation (SCL4A5) and cutaneous glands (EDAR) are also found in regions with anomalous ancestry patterns. These results suggest the possibility that ancestry-specific loci were differentially retained in the modern admixed Colombian population based on their utility in the New World environment.},
author = {Rishishwar, Lavanya and Conley, Andrew B. and Wigington, Charles H. and Wang, Lu and Valderrama-Aguirre, Augusto and {King Jordan}, I.},
doi = {10.1038/srep12376},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rishishwar et al. - 2015 - Ancestry, admixture and fitness in Colombian genomes(2).pdf:pdf},
isbn = {2045-2322 (ISSNLinking)},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {12376},
pmid = {26197429},
publisher = {Nature Publishing Group},
title = {{Ancestry, admixture and fitness in Colombian genomes}},
url = {http://www.nature.com/articles/srep12376},
volume = {5},
year = {2015}
}
@article{Roy2018,
abstract = {Bioinformatics pipelines are an integral component of next-generation sequencing (NGS). Processing raw sequence data to detect genomic alterations has significant impact on disease management and patient care. Because of the lack of published guidance, there is currently a high degree of variability in how members of the global molecular genetics and pathology community establish and validate bioinformatics pipelines. Improperly developed, validated, and/or monitored pipelines may generate inaccurate results that may have negative consequences for patient care. To address this unmet need, the Association of Molecular Pathology, with organizational representation from the College of American Pathologists and the American Medical Informatics Association, has developed a set of 17 best practice consensus recommendations for the validation of clinical NGS bioinformatics pipelines. Recommendations include practical guidance for laboratories regarding NGS bioinformatics pipeline design, development, and operation, with additional emphasis on the role of a properly trained and qualified molecular professional to achieve optimal NGS testing quality.},
author = {Roy, Somak and Coldren, Christopher and Karunamurthy, Arivarasan and Kip, Nefize S. and Klee, Eric W. and Lincoln, Stephen E. and Leon, Annette and Pullambhatla, Mrudula and Temple-Smolkin, Robyn L. and Voelkerding, Karl V. and Wang, Chen and Carter, Alexis B.},
doi = {10.1016/j.jmoldx.2017.11.003},
file = {:home/jennifer/Descargas/PIIS1525157817303732.pdf:pdf},
isbn = {1943-7811},
issn = {19437811},
journal = {Journal of Molecular Diagnostics},
number = {1},
pages = {4--27},
pmid = {29154853},
title = {{Standards and Guidelines for Validating Next-Generation Sequencing Bioinformatics Pipelines: A Joint Recommendation of the Association for Molecular Pathology and the College of American Pathologists}},
volume = {20},
year = {2018}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@techreport{Chu2011,
abstract = {The advent of high-throughput sequencing (HTS) methods has enabled direct approaches to quantitatively profile small RNA populations. However, these methods have been limited by several factors, including representational artifacts and lack of established statistical methods of analysis. Furthermore, massive HTS data sets present new problems related to data processing and mapping to a reference genome. Here, we show that cluster-based sequencing-by-synthesis technology is highly reproducible as a quantitative profiling tool for several classes of small RNA from Arabidopsis thaliana. We introduce the use of synthetic RNA oligoribonucleotide standards to facilitate objective normalization between HTS data sets, and adapt microarray-type methods for statistical analysis of multiple samples. These methods were tested successfully using mutants with small RNA biogenesis (miRNA-defective dcl1 mutant and siRNA-defective dcl2 dcl3 dcl4 triple mutant) or effector protein (ago1 mutant) deficiencies. Computational methods were also developed to rapidly and accurately parse, quantify, and map small RNA data.},
author = {Fahlgren, N. and Sullivan, C. M. and Kasschau, K. D. and Chapman, E. J. and Cumbie, J. S. and Montgomery, T. A. and Gilbert, S. D. and Dasenko, M. and Backman, T. W.H. and Givan, S. A. and Carrington, J. C.},
booktitle = {Rna},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
isbn = {1469-9001 (Electronic)\r1355-8382 (Linking)},
issn = {1355-8382},
keywords = {gene expression,microarrays,significance analysi},
number = {5},
pages = {992--1002},
pmid = {19307293},
title = {{Computational and analytical framework for small RNA profiling by high-throughput sequencing}},
url = {http://rnajournal.cshlp.org/cgi/doi/10.1261/rna.1473809},
volume = {15},
year = {2009}
}
@article{Bash2015,
abstract = {Kajian perbandingan pengukuran kecekapan pengurusan kewangan institusi Majlis Agama Islam Negeri (MAIN) dengan prestasi agihan kewangan dan bukan kewangan di antara institusi zakat boleh membuka ruang untuk meningkatkan mutu agihan institusi zakat. Pengurusan kewangan sesebuah organisasi adalah penting untuk memastikan institusi tersebut dapat mencapai matlamat yang telah ditentukan. Hal ini kerana masih wujud tanggapan negatif ketidakcekapan agihan institusi zakat dan ia merupakan antara salah satu punca yang telah menjejaskan tahap keyakinan masyarakat Islam untuk menjalankan kewajipan membayar zakat kepada institusi zakat. Ketidakcekapan agihan tersebut boleh dilihat melalui lebihan zakat yang tidak diagihkan dan kegagalan institusi zakat mengagihkan zakat kepada kelapan-lapan golongan asnaf dan tidak mengikut keutamaan asnaf. Persoalannya adakah wujud hubungan antara prestasi pengurusan kewangan dan prestasi pengagihan zakat. Kajian telah mengkategorikan prestasi pengurusan kewangan kepada tiga (3) bahagian iaitu kecairan, kesolvenan dan keuntungan; manakala aspek prestasi kecekapan agihan institusi zakat pula terbahagi kepada dua (2) bahagian iaitu kecekapan agihan berbentuk kewangan (lebihan zakat tahunan) dan bukan kewangan (keutamaan asnaf). Kajian ini juga mengkaji corak agihan zakat kepada setiap asnaf sebagai satu tambahan pengukuran prestasi agihan zakat. Data sekunder terdiri daripada laporan tahunan MAIN telah dianalisis bermula tahun 2000 hingga 2013. Kajian ini mengkaji lima institusi MAIN iaitu Majlis Agama Islam Selangor (MAIS) dan Majlis Agama Islam Negeri Pulau Pinang (MAINPP) yang mewakili institusi yang telah mengkorporatkan kutipan dan agihan zakat; dan Majlis Agama Islam Johor (MAIJ), Majlis Agama Islam dan Adat Melayu Terengganu (MAIDAM) dan Majlis Ugama Islam Sabah (MUIS) yang mewakili negeri yang tidak mengkorporatkan kutipan dan agihan zakat. Hasil kajian mendapati secara umumnya wujud hubungan yang sepadan antara prestasi pengurusan kewangan MAIN dengan prestasi pengagihan zakat dalam aspek agihan kewangan dan bukan kewangan. Hasil kajian juga mendapati corak agihan zakat oleh institusi korporat adalah berbeza dengan corak agihan zakat oleh institusi yang tidak korporat pengurusan zakat. Beberapa cadangan dan implikasi dasar turut dicadangkan dalam kajian ini. ABSTRACT},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wahid, Hairunnizam and Ahmad, Sanep and Nor, Mohd Ali Mohd and Rashid, Maryam Abd},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {01261962},
journal = {Jurnal Ekonomi Malaysia},
keywords = {Financial management efficiency performance,State Islamic Religious Council,Zakat distribution efficiency},
number = {2},
pages = {39--54},
pmid = {25246403},
title = {{Prestasi kecekapan pengurusan kewangan dan agihan zakat: perbandingan antara majlis agama islam negeri di Malaysia}},
volume = {51},
year = {2017}
}
@article{Terlizzi2017,
abstract = {BACKGROUND The effect of complex alleles in cystic fibrosis (CF) is poorly defined for the lack of functional studies. OBJECTIVES To describe the genotype-phenotype correlation and the results of either in vitro and ex vivo studies performed on nasal epithelial cells (NEC) in a cohort of patients with CF carrying cystic fibrosis transmembrane conductance regulator (CFTR) complex alleles. METHODS We studied 70 homozygous, compound heterozygous or heterozygous for CFTR mutations: p.[Arg74Trp;Val201Met;Asp1270Asn], n=8; p.[Ile148Thr;Ile1023_Val1024del], n=5; p.[Arg117Leu;Leu997Phe], n=6; c.[1210-34TG[12];1210-12T[5];2930C>T], n=3; p.[Arg74Trp;Asp1270Asn], n=4; p.Asp1270Asn, n=2; p.Ile148Thr, n=6; p.Leu997Phe, n=36. In 39 patients, we analysed the CFTR gating activity on NEC in comparison with patients with CF (n=8) and carriers (n=4). Finally, we analysed in vitro the p.[Arg74Trp;Val201Met;Asp1270Asn] complex allele. RESULTS The p.[Ile148Thr;Ile1023_Val1024del] caused severe CF in five compound heterozygous with a class I-II mutation. Their CFTR activity on NEC was comparable with patients with two class I-II mutations (mean 7.3% vs 6.9%). The p.[Arg74Trp;Asp1270Asn] and the p.Asp1270Asn have scarce functional effects, while p.[Arg74Trp;Val201Met;Asp1270Asn] caused mild CF in four of five subjects carrying a class I-II mutation in trans, or CFTR-related disorders (CFTR-RD) in three having in trans a class IV-V mutation. The p.[Arg74Trp;Val201Met;Asp1270Asn] causes significantly (p<0.001) higher CFTR activity compared with compound heterozygous for class I-II mutations. Furthermore, five of six compounds heterozygous with the p.[Arg117Leu;Leu997Phe] had mild CF, whereas the p.Leu997Phe, in trans with a class I-II CFTR mutation, caused CFTR-RD or a healthy status (CFTR activity: 21.3-36.9%). Finally, compounds heterozygous for the c.[1210-34TG[12];1210-12T[5];2930C>T] and a class I-II mutation had mild CF or CFTR-RD (gating activity: 18.5-19.0%). CONCLUSIONS The effect of complex alleles partially depends on the mutation in trans. Although larger studies are necessary, the CFTR activity on NEC is a rapid contributory tool to classify patients with CFTR dysfunction.},
author = {Terlizzi, Vito and Castaldo, Giuseppe and Salvatore, Donatello and Lucarelli, Marco and Raia, Valeria and Angioni, Adriano and Carnovale, Vincenzo and Cirilli, Natalia and Casciaro, Rosaria and Colombo, Carla and {Di Lullo}, Antonella Miriam and Elce, Ausilia and Iacotucci, Paola and Comegna, Marika and Scorza, Manuela and Lucidi, Vincenzina and Perfetti, Anna and Cimino, Roberta and Quattrucci, Serena and Seia, Manuela and Sofia, Valentina Maria and Zarrilli, Federica and Amato, Felice},
doi = {10.1136/jmedgenet-2016-103985},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Terlizzi et al. - 2017 - Genotype – phenotype correlation and functional studies in patients with cystic fi brosis bearing CFTR comple.pdf:pdf},
issn = {14686244},
journal = {Journal of Medical Genetics},
number = {4},
pages = {224--235},
pmid = {27738188},
title = {{Genotype-phenotype correlation and functional studies in patients with cystic fibrosis bearing CFTR complex alleles}},
volume = {54},
year = {2017}
}
@misc{Babraham2016,
author = {{Babraham Bioinformatics}},
booktitle = {www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules},
title = {{FASTQC manual}},
url = {http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules/},
urldate = {2016-06-25},
year = {2016}
}
@incollection{Kulski2016,
abstract = {Next-generation sequencing (NGS) technologies using DNA, RNA, or methylation sequencing have impacted enormously on the life sciences. NGS is the choice for large-scale genomic and transcriptomic sequencing because of the high-throughput production and outputs of sequencing data in the gigabase range per instrument run and the lower cost compared to the traditional Sanger first-generation sequencing method. The vast amounts of data generated by NGS have broadened our understanding of structural and functional genomics through the concepts of “omics” ranging from basic genomics to integrated systeomics, providing new insight into the workings and meaning of genetic conservation and diversity of living things. NGS today is more than ever about how different organisms use genetic information and molecular biology to survive and reproduce with and without mutations, disease, and diversity within their population networks and changing environments. In this chapter, the advances, applications, and challenges of NGS are reviewed starting with a history of first-generation sequencing followed by the major NGS platforms, the bioinformatics issues confronting NGS data storage and analysis, and the impacts made in the fields of genetics, biology, agriculture, and medicine in the brave, new world of ”omics.”},
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {Kulski, Jerzy K.},
booktitle = {Next Generation Sequencing - Advances, Applications and Challenges},
doi = {10.5772/61964},
eprint = {0803973233},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulski - 2016 - Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications(2).pdf:pdf},
isbn = {ISBN 978-953-51-2240-1},
issn = {978-953-51-2240-1},
month = {jan},
pmid = {100220790},
publisher = {InTech},
title = {{Next-Generation Sequencing — An Overview of the History, Tools, and “Omic” Applications}},
url = {http://www.intechopen.com/books/next-generation-sequencing-advances-applications-and-challenges/next-generation-sequencing-an-overview-of-the-history-tools-and-omic-applications},
year = {2016}
}
@incollection{Kutzera2017b,
author = {Kutzera, Joachim and May, Patrick},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-69751-2_3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutzera, May - 2017 - Variant-DB A Tool for Efficiently Exploring Millions of Human Genetic Variants and Their Annotations(3).pdf:pdf},
isbn = {9783319697505},
issn = {16113349},
month = {nov},
pages = {22--28},
publisher = {Springer, Cham},
title = {{Variant-DB: A tool for efficiently exploring millions of human genetic variants and their annotations}},
url = {http://link.springer.com/10.1007/978-3-319-69751-2_3},
volume = {10649 LNBI},
year = {2017}
}
@article{Pandey2016a,
abstract = {Traditional Sanger sequencing as well as Next-Generation Sequencing have been used for the identification of disease causing mutations in human molecular research. The majority of currently available tools are developed for research and explorative purposes and often do not provide a complete, efficient, one-stop solution. As the focus of currently developed tools is mainly on NGS data analysis, no integrative solution for the analysis of Sanger data is provided and consequently a one-stop solution to analyze reads from both sequencing platforms is not available. We have therefore developed a new pipeline called MutAid to analyze and interpret raw sequencing data produced by Sanger or several NGS sequencing platforms. It performs format conversion, base calling, quality trimming, filtering, read mapping, variant calling, variant annotation and analysis of Sanger and NGS data under a single platform. It is capable of analyzing reads from multiple patients in a single run to create a list of potential disease causing base substitutions as well as insertions and deletions. MutAid has been developed for expert and non-expert users and supports four sequencing platforms including Sanger, Illumina, 454 and Ion Torrent. Furthermore, for NGS data analysis, five read mappers including BWA, TMAP, Bowtie, Bowtie2 and GSNAP and four variant callers including GATK-HaplotypeCaller, SAMTOOLS, Freebayes and VarScan2 pipelines are supported. MutAid is freely available at https://sourceforge.net/projects/mutaid.},
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1371/journal.pone.0147697},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - MutAid Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human m.pdf:pdf},
isbn = {1932-6203 (Electronic) 1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--22},
pmid = {26840129},
title = {{MutAid: Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human molecular genetics}},
volume = {11},
year = {2016}
}
@article{VanderSpoel2015,
abstract = {Kajian perbandingan pengukuran kecekapan pengurusan kewangan institusi Majlis Agama Islam Negeri (MAIN) dengan prestasi agihan kewangan dan bukan kewangan di antara institusi zakat boleh membuka ruang untuk meningkatkan mutu agihan institusi zakat. Pengurusan kewangan sesebuah organisasi adalah penting untuk memastikan institusi tersebut dapat mencapai matlamat yang telah ditentukan. Hal ini kerana masih wujud tanggapan negatif ketidakcekapan agihan institusi zakat dan ia merupakan antara salah satu punca yang telah menjejaskan tahap keyakinan masyarakat Islam untuk menjalankan kewajipan membayar zakat kepada institusi zakat. Ketidakcekapan agihan tersebut boleh dilihat melalui lebihan zakat yang tidak diagihkan dan kegagalan institusi zakat mengagihkan zakat kepada kelapan-lapan golongan asnaf dan tidak mengikut keutamaan asnaf. Persoalannya adakah wujud hubungan antara prestasi pengurusan kewangan dan prestasi pengagihan zakat. Kajian telah mengkategorikan prestasi pengurusan kewangan kepada tiga (3) bahagian iaitu kecairan, kesolvenan dan keuntungan; manakala aspek prestasi kecekapan agihan institusi zakat pula terbahagi kepada dua (2) bahagian iaitu kecekapan agihan berbentuk kewangan (lebihan zakat tahunan) dan bukan kewangan (keutamaan asnaf). Kajian ini juga mengkaji corak agihan zakat kepada setiap asnaf sebagai satu tambahan pengukuran prestasi agihan zakat. Data sekunder terdiri daripada laporan tahunan MAIN telah dianalisis bermula tahun 2000 hingga 2013. Kajian ini mengkaji lima institusi MAIN iaitu Majlis Agama Islam Selangor (MAIS) dan Majlis Agama Islam Negeri Pulau Pinang (MAINPP) yang mewakili institusi yang telah mengkorporatkan kutipan dan agihan zakat; dan Majlis Agama Islam Johor (MAIJ), Majlis Agama Islam dan Adat Melayu Terengganu (MAIDAM) dan Majlis Ugama Islam Sabah (MUIS) yang mewakili negeri yang tidak mengkorporatkan kutipan dan agihan zakat. Hasil kajian mendapati secara umumnya wujud hubungan yang sepadan antara prestasi pengurusan kewangan MAIN dengan prestasi pengagihan zakat dalam aspek agihan kewangan dan bukan kewangan. Hasil kajian juga mendapati corak agihan zakat oleh institusi korporat adalah berbeza dengan corak agihan zakat oleh institusi yang tidak korporat pengurusan zakat. Beberapa cadangan dan implikasi dasar turut dicadangkan dalam kajian ini. ABSTRACT},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wahid, Hairunnizam and Ahmad, Sanep and Nor, Mohd Ali Mohd and Rashid, Maryam Abd},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {01261962},
journal = {Jurnal Ekonomi Malaysia},
keywords = {Financial management efficiency performance,State Islamic Religious Council,Zakat distribution efficiency},
number = {2},
pages = {39--54},
pmid = {25246403},
title = {{Prestasi kecekapan pengurusan kewangan dan agihan zakat: perbandingan antara majlis agama islam negeri di Malaysia}},
volume = {51},
year = {2017}
}
@book{Soames2006,
abstract = {Lasergene's eight modules provide tools that enable users to accomplish each step of sequence analysis, from trimming and assembly of sequence data, to gene discovery, annotation, gene product analysis, sequence similarity searches, sequence alignment, phylogenetic analysis, oligonucleotide primer design, cloning strategies, and publication of the results. The Lasergene software suite provides the functions and customization tools needed so that users can perform analyses the software writers never imagined.},
author = {Burland, Timothy G.},
booktitle = {Bioinformatics Methods and Protocols},
doi = {10.1385/1-59259-192-2:71},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {978-0-89603-732-8},
issn = {1064-3745},
pages = {71--91},
pmid = {10547832},
title = {{DNASTAR's Lasergene Sequence Analysis Software}},
url = {http://link.springer.com/10.1385/1-59259-192-2:71},
volume = {132},
year = {2006}
}
@article{Baes2014a,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\$n$\$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{%} (87.0{%}) and average genotype concordance in correctly identified SNVs was 99.2{%} (99.2{%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\$n$\$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F. and Dolezal, Marlies A. and Koltes, James E. and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J. and Reecy, James M. and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle(2).pdf:pdf},
isbn = {1471-2164},
issn = {14712164},
journal = {BMC Genomics},
keywords = {Next-generation sequencing analysis,Pipeline,Single nucleotide variant identification},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@book{Smith98,
address = {London},
author = {Knuth, Donald E},
edition = {2nd},
number = {44150},
publisher = {The publishing company},
title = {{{The }book}},
year = {1989}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Quinlan2010,
abstract = {Testing for correlations between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.},
author = {Quinlan, Aaron R. and Hall, Ira M.},
doi = {10.1093/bioinformatics/btq033},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan, Hall - 2010 - BEDTools A flexible suite of utilities for comparing genomic features.pdf:pdf},
isbn = {1367-4811 (Electronic)\n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {841--842},
pmid = {20110278},
title = {{BEDTools: A flexible suite of utilities for comparing genomic features}},
volume = {26},
year = {2010}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\n\nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\n\nAVAILABILITY: http://maq.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1303.3997},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
eprint = {1303.3997},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Deng2011,
abstract = {The popularity of massively parallel exome and transcriptome sequencing projects demands new data mining tools with a comprehensive set of features to support a wide range of analysis tasks.},
author = {Deng, Xutao},
doi = {10.1186/1471-2105-12-267},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng - 2011 - SeqGene a comprehensive software solution for mining exome- and transcriptome- sequencing data.pdf:pdf},
isbn = {1471-2105 (Electronic)\r1471-2105 (Linking)},
issn = {14712105},
journal = {BMC Bioinformatics},
month = {jun},
pages = {267},
pmid = {21714929},
publisher = {BioMed Central},
title = {{SeqGene: A comprehensive software solution for mining exome- and transcriptome- sequencing data}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21714929 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3148209},
volume = {12},
year = {2011}
}
@article{Pabinger2014,
abstract = {Recent advances in genome sequencing technologies provide unprecedented opportunities to characterize individual genomic landscapes and identify mutations relevant for diagnosis and therapy. Specifically, whole-exome sequencing using next-generation sequencing (NGS) technologies is gaining popularity in the human genetics community due to the moderate costs, manageable data amounts and straightforward interpretation of analysis results. While whole-exome and, in the near future, whole-genome sequencing are becoming commodities, data analysis still poses significant challenges and led to the development of a plethora of tools supporting specific parts of the analysis workflow or providing a complete solution. Here, we surveyed 205 tools for whole-genome/whole-exome sequencing data analysis supporting five distinct analytical steps: quality assessment, alignment, variant identification, variant annotation and visualization. We report an overview of the functionality, features and specific requirements of the individual tools. We then selected 32 programs for variant identification, variant annotation and visualization, which were subjected to hands-on evaluation using four data sets: one set of exome data from two patients with a rare disease for testing identification of germline mutations, two cancer data sets for testing variant callers for somatic mutations, copy number variations and structural variations, and one semi-synthetic data set for testing identification of copy number variations. Our comprehensive survey and evaluation of NGS tools provides a valuable guideline for human geneticists working on Mendelian disorders, complex diseases and cancers.},
archivePrefix = {arXiv},
arxivId = {209},
author = {Pabinger, Stephan and Dander, Andreas and Fischer, Maria and Snajder, Rene and Sperk, Michael and Efremova, Mirjana and Krabichler, Birgit and Speicher, Michael R. and Zschocke, Johannes and Trajanoski, Zlatko},
doi = {10.1093/bib/bbs086},
eprint = {209},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pabinger et al. - 2014 - A survey of tools for variant analysis of next-generation genome sequencing data.pdf:pdf},
isbn = {4351290037310},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics tools,Cancer,Mendelian disorders,Next-generation sequencing,Variants},
number = {2},
pages = {256--278},
pmid = {23341494},
title = {{A survey of tools for variant analysis of next-generation genome sequencing data}},
volume = {15},
year = {2014}
}
@article{La2012,
abstract = {Ejercicio de citas en Mendeley},
author = {Notario, Silvia},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
journal = {Seminario de tesis 1},
pages = {1--2},
title = {{Ejercicio de citas con Mendeley}},
year = {2016}
}
@article{Cock2009,
abstract = {FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.},
author = {Cock, Peter J A and Fields, Christopher J. and Goto, Naohisa and Heuer, Michael L. and Rice, Peter M.},
doi = {10.1093/nar/gkp1137},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cock et al. - 2009 - The Sanger FASTQ file format for sequences with quality scores, and the SolexaIllumina FASTQ variants.pdf:pdf},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {6},
pages = {1767--1771},
pmid = {20015970},
title = {{The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants}},
volume = {38},
year = {2009}
}
@article{Brueffer2015,
abstract = {Summary: TopHat is a popular spliced junction mapper for RNA sequencing data, and writes files in the BAM format - the binary version of the Sequence Alignment/Map (SAM) format. BAM is the standard exchange format for aligned sequencing reads, thus correct format implementation is paramount for software interoperability and correct analysis. However, TopHat writes its unmapped reads in a way that is not compatible with other software that implements the SAM/BAM format. We have developed TopHat-Recondition, a post-processor for TopHat unmapped reads that restores read information in the proper format. TopHat-Recondition thus enables downstream software to process the plethora of BAM files written by TopHat. Availability and implementation: TopHat-Recondition is implemented in Python using the Pysam library and is freely available under a 2-clause BSD license on GitHub: https://github.com/cbrueffer/tophat-recondition. Contact: christian.brueffer@med.lu.se, lao.saal@med.lu.se},
author = {Brueffer, Christian and Saal, Lao H.},
doi = {10.1186/s12859-016-1058-x},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brueffer, Saal - 2015 - TopHat-Recondition A post-processor for TopHat unmapped reads.pdf:pdf},
isbn = {1471-2105 (Electronic)
1471-2105 (Linking)},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Deep sequencing,RNA-seq,Sequence alignment,Sequence analysis},
number = {1},
pages = {1--6},
pmid = {27142976},
publisher = {BMC Bioinformatics},
title = {{TopHat-Recondition: A post-processor for TopHat unmapped reads}},
url = {http://dx.doi.org/10.1186/s12859-016-1058-x},
volume = {17},
year = {2016}
}
@misc{Louie2007,
abstract = {Genomic medicine aims to revolutionize health care by applying our growing understanding of the molecular basis of disease. Research in this arena is data intensive, which means data sets are large and highly heterogeneous. To create knowledge from data, researchers must integrate these large and diverse data sets. This presents daunting informatic challenges such as representation of data that is suitable for computational inference (knowledge representation), and linking heterogeneous data sets (data integration). Fortunately, many of these challenges can be classified as data integration problems, and technologies exist in the area of data integration that may be applied to these challenges. In this paper, we discuss the opportunities of genomic medicine as well as identify the informatics challenges in this domain. We also review concepts and methodologies in the field of data integration. These data integration concepts and methodologies are then aligned with informatics challenges in genomic medicine and presented as potential solutions. We conclude this paper with challenges still not addressed in genomic medicine and gaps that remain in data integration research to facilitate genomic medicine. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Louie, Brenton and Mork, Peter and Martin-Sanchez, Fernando and Halevy, Alon and Tarczy-Hornoch, Peter},
booktitle = {Journal of Biomedical Informatics},
doi = {10.1016/j.jbi.2006.02.007},
file = {:home/jennifer/Descargas/1-s2.0-S1532046406000244-main.pdf:pdf},
isbn = {1532-0480 (Electronic)\n1532-0464 (Linking)},
issn = {15320464},
keywords = {Biomedical informatics,Data integration,Genomic medicine,Genomics,Knowledge representation},
month = {feb},
number = {1},
pages = {5--16},
pmid = {16574494},
publisher = {Academic Press},
title = {{Data integration and genomic medicine}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046406000244},
volume = {40},
year = {2007}
}
@article{scikit-learn,
archivePrefix = {arXiv},
arxivId = {arXiv:1201.0490v2},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1201.0490v2},
isbn = {9781783281930},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells, Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,Sequence Analysis, RNA,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192&tool=pmcentrez&rendertype=abstract},
volume = {9},
year = {2014}
}
@article{Wang2014,
abstract = {Motivation: The transition/transversion (Ti/Tv) ratio and heterozygous/nonreference-homozygous (het/nonref-hom) ratio have been commonly computed in genetic studies as a quality control (QC) measurement. Additionally, these two ratios are helpful in our understanding of the patterns of DNA sequence evolution.Results: To thoroughly understand these two genomic measures, we performed a study using 1000 Genomes Project (1000G) released genotype data (N = 1092). An additional two datasets (N = 581 and N = 6) were used to validate our findings from the 1000G dataset. We compared the two ratios among continental ancestry, genome regions and gene functionality. We found that the Ti/Tv ratio can be used as a quality indicator for single nucleotide polymorphisms inferred from high-throughput sequencing data. The Ti/Tv ratio varies greatly by genome region and functionality, but not by ancestry. The het/nonref-hom ratio varies greatly by ancestry, but not by genome regions and functionality. Furthermore, extreme guanine + cytosine content (either high or low) is negatively associated with the Ti/Tv ratio magnitude. Thus, when performing QC assessment using these two measures, care must be taken to apply the correct thresholds based on ancestry and genome region. Failure to take these considerations into account at the QC stage will bias any following analysis.Contact: yan.guo@vanderbilt.eduSupplementary information: Supplementary data are available at Bioinformatics online. },
author = {Wang, Jing and Raskin, Leon and Samuels, David C. and Shyr, Yu and Guo, Yan},
doi = {10.1093/bioinformatics/btu668},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Genome measures used for quality control are dependent on gene function and ancestry.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {3},
pages = {318--323},
pmid = {25297068},
title = {{Genome measures used for quality control are dependent on gene function and ancestry}},
volume = {31},
year = {2015}
}
@article{Hashem2015,
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized. {\textcopyright} 2014 Elsevier Ltd.},
author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
doi = {10.1016/j.is.2014.07.006},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Big data,Cloud computing,Hadoop},
month = {jan},
pages = {98--115},
pmid = {1476196123},
publisher = {Pergamon},
title = {{The rise of "big data" on cloud computing: Review and open research issues}},
url = {http://www.sciencedirect.com/science/article/pii/S0306437914001288},
volume = {47},
year = {2015}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an 'appropriate' number of clusters. {\textcopyright} 1987.},
archivePrefix = {arXiv},
arxivId = {z0024},
author = {Rousseeuw, Peter J.},
doi = {10.1016/0377-0427(87)90125-7},
eprint = {z0024},
file = {:home/jennifer/Descargas/silhouettes.pdf:pdf},
isbn = {03770427},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Graphical display,classification,cluster analysis,clustering validity},
number = {C},
pages = {53--65},
pmid = {19382406},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
volume = {20},
year = {1987}
}
@article{Sujansky2001,
abstract = {The rapid expansion of biomedical knowledge, reduction in computing costs, and spread of internet access have created an ocean of electronic data. The decentralized nature of our scientific community and healthcare system, however, has resulted in a patchwork of diverse, or heterogeneous, database implementations, making access to and aggregation of data across databases very difficult. The database heterogeneity problem applies equally to clinical data describing individual patients and biological data characterizing our genome. Specifically, databases are highly heterogeneous with respect to the data models they employ, the data schemas they specify, the query languages they support, and the terminologies they recognize. Heterogeneous database systems attempt to unify disparate databases by providing uniform conceptual schemas that resolve representational heterogeneities, and by providing querying capabilities that aggregate and integrate distributed data. Research in this area has applied a variety of database and knowledge-based techniques, including semantic data modeling, ontology definition, query translation, query optimization, and terminology mapping. Existing systems have addressed heterogeneous database integration in the realms of molecular biology, hospital information systems, and application portability. {\textcopyright} 2001 Elsevier Science (USA).},
author = {Sujansky, Walter},
doi = {10.1006/jbin.2001.1024},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sujansky - 2001 - Heterogeneous database integration in biomedicine.pdf:pdf},
isbn = {1532-0464},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Data warehouse,Database,Database integration,Federated database,Heterogeneous database},
number = {4},
pages = {285--298},
pmid = {11977810},
title = {{Heterogeneous database integration in biomedicine}},
volume = {34},
year = {2001}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma.png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (2)}}
}
@article{Harold2016,
abstract = {Motivation: Integrating heterogeneous data sets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access, data filtering, format conversions, identifier mapping, among further diverse operations. Data integration is especially important when annotating next generation sequencing (NGS) data, where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic locations, such a single nucleotide variants (SNV) or genes. Each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose. However, software that always produces all available data is difficult to maintain and quickly leads to an excess of data, creating an information overload rather than the desired goal-oriented and integrated result. Results: We present SoFIA, a framework for workflow-driven data integration with a focus on genomic annotation. SoFIA conceptualises workflow templates as comprehensive workflows that cover as many data integration operations as possible in a given domain. However, these templates are not intended to be executed as a whole; instead, when given an integration task consisting of a set of input data and a set of desired output data, SoFIA derives a minimal workflow that completes the task. These workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work. Using a comprehensive genome annotation template, we highlight the flexibility, extensibility and power of the framework using real-life case studies. Availability: https://github.com/childsish/sofia/releases/latest under the GNU General Public License Contact: liam.childs@hu-berlin.de Supplementary Information: Supplementary data are available at Bioinformatics online.},
author = {Childs, Liam Harold and Mamlouk, Soulafa and Brandt, J{\"{o}}rgen and Sers, Christine and Leser, Ulf},
doi = {10.1093/bioinformatics/btw302},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harold et al. - 2016 - SoFIA a data integration framework for annotating high-throughput datasets.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {17},
pages = {2590--2597},
pmid = {27187206},
title = {{SoFIA: A data integration framework for annotating high-throughput datasets}},
volume = {32},
year = {2016}
}
@article{Amberger2017,
abstract = {Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support human genetics research and education and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (http://www.ncbi.nlm.nih.gov/omim/) is now distributed electronically by the National Center for Biotechnology Information, where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, HUGO nomenclature, MapViewer, GeneTests, patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics.},
author = {Amberger, Joanna S. and Hamosh, Ada},
doi = {10.1002/cpbi.27},
file = {:home/jennifer/Descargas/amberger2017.pdf:pdf},
isbn = {1362-4962 (Electronic)\r0305-1048 (Linking)},
issn = {1934340X},
journal = {Current Protocols in Bioinformatics},
keywords = {Disease gene discovery,Human genetic disorders,Molecular genetics,OMIM},
number = {June},
pages = {1.2.1--1.2.12},
pmid = {15608251},
title = {{Searching online mendelian inheritance in man (OMIM): A knowledgebase of human genes and genetic phenotypes}},
volume = {2017},
year = {2017}
}
@article{Parma2017,
abstract = {Retinoblastoma (RB) is an inherited childhood ocular cancer caused by mutations in the tumor suppressor RB1 gene. Identification of RB1 mutations is essential to assess the risk of developing retinoblastoma in the patients´ relatives. Retinoblastoma is a potentially curable cancer and an early diagnosis is critical for survival and eye preservation. Unilateral retinoblastoma is mostly non-heritable and results from two somatic mutations whereas bilateral retinoblastoma is heritable and results from one germline and one somatic mutation, both have high penetrance, 90%. The purpose of this study was to identify causative RB1 mutations in RB patients with different clinical presentations. A comprehensive approach was used to study a cohort of 34 patients with unilateral, bilateral and trilateral retinoblastoma. Blood and tumor DNA was analyzed by sequencing and multiplex ligation-dependent probe amplification (MLPA) assay. Validation of an insertion mutation was performed by cloning the PCR product. Most of the patients in our cohort had unilateral RB, eight patients had bilateral RB and one patient had a trilateral tumor with ocular and suprasellar/sellar locations. Other tumors in addition to retinoblastoma were also found in the affected families. One patient had two syndromes, retinoblastoma and schwannomatosis, and another RB patient had a father with a retinoma. Five out of the 25 unilateral RB patients carried germinal mutations (20%), which were mostly missense mutations. The bilateral and trilateral patients carried splice-site, nonsense and frameshift mutations as well as a whole RB1 gene deletion. Missense mutations were associated with mild phenotype: unilateral retinoblastoma, retinoma or no tumor. In this study we identified causative RB1 mutations in most bilateral RB patients and in some unilateral RB patients, including five novel mutations. These data are crucial for genetic counseling and confirm the need to perform complete genetic screening for RB1 mutations in both constitutional and tumor tissues.},
author = {Parma, Diana and Ferrer, Marcela and Luce, Leonela and Giliberto, Florencia and Szijan, Irene},
doi = {10.1371/journal.pone.0189736},
file = {:home/jennifer/Descargas/pone.0189736.pdf:pdf},
isbn = {2002009020049},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pages = {1--12},
pmid = {29261756},
title = {{RB1 gene mutations in Argentine retinoblastoma patients. Implications for genetic counseling}},
volume = {12},
year = {2017}
}
@article{Rao2018,
author = {Rao, A. Ravishankar and Garai, Subrata and Clarke, Daniel and Dey, Soumyabrata},
doi = {10.1109/IJCNN.2018.8489448},
file = {:home/jennifer/Descargas/08489448.pdf:pdf},
isbn = {9781509060146},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {1--8},
publisher = {IEEE},
title = {{A system for exploring big data: An iterative k-means searchlight for outlier detection on open health data}},
volume = {2018-July},
year = {2018}
}
@inproceedings{Karlsson2018,
author = {Karlsson, Rebecca Hellstr{\"{o}}m and Shreenath, Vinutha Magal and Meijer, Sebastiaan},
booktitle = {Artificial Intelligence Research Society Conference (FLAIRS-31) Aiding},
file = {:home/jennifer/Descargas/17657-77678-1-PB.pdf:pdf},
keywords = {Artificial Intelligence in Healthcare Informatics},
pages = {311--316},
title = {{Aiding Remote Diagnosis with Text Mining}},
year = {2018}
}
@article{Koike1995,
author = {Koike, Teruaki and Terashima, Masanori and Takizawa, Tsuneyo and Akamatsu, Hideki},
doi = {10.2482/haigan.35.311},
file = {:home/jennifer/Descargas/paper_9.pdf:pdf},
issn = {03869628},
journal = {Data Integration in the Life Sciences. DILS 2018. Lecture Notes in Computer Science.},
keywords = {Informed consent,Lung cancer,Truth telling regarding cancer},
number = {3},
pages = {311--316},
title = {{Lung Cancer Concept Annotation from Spanish Clinical Narratives}},
volume = {11371},
year = {2019}
}
@article{Malik2018,
abstract = {With the widespread use of healthcare information systems commonly known as electronic health records, there is significant scope for improving the way healthcare is delivered by resorting to the power of big data. This has made data mining and predictive analytics an important tool for healthcare decision making. The literature has reported attempts for knowledge discovery from the big data to improve the delivery of healthcare services, however, there appears no attempt for assessing and synthesizing the available information on how the big data phenomenon has contributed to better outcomes for the delivery of healthcare services. This paper aims to achieve this by systematically reviewing the existing body of knowledge to categorize and evaluate the reported studies on healthcare operations and data mining frameworks. The outcome of this study is useful as a reference for the practitioners and as a research platform for the academia.},
author = {Malik, M. M. and Abdallah, S. and Ala'raj, M.},
doi = {10.1007/s10479-016-2393-z},
file = {:home/jennifer/Descargas/malik2016.pdf:pdf},
isbn = {02545330 (ISSN)},
issn = {15729338},
journal = {Annals of Operations Research},
keywords = {Big data,Data mining,Healthcare operations management,Predictive analytics,Systematic literature review},
number = {1-2},
pages = {287--312},
publisher = {Springer US},
title = {{Data mining and predictive analytics applications for the delivery of healthcare services: a systematic literature review}},
volume = {270},
year = {2018}
}
@article{Neveol2014,
abstract = {Abstract Background: Natural language processing applied to clinical text or aimed at a clinical outcome has been thriving in recent years. This paper offers the first broad overview of clinical Natural Language Processing (NLP) for languages other than English. Recent studies are summarized to offer insights and outline opportunities in this area. Main Body: We envision three groups of intended readers: (1) NLP researchers leveraging experience gained in other languages, (2) NLP researchers faced with establishing clinical text processing in a language other than English, and (3) clinical informatics researchers and practitioners looking for resources in their languages in order to apply NLP techniques and tools to clinical practice and/or investigation. We review work in clinical NLP in languages other than English. We classify these studies into three groups: (i) studies describing the development of new NLP systems or components de novo, (ii) studies describing the adaptation of NLP architectures developed for English to another language, and (iii) studies focusing on a particular clinical application. Conclusion: We show the advantages and drawbacks of each method, and highlight the appropriate application context. Finally, we identify major challenges and opportunities that will affect the impact of NLP on clinical practice and},
author = {N{\'{e}}v{\'{e}}ol, A and Dalianis, H K and Savova, G and Zweigenbaum, P},
file = {:home/jennifer/Descargas/Clinical_Natural_Language_Processing_in_languages_.pdf:pdf},
journal = {Journal of Biomedical Semantics},
keywords = {clinical decision-making,languages other than english,natural language processing},
number = {12},
pages = {1--13},
title = {{Clinical Natural Language Processing in languages other than English: opportunities and challenges}},
volume = {9},
year = {2018}
}
@article{Huttenhower2010,
abstract = {The aim of this work is to be able to publish the information concerning communication with cancer patients as recommended in England. The observation and the study protocol during the stay abroad have been given the opportunity to stylize specific information on the methodology of communication of important information to terminally ill patients. It seems readily apparent as they characterized by both technical precision and sensivity to emotions and descriptions for the individual patient. How is shared by all chronic pain is predominantly complex emotion, a mix of additions and perceived physical and emotional pain - emotional. Because accurate information is beneficial to the patient and that really is not turned, so to speak, a "bullet" it is necessary that you have created, over time, a concrete "therapeutic alliance" between body physician, patient and possibly family. This arises, for sure, even at first accepted the patient during the clinical visit attentive to detail, is renewed in the definition of the common objective to be achieved, so analgesia and it is expressed in the certainty that the physician provides all the resources realistically available. It is then up to the sensitivity of the operator, doctor and/or nurse, described in the "take charge" find, from time to time, the words and manners, verbal and nonverbal, to respond fully to questions of the patient same.},
author = {Huttenhower, Curtis and Hofmann, Oliver},
doi = {10.1371/journal.pcbi.1000779},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huttenhower, Hofmann - 2010 - A quick guide to large-scale genomic data mining.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {5},
pages = {1--6},
pmid = {20523745},
title = {{A quick guide to large-scale genomic data mining}},
volume = {6},
year = {2010}
}
@article{Jurca2016,
abstract = {BACKGROUND Breast cancer is a serious disease which affects many women and may lead to death. It has received considerable attention from the research community. Thus, biomedical researchers aim to find genetic biomarkers indicative of the disease. Novel biomarkers can be elucidated from the existing literature. However, the vast amount of scientific publications on breast cancer make this a daunting task. This paper presents a framework which investigates existing literature data for informative discoveries. It integrates text mining and social network analysis in order to identify new potential biomarkers for breast cancer. RESULTS We utilized PubMed for the testing. We investigated gene-gene interactions, as well as novel interactions such as gene-year, gene-country, and abstract-country to find out how the discoveries varied over time and how overlapping/diverse are the discoveries and the interest of various research groups in different countries. CONCLUSIONS Interesting trends have been identified and discussed, e.g., different genes are highlighted in relationship to different countries though the various genes were found to share functionality. Some text analysis based results have been validated against results from other tools that predict gene-gene relations and gene functions.},
author = {Jurca, Gabriela and Addam, Omar and Aksac, Alper and Gao, Shang and {\"{O}}zyer, Tansel and Demetrick, Douglas and Alhajj, Reda},
doi = {10.1186/s13104-016-2023-5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurca et al. - 2016 - Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends(2).pdf:pdf},
isbn = {1756-0500},
issn = {17560500},
journal = {BMC Research Notes},
keywords = {Breast cancer,Data mining,Network analysis,Text mining},
number = {1},
pages = {236},
pmid = {27112211},
publisher = {BioMed Central},
title = {{Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends}},
url = {http://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-016-2023-5},
volume = {9},
year = {2016}
}
@article{Senthilkumar2018,
abstract = {A systematic literature review of papers on big data in healthcare published between 2010 and 2015 was conducted. This paper reviews the definition, process, and use of big data in healthcare management. Unstructured data are growing very faster than semi-structured and structured data.},
author = {Senthilkumar, SA and Rai, Bharatendara K and Meshram, Amruta A and Gunasekaran, Angappa},
doi = {10.11648/j.ajtab.20180402.14},
file = {:home/jennifer/Descargas/10.11648.j.ajtab.20180402.14.pdf:pdf},
issn = {2469-7842},
journal = {American Journal of Theoretical and Applied Business},
keywords = {Big Data,Data Acquisition,Data Analytics,Data Storage,Data Visualization,Healthcare Management},
number = {2},
pages = {57--69},
title = {{Big Data in Healthcare Management: A Review of Literature}},
url = {http://www.sciencepublishinggroup.com/j/ajtab},
volume = {4},
year = {2018}
}
@inproceedings{Tang2018,
abstract = {Objectives To assess the feasibility of combining a similarity hash function and a clustering algorithm to identify similarity among free-text clinical documents. Materials and Methods Our corpus contains 1,063,893 free-text clinical documents corresponding to 32,682 unique patients and 19,146 unique providers between January 2011 and July 2016. Under the K-means paradigm, we propose a clustering algorithm that first identifies centroids by creating a one-level partitioning of all fingerprints produced by Charikar's SimHash, and then updates the centroids incrementally by assigning each point to its closest centroid and re- computing the centroid of each cluster. We designed two experiments: 1) to tune two parameters: the number of clusters to use and the centroid threshold; and 2) to evaluate the feasibility of utilizing our algorithm at the individual author level. We validated our algorithm using human annotators' judgments. We calculated Fleiss' Kappa to measure inter-annotator reliability. Results Our algorithm achieved 80.0% and 89.5% precision in experiments 1 and 2, respectively. Annotator agreement in both experiments was moderate. From the first experiment, we identified an appropriate centroid threshold to be less than or equal to 1 for this task. From the second experiment, we identified 30 clusters from nine clinicians. Discussion Our algorithm is able to efficiently identify a particular clinical document that can represent the other documents in the same cluster at both an organization and the individual clinician level. Conclusion},
author = {Tang, Chunlei and Plasek, Joseph M and Xiong, Yun and Bates, David W and Zhou, Li},
booktitle = {International Conference on Data Science},
doi = {DOI: 10.13140/RG.2.2.16490.41920},
file = {:home/jennifer/Descargas/simnotes_ICDS-2018.pdf:pdf},
title = {{Clustering Similar Clinical Documents in Electronic Health Records}},
year = {2018}
}
@article{Sun2017,
abstract = {As the prevalence of social media on the Internet, opinion mining has become an essential approach to analyzing so many data. Various applications appear in a wide range of industrial domains. Meanwhile, opinions have diverse expressions which bring along research challenges. Both of the practical demands and research challenges make opinion mining an active research area in recent years. In this paper, we present a review of Natural Language Processing (NLP) techniques for opinion mining. First, we introduce general NLP techniques which are required for text preprocessing. Second, we investigate the approaches of opinion mining for different levels and situations. Then we introduce comparative opinion mining and deep learning approaches for opinion mining. Opinion summarization and advanced topics are introduced later. Finally, we discuss some challenges and open problems related to opinion mining.},
author = {Sun, Shiliang and Luo, Chen and Chen, Junyu},
doi = {10.1016/j.inffus.2016.10.004},
file = {:home/jennifer/Descargas/sun2017.pdf:pdf},
isbn = {8621543451},
issn = {15662535},
journal = {Information Fusion},
keywords = {Deep learning,Machine learning,Natural language processing,Opinion mining,Sentiment analysis},
pages = {10--25},
publisher = {Elsevier B.V.},
title = {{A review of natural language processing techniques for opinion mining systems}},
url = {http://dx.doi.org/10.1016/j.inffus.2016.10.004},
volume = {36},
year = {2017}
}
@misc{He,
abstract = {Genomic medicine attempts to build individualized strategies for diagnostic or therapeutic decision-making by utilizing patients' genomic information. Big Data analytics uncovers hidden patterns, unknown correlations, and other insights through examining large-scale various data sets. While integration and manipulation of diverse genomic data and comprehensive electronic health records (EHRs) on a Big Data infrastructure exhibit challenges, they also provide a feasible opportunity to develop an efficient and effective approach to identify clinically actionable genetic variants for individualized diagnosis and therapy. In this paper, we review the challenges of manipulating large-scale next-generation sequencing (NGS) data and diverse clinical data derived from the EHRs for genomic medicine. We introduce possible solutions for different challenges in manipulating, managing, and analyzing genomic and clinical data to implement genomic medicine. Additionally, we also present a practical Big Data toolset for identifying clinically actionable genetic variants using high-throughput NGS data and EHRs.},
author = {He, Karen Y. and Ge, Dongliang and He, Max M.},
booktitle = {International Journal of Molecular Sciences},
doi = {10.3390/ijms18020412},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Ge, He - Unknown - Big Data Analytics for Genomic Medicine.pdf:pdf},
isbn = {14220067 (Electronic)},
issn = {14220067},
keywords = {Big data analytics,Clinically actionable genetic variants,Electronic health records,Healthcare,Next-generation sequencing},
number = {2},
pmid = {28212287},
title = {{Big data analytics for genomic medicine}},
volume = {18},
year = {2017}
}
@article{Wu2017,
abstract = {Objective: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. Methods: In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. Results: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. Conclusion: Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. Significance: Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
doi = {10.1109/TBME.2016.2573285},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2017 - –Omic and Electronic Health Record Big Data Analytics for Precision Medicine.pdf:pdf},
isbn = {0018-9294 VO  - 64},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
number = {2},
pages = {263--273},
pmid = {28113246},
title = {{–Omic and Electronic Health Record Big Data Analytics for Precision Medicine}},
url = {http://ieeexplore.ieee.org/document/7587347/},
volume = {64},
year = {2017}
}
@article{Goldstein2017,
abstract = {OBJECTIVE Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. METHODS We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. RESULTS We identified 107 articles from 15 different countries. Studies were generally very large (median sample size = 26 100) and utilized a diverse array of predictors. Most used validation techniques (n = 94 of 107) and reported model coefficients for reproducibility (n = 83). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n = 37) and employed relatively few predictor variables (median = 27 variables). Less than half of the studies were multicenter (n = 50) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow-up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). CONCLUSIONS EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.2609v5},
author = {Goldstein, Benjamin A. and Navar, Ann Marie and Pencina, Michael J. and Ioannidis, John P.A.},
doi = {10.1093/jamia/ocw042},
eprint = {arXiv:1301.2609v5},
file = {:home/jennifer/Descargas/ocw042.pdf:pdf},
isbn = {1067-5027},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {Electronic medical record,Review,Risk assessment},
number = {1},
pages = {198--208},
pmid = {27189013},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: A systematic review}},
volume = {24},
year = {2017}
}
@article{Perumal2018,
author = {Perumal, Govindasamy and Lakshmanaprabu, S K},
file = {:home/jennifer/Descargas/JARDCS_GVN.pdf:pdf},
number = {April},
title = {{Document Clustering Based On Text Mining K-Means Algorithm Using Euclidean Distance Similarity}},
year = {2018}
}
@inproceedings{Najafabadipour2004,
abstract = {Recent rapid increase in the generation of clinical data and rapid development of computational science make us able to extract new insights from massive datasets in healthcare industry. Oncological Electronic Health Records (EHRs) are creating rich databases for documenting patient's history and they potentially contain a lot of patterns that can help in better management of the disease. However, these patterns are locked within free text (unstructured) portions of EHRs and consequence in limiting health professionals to extract useful information from them and to finally perform Query and Answering (Q&A) process in an accurate way. The Information Extraction (IE) process requires Natural Language Processing (NLP) techniques to assign semantics to these patterns. Therefore, in this paper, we analyze the design of annotators for specific lung cancer concepts that can be integrated over Apache Unstructured Information Management Architecture (UIMA) framework. In addition, we explain the details of generation and storage of annotation outcomes.},
archivePrefix = {arXiv},
arxivId = {1010.2656},
author = {Najafabadipour, Marjan and Manuel, Juan and Rodr, Alejandro and Menasalvas, Ernestina},
booktitle = {Data Integration in the Life Sciences},
doi = {10.1007/b96666},
eprint = {1010.2656},
file = {:home/jennifer/Descargas/najafabadipour2018.pdf:pdf},
isbn = {978-3-540-21300-0},
issn = {0302-9743},
keywords = {electronic health record {\'{a}},lung cancer,named entity recognition {\'{a}},natural language processing},
number = {1},
pages = {153--163},
pmid = {17015151},
title = {{Lung Cancer Concept Annotation from Spanish Clinical Narratives Marjan}},
url = {http://link.springer.com/10.1007/b96666},
volume = {11371},
year = {2018}
}
@article{Gruca2017,
author = {Micek, Marta and Pacholczyk, Marcin},
doi = {10.1007/978-3-319-67792-7},
file = {:home/jennifer/Descargas/micek2018.pdf:pdf},
isbn = {9783319677910},
journal = {Springer International Publishing},
keywords = {apriori,basket analysis,data mining},
title = {{Searching for Cancer Signatures Using Data Mining Techniques}},
year = {2018}
}
@article{Peng2018,
abstract = {Objective: Data quality assessment is a challenging facet for research using coded administrative health data. Current assessment approaches are time and resource intensive. We explored whether association rule mining (ARM) can be used to develop rules for assessing data quality. Materials and methods: We extracted 2013 and 2014 records from the hospital discharge abstract database (DAD) for patients between the ages of 55 and 65 from five acute care hospitals in Alberta, Canada. The ARM was conducted using the 2013 DAD to extract rules with support ≥0.0019 and confidence ≥0.5 using the bootstrap technique, and tested in the 2014 DAD. The rules were compared against the method of coding frequency and assessed for their ability to detect error introduced by two kinds of data manipulation: random permutation and random deletion. Results: The association rules generally had clear clinical meanings. Comparing 2014 data to 2013 data (both original), there were 3 rules with a confidence difference >0.1, while coding frequency difference of codes in the right hand of rules was less than 0.004. After random permutation of 50% of codes in the 2014 data, average rule confidence dropped from 0.72 to 0.27 while coding frequency remained unchanged. Rule confidence decreased with the increase of coding deletion, as expected. Rule confidence was more sensitive to code deletion compared to coding frequency, with slope of change ranging from 1.7 to 184.9 with a median of 9.1. Conclusion: The ARM is a promising technique to assess data quality. It offers a systematic way to derive coding association rules hidden in data, and potentially provides a sensitive and efficient method of assessing data quality compared to standard methods.},
author = {Peng, Mingkai and Sundararajan, Vijaya and Williamson, Tyler and Minty, Evan P. and Smith, Tony C. and Doktorchik, Chelsea T.A. and Quan, Hude},
doi = {10.1016/j.jbi.2018.02.001},
file = {:home/jennifer/Descargas/10.1016@j.jbi.2018.02.001.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Association rule mining,Coding completeness,Coding inconsistency,Diagnosis code,Inpatient administrative health data,International classification of disease},
pages = {41--47},
pmid = {29425732},
title = {{Exploration of association rule mining for coding consistency and completeness assessment in inpatient administrative health data}},
url = {https://doi.org/10.1016/j.jbi.2018.02.001},
volume = {79},
year = {2018}
}
@article{Karabatak2009,
abstract = {This paper presents an automatic diagnosis system for detecting breast cancer based on association rules (AR) and neural network (NN). In this study, AR is used for reducing the dimension of breast cancer database and NN is used for intelligent classification. The proposed AR + NN system performance is compared with NN model. The dimension of input feature space is reduced from nine to four by using AR. In test stage, 3-fold cross validation method was applied to the Wisconsin breast cancer database to evaluate the proposed system performances. The correct classification rate of proposed system is 95.6%. This research demonstrated that the AR can be used for reducing the dimension of feature space and proposed AR + NN model can be used to obtain fast automatic diagnostic systems for other diseases. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Karabatak, Murat and Ince, M. Cevdet},
doi = {10.1016/j.eswa.2008.02.064},
file = {:home/jennifer/Descargas/karabatak2009.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Association rules,Automatic detection,Breast cancer,Neural network},
number = {2 PART 2},
pages = {3465--3469},
publisher = {Elsevier Ltd},
title = {{An expert system for detection of breast cancer based on association rules and neural network}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.02.064},
volume = {36},
year = {2009}
}
@article{Agrawal1994,
abstract = {This paper uses Data Envelopment Analysis (DEA), a non-parametric approach to the estimation of production functions, in order to assess efficiency in dot com firms. These firms have two objectives: to make an impact in the Internet and to obtain revenues from their activities. For this reason, the outputs have been two: unique visitors-a web metric-and revenues. DEA efficiencies have been obtained under various input/output combinations. A ranking of dot com firms in terms of relative efficiency has been obtained. A method based on multivariate analysis has been proven to be successful at showing the strengths and weaknesses of individual dot com firms. It is shown that there is a relationship between the type of e-business (e-tailers, search/portal, content/communities), and the way in which efficiency is obtained. The paper suggests a new approach to the problem of deciding which inputs and outputs the model should contain.{\textcopyright} 2003 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1011.1669v3},
author = {Serrano-Cinca, Carlos and Fuertes-Call{\'{e}}n, Yolanda and Mar-Molinero, Cecilio},
doi = {10.1016/j.dss.2003.08.004},
eprint = {1011.1669v3},
file = {:home/jennifer/Descargas/12.pdf:pdf},
isbn = {01679236 (ISSN)},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Data Envelopment Analysis,Dot com,Efficiency; E-business,Inputs/outputs selection,Principal components analysis,Web metrics},
number = {4},
pages = {557--573},
pmid = {27235000},
title = {{Measuring DEA efficiency in Internet companies}},
volume = {38},
year = {2005}
}
@article{breuler2017,
abstract = {Disentangling the etiology of common, complex diseases is a major challenge in genetic research. For bipolar disorder (BD), several genome-wide association studies (GWAS) have been performed. Similar to other complex disorders, major breakthroughs in explaining the high heritability of BD through GWAS have remained elusive. To overcome this dilemma, genetic research into BD, has embraced a variety of strategies such as the formation of large consortia to increase sample size and sequencing approaches. Here we advocate a complementary approach making use of already existing GWAS data: applying a data mining procedure to identify yet undetected genotype-phenotype relationships. We adapted association rule mining, a data mining technique traditionally used in retail market research,to identify frequent and characteristic genotype patterns showing strong associations to phenotype clusters. We applied this strategy to three independent GWAS datasets from 2,835 phenotypically characterized patients with BD. In a discovery step, 20,882 candidate association rules were extracted. Two of these - one associated with eating disorder and the other with anxiety - remained significant in an independent dataset after robust correction for multiple testing, showing considerable effect sizes (odds ratio $\sim$ 3.4 and 3.0, respectively). Our approach may help detect novel specific genotype-phenotype relationships in BD typically not explored by analyses like GWAS. While we adapted the data mining tool within the context of BD gene discovery, it may facilitate identifying highly specific genotype-phenotype relationships in subsets of genome-wide data sets of other complex phenotype with similar epidemiological properties and challenges to gene discovery efforts.},
author = {Breuer, Ren{\'{e}} and Mattheisen, Manuel and Frank, Josef and Krumm, Bertram and Treutlein, Jens and Kassem, Layla and Strohmaier, Jana and Herms, Stefan and M{\"{u}}hleisen, Thomas W and Degenhardt, Franziska and Cichon, Sven and N{\"{o}}then, Markus and Karypis, George and Consortium, Bipolar Disorder Genetics (BiGS) and McMahon, Francis J and Rietschel, Marcella and Schulze, Thomas G.},
doi = {10.1101/116624},
file = {:home/jennifer/Descargas/116624.full.pdf:pdf},
journal = {bioRxiv},
pages = {116624},
title = {{Genotype-phenotype association mining in bipolar disorder: market research meets complex genetics}},
url = {https://www.biorxiv.org/content/early/2017/03/14/116624},
year = {2017}
}
@article{Hahsler2005,
abstract = {Mining frequent itemsets and association rules is a popular and well researched approach for discovering interesting relationships between variables in large databases. The R package \pkgarules presented in this paper provides a basic infrastructure for creating and manipulating input data sets and for analyzing the resulting itemsets and rules. The package also includes interfaces to two fast mining algorithms, the popular C implementations of Apriori and Eclat by Christian Borgelt. These algorithms can be used to mine frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hahsler, Michael and Gr{\"{u}}n, Bettina and Hornik, Kurt},
doi = {10.1007/978-1-84800-201-2},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/Descargas/arules.pdf:pdf},
isbn = {978-1-84800-200-5},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {apriori,association rules,data mining,eclat},
number = {15},
pages = {1--25},
pmid = {25246403},
title = {{Mathematical Tools for Data Mining}},
url = {http://link.springer.com/10.1007/978-1-84800-201-2},
volume = {14},
year = {2008}
}
@article{Neto2014,
abstract = {Inherited myopathies are a heterogeneous group of disabling disorders with still barely understood pathological mechanisms. Around 40% of afflicted patients remain without a molecular diagnosis after exclusion of known genes. The advent of high-throughput sequencing has opened avenues to the discovery of new implicated genes, but a working list of prioritized candidate genes is necessary to deal with the complexity of analyzing large-scale sequencing data. Here we used an integrative data mining strategy to analyze the genetic network linked to myopathies, derive specific signatures for inherited myopathy and related disorders, and identify and rank candidate genes for these groups. Training sets of genes were selected after literature review and used in Manteia, a public web-based data mining system, to extract disease group signatures in the form of enriched descriptor terms, which include functional annotation, human and mouse phenotypes, as well as biological pathways and protein interactions. These specific signatures were then used as an input to mine and rank candidate genes, followed by filtration against skeletal muscle expression and association with known diseases. Signatures and identified candidate genes highlight both potential common pathological mechanisms and allelic disease groups. Recent discoveries of gene associations to diseases, like B3GALNT2, GMPPB and B3GNT1 to congenital muscular dystrophies, were prioritized in the ranked lists, suggesting a posteriori validation of our approach and predictions. We show an example of how the ranked lists can be used to help analyze high-throughput sequencing data to identify candidate genes, and highlight the best candidate genes matching genomic regions linked to myopathies without known causative genes. This strategy can be automatized to generate fresh candidate gene lists, which help cope with database annotation updates as new knowledge is incorporated.},
author = {Neto, Osorio Abath and Tassy, Olivier and Biancalana, Val{\'{e}} Rie and Zanoteli, Edmar and Pourqui{\'{e}}, Olivier and Laporte, Jocelyn},
doi = {10.1371/journal.pone.0110888},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto et al. - 2014 - Integrative data mining highlights candidate genes for monogenic myopathies.pdf:pdf},
isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {25353622},
title = {{Integrative data mining highlights candidate genes for monogenic myopathies}},
volume = {9},
year = {2014}
}
@inproceedings{Buchanan2012,
author = {Buchanan, Carrie C. and Wallace, John R. and Frase, Alex T. and Torstenson, Eric S. and Pendergrass, Sarah A. and Ritchie, Marylyn D.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29066-4_18},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchanan et al. - 2012 - A biologically informed method for detecting associations with rare variants.pdf:pdf},
isbn = {9783642290657},
issn = {03029743},
keywords = {Collapsing Tool,Pathway Analysis,Prior Knowledge,Rare Variants},
pages = {201--210},
pmid = {27582876},
title = {{A biologically informed method for detecting associations with rare variants}},
volume = {7246 LNCS},
year = {2012}
}
@article{Thiebaut2017,
abstract = {The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.},
archivePrefix = {arXiv},
arxivId = {1712.02259},
author = {Thiebaut, Nicolas and Simoulin, Antoine and Neuberger, Karl and Ibnoushein, Issam and Bousquet, Nicolas and Reix, Nathalie and Moli{\`{e}}re, S{\'{e}}bastien and Mathelin, Carole},
eprint = {1712.02259},
file = {:home/jennifer/Descargas/1712.02259.pdf:pdf},
journal = {arXiv},
title = {{An innovative solution for breast cancer textual big data analysis}},
url = {http://arxiv.org/abs/1712.02259%0Ahttps://arxiv.org/abs/1712.02259},
year = {2017}
}
@article{Kawashima2017,
author = {Kawashima, Koya},
file = {:home/jennifer/Descargas/kawashima2017.pdf:pdf},
isbn = {9781509055043},
keywords = {are applied for finding,association words that indicate,biomedical literatures,breast,cancer,clustering,k-means clustering,relation extraction,text mining,the candidate,the relationship between breast},
pages = {1--5},
title = {{Text Mining and Pattern Clustering for Relation Extraction of Breast Cancer and Related Genes}},
year = {2017}
}
@article{Kodinariya2013,
abstract = {Clustering is widely used in different field such as biology, psychology, and economics. The result of clustering varies as number of cluster parameter changes hence main challenge of cluster analysis is that the number of clusters or the number of model parameters is seldom known, and it must be determined before clustering. The several clustering algorithm has been proposed. Among them k-means method is a simple and fast clustering technique. We address the problem of cluster number selection by using a k-means approach We can ask end users to provide a number of clusters in advance, but it is not feasible end user requires domain knowledge of each data set. There are many methods available to estimate the number of clusters such as statistical indices, variance based method, Information Theoretic, goodness of fit method etc...The paper explores six different approaches to determine the right number of clusters in a dataset Keywords: Akaike's information criterion, Bayesian inference criterion, Clustering, Cross-validation, Elbow Method, Jump Method, Number of Cluster, Silhouette.},
author = {Kodinariya, Trupti M and Makwana, Prashant R},
file = {:home/jennifer/Descargas/Review_on_determining_number_of_Cluster.pdf:pdf},
journal = {International Journal of Advance Research in Computer Science and Management Studies},
keywords = {akaike,bayesian inference criterion,clustering,cross-validation,elbow method,jump,method,number of cluster,s information criterion,silhouette},
number = {6},
pages = {2321--7782},
title = {{Review on determining number of Cluster in K-Means Clustering}},
volume = {1},
year = {2013}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Jain, Anil K.},
doi = {10.1016/j.patrec.2009.09.011},
eprint = {0402594v3},
file = {:home/jennifer/Descargas/jain2010.pdf:pdf},
isbn = {9781424417360},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma},
number = {8},
pages = {651--666},
pmid = {21856},
primaryClass = {arXiv:cond-mat},
publisher = {Elsevier B.V.},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.09.011},
volume = {31},
year = {2010}
}
@article{Huang2008,
abstract = {Clustering is a useful technique that organizes a large quan- tity of unordered text documents into a small number of meaningful and coherent clusters, thereby providing a ba- sis for intuitive and informative navigation and browsing mechanisms. Partitional clustering algorithms have been recognized to be more suitable as opposed to the hierar- chical clustering schemes for processing large datasets. A wide variety of distance functions and similarity measures have been used for clustering, such as squared Euclidean distance, cosine similarity, and relative entropy. In this paper, we compare and analyze the effectiveness of these measures in partitional clustering for text docu- ment datasets. Our experiments utilize the standard K- means algorithm and we report results on seven text doc- ument datasets and five distance/similarity measures that have been most commonly used in text clustering.},
author = {Huang, Anna},
file = {:home/jennifer/Descargas/Similarity_Measures_for_Text_Document_Cl.pdf:pdf},
journal = {Proceedings of the Sixth New Zealand},
keywords = {Clustering,I53 [Clustering],partitional clustering,text clustering},
number = {April},
pages = {49--56},
title = {{Similarity measures for text document clustering}},
url = {http://nzcsrsc08.canterbury.ac.nz/site/proceedings/Individual_Papers/pg049_Similarity_Measures_for_Text_Document_Clustering.pdf},
year = {2008}
}
@article{Cartwright2012,
abstract = {Recent advances in high-throughput DNA sequencing technologies and associated statistical analyses have enabled in-depth analysis of whole-genome sequences. As this technology is applied to a growing number of individual human genomes, entire families are now being sequenced. Information contained within the pedigree of a sequenced family can be leveraged when inferring the donors' genotypes. The presence of a de novo mutation within the pedigree is indicated by a violation of Mendelian inheritance laws. Here, we present a method for probabilistically inferring genotypes across a pedigree using high-throughput sequencing data and producing the posterior probability of de novo mutation at each genomic site examined. This framework can be used to disentangle the effects of germline and somatic mutational processes and to simultaneously estimate the effect of sequencing error and the initial genetic variation in the population from which the founders of the pedigree arise. This approach is examined in detail through simulations and areas for method improvement are noted. By applying this method to data from members of a well-defined nuclear family with accurate pedigree information, the stage is set to make the most direct estimates of the human mutation rate to date.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cartwright, Reed A. and Hussin, Julie and Keebler, Jonathan E M and Stone, Eric A. and Awadalla, Philip},
doi = {10.2202/1544-6115.1713},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright et al. - 2012 - S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput S.pdf:pdf},
isbn = {1544-6115 (Electronic)\r1544-6115 (Linking)},
issn = {15446115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {De novo mutations,Mutation rates,Pedigree,Short-read data,Trio model},
number = {2},
pmid = {22499693},
title = {{A family-based probabilistic method for capturing de novo mutations from high-throughput short-read sequencing data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3728889&tool=pmcentrez&rendertype=abstract%5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/22499693%5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3728889},
volume = {11},
year = {2012}
}
@article{Farrell2008,
abstract = {Newborn screening (NBS) for cystic fibrosis (CF) is increasingly being implemented and is soon likely to be in use throughout the United States, because early detection permits access to specialized medical care and improves outcomes. The diagnosis of CF is not always straightforward, however. The sweat chloride test remains the gold standard for CF diagnosis but does not always give a clear answer. Genotype analysis also does not always provide clarity; more than 1500 mutations have been identified in the CF transmembrane conductance regulator (CFTR) gene, not all of which result in CF. Harmful mutations in the gene can present as a spectrum of pathology ranging from sinusitis in adulthood to severe lung, pancreatic, or liver disease in infancy. Thus, CF identified postnatally must remain a clinical diagnosis. To provide guidance for the diagnosis of both infants with positive NBS results and older patients presenting with an indistinct clinical picture, the Cystic Fibrosis Foundation convened a meeting of experts in the field of CF diagnosis. Their recommendations, presented herein, involve a combination of clinical presentation, laboratory testing, and genetics to confirm a diagnosis of CF. {\textcopyright} 2008 Mosby, Inc. All rights reserved.},
author = {Farrell, Philip M. and Rosenstein, Beryl J. and White, Terry B. and Accurso, Frank J. and Castellani, Carlo and Cutting, Garry R. and Durie, Peter R. and LeGrys, Vicky A. and Massie, John and Parad, Richard B. and Rock, Michael J. and Campbell, Preston W.},
doi = {10.1016/j.jpeds.2008.05.005},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farrell et al. - 2008 - Guidelines for diagnosis of cystic fibrosis in newborns through older adults Cystic Fibrosis Foundation conse(2).pdf:pdf},
isbn = {00223476},
issn = {00223476},
journal = {Journal of Pediatrics},
month = {aug},
number = {2},
pages = {S4--S14},
pmid = {18639722},
publisher = {NIH Public Access},
title = {{Guidelines for Diagnosis of Cystic Fibrosis in Newborns through Older Adults: Cystic Fibrosis Foundation Consensus Report}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18639722 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2810958},
volume = {153},
year = {2008}
}
@article{Rowntree2003,
abstract = {Cystic fibrosis is a common autosomal recessive disorder that primarily affects the epithelial cells in the intestine, respiratory system, pancreas, gall bladder and sweat glands. Over one thousand mutations have currently been identified in the Cystic Fibrosis Transmembrane Conductance Regulator (CFTR) gene that are associated with CF disease. There have been many studies on the correlation of the CFTR genotype and CF disease phenotype; however, this relationship is still not well understood. A connection between CFTR genotype and disease manifested in the pancreas has been well described, but pulmonary disease appears to be highly variable even between individuals with the same genotype. This review describes the current classification of CFTR mutation classes and resulting CF disease phenotypes. Complex disease alleles and modifier genes are discussed along with alternative disorders, such as disseminated bronchiectasis and pancreatitis, which are also thought to result from CFTR mutations.},
author = {Rowntree, Rebecca K. and Harris, Ann},
doi = {10.1046/j.1469-1809.2003.00028.x},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rowntree, Harris - 2003 - The Phenotypic Consequences of CFTR Mutations.pdf:pdf},
isbn = {0003-4800 (Print)\r0003-4800 (Linking)},
issn = {00034800},
journal = {Annals of Human Genetics},
month = {sep},
number = {5},
pages = {471--485},
pmid = {12940920},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The phenotypic consequences of CFTR mutations}},
url = {http://doi.wiley.com/10.1046/j.1469-1809.2003.00028.x},
volume = {67},
year = {2003}
}
@article{Stenson2017,
abstract = {The Human Gene Mutation Database (HGMD{\textregistered}) constitutes a comprehensive collection of published germline mutations in nuclear genes that underlie, or are closely associated with human inherited disease. At the time of writing (March 2017), the database contained in excess of 203,000 different gene lesions identified in over 8000 genes manually curated from over 2600 journals. With new mutation entries currently accumulating at a rate exceeding 17,000 per annum, HGMD represents de facto the central unified gene/disease-oriented repository of heritable mutations causing human genetic disease used worldwide by researchers, clinicians, diagnostic laboratories and genetic counsellors, and is an essential tool for the annotation of next-generation sequencing data. The public version of HGMD ( http://www.hgmd.org ) is freely available to registered users from academic institutions and non-profit organisations whilst the subscription version (HGMD Professional) is available to academic, clinical and commercial users under license via QIAGEN Inc.},
author = {Stenson, Peter D. and Mort, Matthew and Ball, Edward V. and Evans, Katy and Hayden, Matthew and Heywood, Sally and Hussain, Michelle and Phillips, Andrew D. and Cooper, David N.},
doi = {10.1007/s00439-017-1779-6},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stenson et al. - 2017 - The Human Gene Mutation Database towards a comprehensive repository of inherited mutation data for medical resea.pdf:pdf},
isbn = {1432-1203},
issn = {14321203},
journal = {Human Genetics},
month = {jun},
number = {6},
pages = {665--677},
pmid = {28349240},
publisher = {Springer Berlin Heidelberg},
title = {{The Human Gene Mutation Database: towards a comprehensive repository of inherited mutation data for medical research, genetic diagnosis and next-generation sequencing studies}},
url = {http://link.springer.com/10.1007/s00439-017-1779-6},
volume = {136},
year = {2017}
}
