@article{Kumari2014,
author = {Kumari, D Aruna and Bhavana, D Poojitha and Aditya, V Venkata Sai},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumari, Bhavana, Aditya - 2014 - Data Mining in Biodata Analysis.pdf:pdf},
number = {9},
pages = {1--3},
title = {{Data Mining in Biodata Analysis}},
volume = {14},
year = {2014}
}
@article{Parnell2014,
abstract = {BACKGROUND: Genetic understanding of complex traits has developed immensely over the past decade but remains hampered by incomplete descriptions of contribution to phenotypic variance. Gene-environment (GxE) interactions are one of these contributors and in the guise of diet and physical activity are important modulators of cardiometabolic phenotypes and ensuing diseases.$\backslash$n$\backslash$nRESULTS: We mined the scientific literature to collect GxE interactions from 386 publications for blood lipids, glycemic traits, obesity anthropometrics, vascular measures, inflammation and metabolic syndrome, and introduce CardioGxE, a gene-environment interaction resource. We then analyzed the genes and SNPs supporting cardiometabolic GxEs in order to demonstrate utility of GxE SNPs and to discern characteristics of these important genetic variants. We were able to draw many observations from our extensive analysis of GxEs. 1) The CardioGxE SNPs showed little overlap with variants identified by main effect GWAS, indicating the importance of environmental interactions with genetic factors on cardiometabolic traits. 2) These GxE SNPs were enriched in adaptation to climatic and geographical features, with implications on energy homeostasis and response to physical activity. 3) Comparison to gene networks responding to plasma cholesterol-lowering or regression of atherosclerotic plaques showed that GxE genes have a greater role in those responses, particularly through high-energy diets and fat intake, than do GWAS-identified genes for the same traits. Other aspects of the CardioGxE dataset were explored.$\backslash$n$\backslash$nCONCLUSIONS: Overall, we demonstrate that SNPs supporting cardiometabolic GxE interactions often exhibit transcriptional effects or are under positive selection. Still, not all such SNPs can be assigned potential functional or regulatory roles often because data are lacking in specific cell types or from treatments that approximate the environmental factor of the GxE. With research on metabolic related complex disease risk embarking on genome-wide GxE interaction tests, CardioGxE will be a useful resource.},
author = {Parnell, Laurence D and Blokker, Britt A and Dashti, Hassan S and Nesbeth, Paula-Dene and Cooper, Brittany Elle and Ma, Yiyi and Lee, Yu-Chi and Hou, Ruixue and Lai, Chao-Qiang and Richardson, Kris and Ordov{\'{a}}s, Jos{\'{e}} M},
doi = {10.1186/1756-0381-7-21},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnell et al. - 2014 - CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.pdf:pdf},
issn = {1756-0381},
journal = {BioData mining},
pages = {21},
pmid = {25368670},
title = {{CardioGxE, a catalog of gene-environment interactions for cardiometabolic traits.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4217104{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2014}
}
@article{Xu2016,
author = {Xu, Zhiwei and Chi, Xuebin and Xiao, Nong},
doi = {10.1093/nsr/nww001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Chi, Xiao - 2016 - High-Performance Computing Environment A Review of Twenty Years of Experiments in China.pdf:pdf},
issn = {2053714X},
keywords = {22-dec-2015,24-sep-2015,25-dec-2015,accepted,cyberinfrastructure,e-science environment,middleware,received,revised,supercomputing},
pages = {1--24},
title = {{High-Performance Computing Environment : A Review of Twenty Years of Experiments in China}},
year = {2016}
}
@article{Wang2009,
abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
doi = {10.1038/nrg2484},
eprint = {NIHMS150003},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gerstein, Snyder - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf:pdf},
isbn = {1471-0064 (Electronic)$\backslash$r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
keywords = {Animals,Base Sequence,Chromosome Mapping,Exons,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Models, Genetic,Molecular Sequence Data,RNA,RNA: analysis,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Transcription, Genetic},
number = {1},
pages = {57--63},
pmid = {19015660},
title = {{RNA-Seq: a revolutionary tool for transcriptomics.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2949280{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Cleary2014,
author = {Cleary, John G and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S and Inglis, Stuart and Irvine, Sean A and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M},
doi = {10.1089/cmb.2014.0029},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and {\{}{\textless}{\}}i{\{}{\textgreater}{\}}De Novo{\{}{\textless}{\}}/i{\{}{\textgreater}{\}} Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@article{Li2014a,
abstract = {MOTIVATION: Whole-genome high-coverage sequencing has been widely used for personal and cancer genomics as well as in various research areas. However, in the lack of an unbiased whole-genome truth set, the global error rate of variant calls and the leading causal artifacts still remain unclear even given the great efforts in the evaluation of variant calling methods.$\backslash$n$\backslash$nRESULTS: We made 10 single nucleotide polymorphism and INDEL call sets with two read mappers and five variant callers, both on a haploid human genome and a diploid genome at a similar coverage. By investigating false heterozygous calls in the haploid genome, we identified the erroneous realignment in low-complexity regions and the incomplete reference genome with respect to the sample as the two major sources of errors, which press for continued improvements in these two areas. We estimated that the error rate of raw genotype calls is as high as 1 in 10-15 kb, but the error rate of post-filtered calls is reduced to 1 in 100-200 kb without significant compromise on the sensitivity. Availability and implementation: BWA-MEM alignment and raw variant calls are available at http://bit.ly/1g8XqRt scripts and miscellaneous data at https://github.com/lh3/varcmp.$\backslash$n$\backslash$nCONTACT: hengli@broadinstitute.org Supplementary information: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.0929v1},
author = {Li, Heng},
doi = {10.1093/bioinformatics/btu356},
eprint = {arXiv:1404.0929v1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2014 - Toward better understanding of artifacts in variant calling from high-coverage samples.pdf:pdf},
isbn = {1367-4803},
issn = {13674811},
journal = {Bioinformatics (Oxford, England)},
number = {20},
pages = {2843--2851},
pmid = {24974202},
title = {{Toward better understanding of artifacts in variant calling from high-coverage samples}},
volume = {30},
year = {2014}
}
@article{Sims2014,
abstract = {Sequencing technologies have placed a wide range of genomic analyses within the capabilities of many laboratories. However, sequencing costs often set limits to the amount of sequences that can be generated and, consequently, the biological outcomes that can be achieved from an experimental design. In this Review, we discuss the issue of sequencing depth in the design of next-generation sequencing experiments. We review current guidelines and precedents on the issue of coverage, as well as their underlying considerations, for four major study designs, which include de novo genome sequencing, genome resequencing, transcriptome sequencing and genomic location analyses (for example, chromatin immunoprecipitation followed by sequencing (ChIP-seq) and chromosome conformation capture (3C)).},
author = {Sims, David and Sudbery, Ian and Ilott, Nicholas E and Heger, Andreas and Ponting, Chris P},
doi = {10.1038/nrg3642},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sims et al. - 2014 - Genomics is extending its reach into diverse fields of biomedical research from agriculture to clinical diag- nosti.pdf:pdf},
isbn = {1471-0056},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
number = {2},
pages = {121--32},
pmid = {24434847},
title = {{Sequencing depth and coverage: key considerations in genomic analyses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24434847},
volume = {15},
year = {2014}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Li2009a,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803$\backslash$r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Stelzer,
abstract = {GeneCards, the human gene compendium, enables researchers to effectively navigate and inter-relate the wide universe of human genes, diseases, variants, proteins, cells, and biological pathways. Our recently launched Version 4 has a revamped infrastructure facilitating faster data updates, better-targeted data queries, and friendlier user experience. It also provides a stronger foundation for the GeneCards suite of companion databases and analysis tools. Improved data unification includes gene-disease links via MalaCards and merged biological pathways via PathCards, as well as drug information and proteome expression. VarElect, another suite member, is a phenotype prioritizer for next-generation sequencing, leveraging the GeneCards and MalaCards knowledgebase. It au-tomatically infers direct and indirect scored associations between hundreds or even thousands of variant-containing genes and disease phenotype terms. Var-Elect's capabilities, either independently or within TGex, our comprehensive variant analysis pipeline, help prepare for the challenge of clinical projects that involve thousands of exome/genome NGS analyses. C},
author = {Stelzer, Gil and Rosen, Naomi and Plaschkes, Inbar and Zimmerman, Shahar and Twik, Michal and Fishilevich, Simon and Stein, Tsippi Iny and Nudel, Ron and Lieder, Iris and Mazor, Yaron and Kaplan, Sergey and Dahary, Dvir and Warshawsky, David and Guan-Golan, Yaron and Kohn, Asher and Rappaport, Noa and Safran, Marilyn and Lancet, Doron},
doi = {10.1002/cpbi.5},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stelzer et al. - Unknown - The GeneCards Suite From Gene Data Mining to Disease Genome Sequence Analyses.pdf:pdf},
journal = {Curr. Protoc. Bioinform},
keywords = {Elect,Gene,Var,biological database r bioinformatics r diseases r},
number = {1},
pages = {1--130},
title = {{The GeneCards Suite: From Gene Data Mining to Disease Genome Sequence Analyses}},
volume = {5430}
}
@article{Parla,
abstract = {Background: Human exome resequencing using commercial target capture kits has been and is being used for sequencing large numbers of individuals to search for variants associated with various human diseases. We rigorously evaluated the capabilities of two solution exome capture kits. These analyses help clarify the strengths and limitations of those data as well as systematically identify variables that should be considered in the use of those data. Results: Each exome kit performed well at capturing the targets they were designed to capture, which mainly corresponds to the consensus coding sequences (CCDS) annotations of the human genome. In addition, based on their respective targets, each capture kit coupled with high coverage Illumina sequencing produced highly accurate nucleotide calls. However, other databases, such as the Reference Sequence collection (RefSeq), define the exome more broadly, and so not surprisingly, the exome kits did not capture these additional regions. Conclusions: Commercial exome capture kits provide a very efficient way to sequence select areas of the genome at very high accuracy. Here we provide the data to help guide critical analyses of sequencing data derived from these products. Background},
author = {Parla, Jennifer S and Iossifov, Ivan and Grabill, Ian and Spector, Mona S and Kramer, Melissa and Mccombie, W Richard},
doi = {10.1186/gb-2011-12-9-r97},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parla et al. - Unknown - A comparative analysis of exome capture.pdf:pdf},
title = {{A comparative analysis of exome capture}}
}
@article{Buchard2016,
abstract = {The HID-Ion AmpliSeq™ Identity Panel is a next-generation sequencing assay with 90 autosomal and 34 Y-chromosome SNPs that are amplified in one PCR step and subsequently sequenced using the Ion Personal Genome Machine (Ion PGM™) System. This assay was validated for relationship testing in our ISO 17025 accredited laboratory in 2015. Here, the essential parts of the validation report submitted to the Danish Accreditation Fund are presented. A total of 100 unrelated Danes were typed in duplicates and the locus balance, heterozygote balance (Hb) and noise levels were analysed in detail. Two loci were disregarded for casework because genotyping was uncertain. Hb for rs7520386 was skewed and high levels of noise were observed in rs576261. Three general acceptance criteria for analysis of single-source samples were defined: (i) sequencing depth {\textgreater} 200 reads, (ii) noise level {\textless} 3{\%} and (iii) Hb {\textgreater} 0.3. A Python script named SNPonPGM was developed to assist the analyst by highlighting loci that do not fulfil the general acceptance criteria. Furthermore, SNPonPGM has functions that reduce the hands-on time of the reporting officer to a few minutes per case. Mixtures with DNA from two individuals in a 1:24 ratio were readily identified using the three criteria and the SNPonPGM script.},
author = {Buchard, Anders and Kampmann, Marie-Louise and Poulsen, Lena and B{\o}rsting, Claus and Morling, Niels},
doi = {10.1002/elps.201600269},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchard et al. - 2016 - ISO 17025 validation of a next-generation sequencing assay for relationship testing(3).pdf:pdf},
issn = {1522-2683},
journal = {Electrophoresis},
keywords = {Forensic genetics,ISO 17025 accreditation,Kinship testing,Next-generation sequencing,SNPs},
pages = {1--10},
pmid = {27709635},
title = {{ISO 17025 validation of a next-generation sequencing assay for relationship testing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27709635},
year = {2016}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Systems2009,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - FASTQ format and data quality.pdf:pdf},
title = {{FASTQ format and data quality}},
year = {2009}
}
@article{Yang2015,
abstract = {Recent developments in sequencing techniques have enabled rapid and high-throughput generation of sequence data, democratizing the ability to compile information on large amounts of genetic variations in individual laboratories. However, there is a growing gap between the generation of raw sequencing data and the extraction of meaningful biological information. Here, we describe a protocol to use the ANNOVAR (ANNOtate VARiation) software to facilitate fast and easy variant annotations, including gene-based, region-based and filter-based annotations on a variant call format (VCF) file generated from human genomes. We further describe a protocol for gene-based annotation of a newly sequenced nonhuman species. Finally, we describe how to use a user-friendly and easily accessible web server called wANNOVAR to prioritize candidate genes for a Mendelian disease. The variant annotation protocols take 5-30 min of computer time, depending on the size of the variant file, and 5-10 min of hands-on time. In summary, through the command-line tool and the web server, these protocols provide a convenient means to analyze genetic variants generated in humans and other species.},
author = {Yang, H and Wang, K},
doi = {10.1038/nprot.2015.105},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Wang - 2015 - Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR.pdf:pdf},
isbn = {1750-2799 (Electronic)$\backslash$r1750-2799 (Linking)},
issn = {1750-2799},
journal = {Nat Protoc},
number = {10},
pages = {1556--1566},
pmid = {26379229},
publisher = {Nature Publishing Group},
title = {{Genomic variant annotation and prioritization with ANNOVAR and wANNOVAR}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26379229{\%}5Cnhttp://www.nature.com/nprot/journal/v10/n10/pdf/nprot.2015.105.pdf},
volume = {10},
year = {2015}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Zaki2007,
abstract = {This is a meeting report for the 6th SIGKDD Workshop on Data Mining in Bioinformatics.},
author = {Zaki, Mohammed J and Karypis, George and Yang, Jiong},
doi = {10.1186/1748-7188-2-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Karypis, Yang - 2007 - Data Mining in Bioinformatics (BIOKDD).pdf:pdf},
isbn = {1852335831},
issn = {17487188},
journal = {Algorithms for molecular biology : AMB},
pages = {4},
pmid = {17428327},
title = {{Data Mining in Bioinformatics (BIOKDD).}},
volume = {2},
year = {2007}
}
@article{Frazer2009,
abstract = {The last few years have seen extensive efforts to catalogue human genetic variation and correlate it with phenotypic differences. Most common SNPs have now been assessed in genome-wide studies for statistical associations with many complex traits, including many important common diseases. Although these studies have provided new biological insights, only a limited amount of the heritable component of any complex trait has been identified and it remains a challenge to elucidate the functional link between associated variants and phenotypic traits. Technological advances, such as the ability to detect rare and structural variants, and a clear understanding of the challenges in linking different types of variation with phenotype, will be essential for future progress.},
author = {Frazer, KA and Murray, SS and Schork, NJ and Topol, EJ},
doi = {10.1038/nrg2554},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frazer et al. - 2009 - Human genetic variation and its contribution to complex traits.pdf:pdf},
isbn = {1471-0064 (Electronic)1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Reviews Genetics},
number = {4},
pages = {241--51},
pmid = {19293820},
title = {{Human genetic variation and its contribution to complex traits}},
volume = {10},
year = {2009}
}
@misc{Paez2012,
abstract = {El contexto din{\{}{\'{a}}{\}}mico y competitivo de la organizaci{\{}{\'{o}}{\}}n actual exige permanentes soluciones inform{\{}{\'{a}}{\}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\{}{\'{o}}{\}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\{}{\'{o}}{\}}n mayormente, desde el punto de vista tecnol{\{}{\'{o}}{\}}gico, sin considerar los aspectos organizacionales y metodol{\{}{\'{o}}{\}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
issn = {0120-341X},
keywords = {Sistemas de informaci{\{}{\'{o}}{\}}n en administraci{\{}{\'{o}}{\}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\{}{\'{o}}{\}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Guo2016,
abstract = {The HID-Ion AmpliSeq??? Identity Panel (the HID Identity Panel) is designed to detect 124-plex single nucleotide polymorphisms (SNPs) with next generation sequencing (NGS) technology on the Ion Torrent PGM??? platform, including 90 individual identification SNPs (IISNPs) on autosomal chromosomes and 34 lineage informative SNPs (LISNPs) on Y chromosome. In this study, we evaluated performance for the HID Identity Panel to provide a reference for NGS-SNP application, focusing on locus strand balance, locus coverage balance, heterozygote balance, and background signals. Besides, several experiments were carried out to find out improvements and limitations of this panel, including studies of species specificity, repeatability and concordance, sensitivity, mixtures, case-type samples and degraded samples, population genetics and pedigrees following the Scientific Working Group on DNA Analysis Methods (SWGDAM) guidelines. In addition, Southern and Northern Chinese Han were investigated to assess applicability of this panel. Results showed this panel led to cross-reactivity with primates to some extent but rarely with non-primate animals. Repeatable and concordant genotypes could be obtained in triplicate with one exception at rs7520386. Full profiles could be obtained from 100 pg input DNA, but the optimal input DNA would be 1 ng???200 pg with 21 initial PCR cycles. A sample with ???20{\%} minor contributor could be considered as a mixture by the number of homozygotes, and full profiles belonging to minor contributors could be detected between 9:1 and 1:9 mixtures with known reference profiles. Also, this assay could be used for case-type samples and degraded samples. For autosomal SNPs (A-SNPs), FST across all 90 loci was not significantly different between Southern and Northern Chinese Han or between male and female samples. All A-SNP loci were independent in Chinese Han population. Except for 18 loci with He {\textless}0.4, most of the A-SNPs in the HID Identity Panel presented high polymorphisms. Forensic parameters were calculated as {\textgreater}99.999{\%} for combined discrimination power (CDP), 0.999999724 for combined power of exclusion (CPE), 1.390 ?? 1011 for combined likelihood ratio (CLR) of trios, and 2.361 ?? 106 for CLR of motherless duos. For Y-SNPs, a total of 8 haplotypes were observed with the value of 0.684 for haplotype diversity. As a whole, the HID Identity Panel is a well-performed, robust, reliable and high informative NGS-SNP assay and it can fully meet requirements for individual identification and paternity testing in forensic science.},
author = {Guo, Fei and Zhou, Yishu and Song, He and Zhao, Jinling and Shen, Hongying and Zhao, Bin and Liu, Feng and Jiang, Xianhua},
doi = {10.1016/j.fsigen.2016.07.021},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2016 - Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Evaluation,HID-Ion AmpliSeq???,Identity Panel,Ion Torrent PGM???,Next generation sequencing (NGS),Population genetics,Single nucleotide polymorphism (SNP)},
pages = {73--84},
pmid = {27500651},
publisher = {Elsevier Ireland Ltd},
title = {{Next generation sequencing of SNPs using the HID-Ion AmpliSeqTM Identity Panel on the Ion Torrent PGMTM platform}},
url = {http://dx.doi.org/10.1016/j.fsigen.2016.07.021},
volume = {25},
year = {2016}
}
@article{Baes2014a,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{\{}{\%}{\}} (87.0{\{}{\%}{\}}) and average genotype concordance in correctly identified SNVs was 99.2{\{}{\%}{\}} (99.2{\{}{\%}{\}}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.{\$}\backslash{\$}n{\$}\backslash{\$}nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle(2).pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{Li2014,
abstract = {In ''Omics'' era of the life sciences, data is presented in many forms, which represent the information at various levels of bio-logical systems, including data about genome, transcriptome, epigenome, proteome, metabolome, molecular imaging, molec-ular pathways, different population of people and clinical/med-ical records. The biological data is big, and its scale has already been well beyond petabyte (PB) even exabyte (EB). Nobody doubts that the biological data will create huge amount of val-ues, if scientists can overcome many challenges, e.g., how to handle the complexity of information, how to integrate the data from very heterogeneous resources, what kind of principles or standards to be adopted when facing with the big data. Tools and techniques for analyzing big biological data enable us to translate massive amount of information into a better under-standing of the basic biomedical mechanisms, which can be fur-ther applied to translational or personalized medicine. Today, big data is one of the hottest topics in information science, but its concept can be misleading or confusing. The name itself suggests huge amount of data, which, however, represents only one aspect. In general, big data has four impor-tant features, so called four V's: volume of data, velocity of processing the data, variability of data sources, and veracity of the data quality. These four hallmarks of big data require to be characterized by special theory and technology; however, currently there is no satisfactory solution. Now, more biolo-gists are involved with the big data due to the rapid advance of high-throughput biotechnologies. As an example, the Human Genome Project utilized the expertise, infrastructure, and people from 20 institutions and took 13 years of work with over {\$}3 billion to determine the whole genome structure of approximately three billion nucleotides. But now we can sequence a whole human genome for {\$}1000 and within three days. We have spent decades struggling to collect enough bio-logical and biomedical data, but when big data overwhelms us, are we ready to face the challenge? The new bottleneck to this problem in biology is how to reveal the essential mechanisms of biological systems by understanding the big noisy data. Life sciences today need more robust, expressive, computable, quantitative, accurate and precise ways to handle the big data. As a matter of fact, recent works in this area have already brought remarkable advantage and opportunities, which implies the central roles of bioinformatics and bioinformati-cians in the future research of the biological and biomedical fields. In the following text, we describe several aspects of big biological data based on our recent studies.},
author = {Li, Yixue and Chen, Luonan},
doi = {10.1016/j.gpb.2014.10.001},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Chen - 2014 - Big Biological Data Challenges and Opportunities Expanding volume of the big biological data and its bonanza.pdf:pdf},
issn = {1672-0229},
journal = {Genomics, Proteomics {\&} Bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Data Mining,Data Mining: methods,Gene Expression Profiling,High-Throughput Screening Assays,Humans,Software},
number = {5},
pages = {187--189},
pmid = {25462151},
publisher = {Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China},
title = {{Big Biological Data: Challenges and Opportunities Expanding volume of the big biological data and its bonanza}},
url = {http://www.sciencedirect.com/science/article/pii/S1672022914001041},
volume = {12},
year = {2014}
}
@article{Trapnell2009,
abstract = {MOTIVATION: A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.$\backslash$n$\backslash$nRESULTS: We mapped the RNA-Seq reads from a recent mammalian RNA-Seq experiment and recovered more than 72{\%} of the splice junctions reported by the annotation-based software from that study, along with nearly 20,000 previously unreported junctions. The TopHat pipeline is much faster than previous systems, mapping nearly 2.2 million reads per CPU hour, which is sufficient to process an entire RNA-Seq experiment in less than a day on a standard desktop computer. We describe several challenges unique to ab initio splice site discovery from RNA-Seq reads that will require further algorithm development.$\backslash$n$\backslash$nAVAILABILITY: TopHat is free, open-source software available from http://tophat.cbcb.umd.edu.$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L.},
doi = {10.1093/bioinformatics/btp120},
eprint = {9605103},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trapnell, Pachter, Salzberg - 2009 - TopHat Discovering splice junctions with RNA-Seq.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {9},
pages = {1105--1111},
pmid = {19289445},
primaryClass = {cs},
title = {{TopHat: Discovering splice junctions with RNA-Seq}},
volume = {25},
year = {2009}
}
@article{Biesecker2014,
abstract = {Sequencing of the genome or exome for clinical applications, hereafter referred to as clinical genome and exome sequencing (CGES), has now entered medical practice.1 Several thousand CGES tests have already been ordered for patients, with the goal of establishing diagnoses for rare, clinically unrecognizable, or puzzling disorders that are suspected to be genetic in origin. We anticipate increases in the use of CGES, the key attribute of which — its breadth — distinguishes it from other forms of laboratory testing. The interrogation of variation in about 20,000 genes simultaneously can be a powerful and effective diagnostic method.2 CGES has been hailed as an important tool in the implementation of predictive and individualized medicine, and there is intense research interest in the clinical benefits and risks of sequencing for screening healthy persons3; however, current practice recommendations4 do not support the use of sequencing for this purpose, and for that reason we do not further address it here. We have also limited this overview of CGES to the analysis of germline sequence variants for diagnostic purposes and do not discuss the use of CGES to uncover somatic variants in cancer in order to individualize cancer therapy. Clinicians should understand the diagnostic indications for CGES so that they can effectively deploy it in their practices. Because the success rate of CGES for the identification of a causative variant is approximately 25{\%},5 it is important to understand the basis of this testing and how to select the patients most likely to benefit from it. Here, we summarize the technologies underlying CGES and offer our insights into how clinicians should order such testing, interpret the results, and communicate the results to their patients (an interactive graphic giving an overview of the process is available with the full text of this article at NEJM.org).},
author = {Biesecker, Leslie G. and Green, Robert C.},
doi = {10.1056/NEJMra1312543},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Biesecker, Green - 2014 - Diagnostic Clinical Genome and Exome Sequencing.pdf:pdf},
isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
issn = {1533-4406},
journal = {New England Journal of Medicine},
keywords = {Exome,Genetic Counseling,Genetic Diseases, Inborn,Genetic Diseases, Inborn: diagnosis,Genetic Diseases, Inborn: genetics,Genetic Testing,Genome, Human,Humans,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {25},
pages = {2418--2425},
pmid = {24941179},
title = {{Diagnostic Clinical Genome and Exome Sequencing}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMra1312543},
volume = {370},
year = {2014}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{\%} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P. and Houwing-Duistermaat, Jeanine J. and {Eline Slagboom}, P. and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Bacardit2014,
abstract = {Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented.},
author = {Bacardit, Jaume and Widera, Pawe{\l} and Lazzarini, Nicola and Krasnogor, Natalio},
doi = {10.1089/big.2014.0023},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bacardit et al. - 2014 - Hard Data Analytics Problems Make for Better Data Analysis Algorithms Bioinformatics as an Example.pdf:pdf},
issn = {2167-6461},
journal = {Big data},
number = {3},
pages = {164--176},
pmid = {25276500},
title = {{Hard Data Analytics Problems Make for Better Data Analysis Algorithms: Bioinformatics as an Example.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4174911{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2014}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
doi = {10.1007/s13398-014-0173-7.2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cossio et al. - 2012 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Uma {\'{e}}tica para quantos?},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,epidural,esth{\'{e}}tique,est{\'{e}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,needle through a,nes corporales,perforaci{\'{o}}n corporal,piel,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
title = {{No Title No Title}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161 http://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991 http://www.scielo.cl/pdf/udecada/v15n26/art06.pdf http://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\&}partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@article{Hegde2017,
abstract = {Context.—With the decrease in the cost of sequencing, the clinical testing paradigm has shifted from single gene to gene panel and now whole-exome and whole-genome sequencing. Clinical laboratories are rapidly implementing next-generation sequencing–based whole-exome and whole-genome sequencing. Because a large number of targets are covered by whole-exome and whole-genome sequencing, it is critical that a laboratory perform appropriate validation studies, develop a quality assurance and quality control program, and participate in proficiency testing. Objective.—To provide recommendations for whole-exome and whole-genome sequencing assay design, validation, and implementation for the detection of germ-line variants associated in inherited disorders. Data Sources.—An example of trio sequencing, filtration and annotation of variants, and phenotypic consideration to arrive at clinical diagnosis is discussed. Conclusions.—It is critical that clinical laboratories planning to implement whole-exome and whole-genome sequencing design and validate the assay to specifications and ensure adequate performance prior to implementa-tion. Test design specifications, including variant filtering and annotation, phenotypic consideration, guidance on consenting options, and reporting of incidental findings, are provided. These are important steps a laboratory must take to validate and implement whole-exome and whole-genome sequencing in a clinical setting for germline variants in inherited disorders.},
author = {Hegde, Madhuri and Santani, Avni and Mao, Rong and Ferreira-Gonzalez, Andrea and Weck, Karen E. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2016-0622-RA},
file = {:home/jennifer/Descargas/hegde2017.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {798--805},
pmid = {28362156},
title = {{Development and validation of clinical whole-exome and whole-genome sequencing for detection of germline variants in inherited disease}},
volume = {141},
year = {2017}
}
@article{FigueroaFranco2014,
abstract = {La investigaci{\'{o}}n, recuperaci{\'{o}}n e identificaci{\'{o}}n de cuerpos relacionados con la desaparici{\'{o}}n de personas dentro del marco del conflicto armado en Colombia se ha llevado a cabo por parte del Estado: Fiscal{\'{i}}a General de la Naci{\'{o}}n (FGN), Polic{\'{i}}a Nacional, Departamento Administrativo de Seguridad (DAS) e Instituto Nacional de Medicina Legal y Ciencias Forenses (INMLCF). El an{\'{a}}lisis de ADN corresponde a una etapa del proceso de identificaci{\'{o}}n cuando otros abordajes –como el dactilosc{\'{o}}pico, odontol{\'{o}}gico y/o antropol{\'{o}}gico– no han permitido la identificaci{\'{o}}n fehaciente. Este an{\'{a}}lisis es de uso sistem{\'{a}}tico en laboratorios forenses a nivel mundial para comparar la informaci{\'{o}}n gen{\'{e}}tica de cad{\'{a}}veres en condici{\'{o}}n de no identificados con los posibles familiares de personas reportadas como desaparecidas. El presente estudio revis{\'{o}} la documentaci{\'{o}}n asociada a 154 solicitudes de identificaci{\'{o}}n recibidas durante 2009, analizadas por los peritos del Grupo de Gen{\'{e}}tica del INMLCF, sede Bogot{\'{a}}, D. C., logrando contribuir a la identificaci{\'{o}}n positiva de 95 cuerpos que inicialmente se encontraban en condici{\'{o}}n de no identificados (CNI). La mayor{\'{i}}a de muestras del estudio correspondieron a cuerpos exhumados de la regi{\'{o}}n Caribe colombiana. En el 80{\%} de las solicitudes se realiz{\'{o}} cotejo gen{\'{e}}tico entre perfiles gen{\'{e}}ticos de familiares y de restos humanos; en el 20{\%} los perfiles gen{\'{e}}ticos se almacenaron en la “Base Nacional de Perfiles Gen{\'{e}}ticos de Aplicaci{\'{o}}n Judicial”, conocida como CODIS por sus siglas en ingl{\'{e}}s (Combined DNA Index System). De las muestras {\'{o}}seas analizadas, el f{\'{e}}mur y la tibia arrojaron mejores resultados. El 77{\%} de los cotejos fueron no exclusiones, el 9{\%} resultados negativos o no concluyentes y el 14{\%} exclusiones.},
author = {{Figueroa Franco}, Ruth Marl{\'{e}}n and {Romero M{\'{a}}rtinez}, Rosa Elena and {Terreros Ib{\'{a}}{\~{n}}ez}, Grace Alejandra and {Alava Narv{\'{a}}ez}, Mar{\'{i}}a Cristina and {Vicu{\~{n}}a Giraldo}, Gloria Carolina and {Mart{\'{i}}n La Rotta}, Claudia Jannet},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueroa Franco et al. - 2014 - Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lis.pdf:pdf},
journal = {Revista Colombiana de medicina Legal y Ciencias Forense},
keywords = {Cromosomas,Mitochondrial ADN},
number = {Revista Colombiana de Medicina Legal y Ciencias Forenses},
pages = {8--14},
title = {{Identificaci{\'{o}}n De Personas Desaparecidas En El Marco Del Conflicto Armado Colombiano Mediante An{\'{a}}lisis Gen{\'{e}}tico De Restos Humanos Durante 2009 En El Inmlcf De Bogot{\'{a}}, D. C.}},
volume = {2},
year = {2014}
}
@article{Li2015,
abstract = {Background: Conditions associated with sudden cardiac arrest/death (SCA/D) in youth often have a genetic etiology. While SCA/D is uncommon, a pro-active family screening approach may identify these inherited structural and electrical abnormalities prior to symptomatic events and allow appropriate surveillance and treatment. This study investigated the diagnostic utility of exome sequencing (ES) by evaluating the capture and coverage of genes related to SCA/D. Methods: Samples from 102 individuals (13 with known molecular etiologies for SCA/D, 30 individuals without known molecular etiologies for SCA/D and 59 with other conditions) were analyzed following exome capture and sequencing at an average read depth of 100X. Reads were mapped to human genome GRCh37 using Novoalign, and post-processing and analysis was done using Picard and GATK. A total of 103 genes (2,190 exons) related to SCA/D were used as a primary filter. An additional 100 random variants within the targeted genes associated with SCA/D were also selected and evaluated for depth of sequencing and coverage. Although the primary objective was to evaluate the adequacy of depth of sequencing and coverage of targeted SCA/D genes and not for primary diagnosis, all patients who had SCA/D (known or unknown molecular etiologies) were evaluated with the project's variant analysis pipeline to determine if the molecular etiologies could be successfully identified. Results: The majority of exons (97.6 {\%}) were captured and fully covered on average at minimum of 20x sequencing depth. The proportion of unique genomic positions reported within poorly covered exons remained small (4 {\%}). Exonic regions with less coverage reflect the need to enrich these areas to improve coverage. Despite limitations in coverage, we identified 100 {\%} of cases with a prior known molecular etiology for SCA/D, and analysis of an additional 30 individuals with SCA/D but no known molecular etiology revealed a diagnostic answer in 5/30 (17 {\%}). We also demonstrated 95 {\%} of 100 randomly selected reported variants within our targeted genes would have been picked up on ES based on our coverage analysis. Conclusions: ES is a helpful clinical diagnostic tool for SCA/D given its potential to successfully identify a molecular diagnosis, but clinicians should be aware of limitations of available platforms from technical and diagnostic perspectives.},
author = {Li, Mindy H. and Abrudan, Jenica L. and Dulik, Matthew C. and Sasson, Ariella and Brunton, Joshua and Jayaraman, Vijayakumar and Dugan, Noreen and Haley, Danielle and Rajagopalan, Ramakrishnan and Biswas, Sawona and Sarmady, Mahdi and DeChene, Elizabeth T. and Deardorff, Matthew A. and Wilkens, Alisha and Noon, Sarah E. and Scarano, Maria I. and Santani, Avni B. and White, Peter S. and Pennington, Jeffrey and Conlin, Laura K. and Spinner, Nancy B. and Krantz, Ian D. and Vetter, Victoria L.},
doi = {10.1186/s40246-015-0038-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2015 - Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sud.pdf:pdf},
issn = {1479-7364},
journal = {Human Genomics},
number = {1},
pages = {15},
pmid = {26187847},
publisher = {Human Genomics},
title = {{Utility and limitations of exome sequencing as a genetic diagnostic tool for conditions associated with pediatric sudden cardiac arrest/sudden cardiac death}},
url = {http://www.humgenomics.com/content/9/1/15/abstract{\%}5Cnhttp://www.humgenomics.com/content/9/1/15{\%}5Cnhttp://www.humgenomics.com/content/pdf/s40246-015-0038-y.pdf},
volume = {9},
year = {2015}
}
@article{Warden2014,
abstract = {The Genome Analysis Toolkit (GATK) is commonly used for variant calling of single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels) from short-read sequencing data aligned against a reference genome. There have been a number of variant calling comparisons against GATK, but an equally comprehensive comparison for VarScan not yet been performed. More specifically, we compare (1) the effects of different pre-processing steps prior to variant calling with both GATK and VarScan, (2) VarScan variants called with increasingly conservative parameters, and (3) filtered and unfiltered GATK variant calls (for both the UnifiedGenotyper and the HaplotypeCaller). Variant calling was performed on three datasets (1 targeted exon dataset and 2 exome datasets), each with approximately a dozen subjects. In most cases, pre-processing steps (e.g., indel realignment and quality score base recalibration using GATK) had only a modest impact on the variant calls, but the importance of the pre-processing steps varied between datasets and variant callers. Based upon concordance statistics presented in this study, we recommend GATK users focus on "high-quality" GATK variants by filtering out variants flagged as low-quality. We also found that running VarScan with a conservative set of parameters (referred to as "VarScan-Cons") resulted in a reproducible list of variants, with high concordance ({\textgreater}97{\%}) to high-quality variants called by the GATK UnifiedGenotyper and HaplotypeCaller. These conservative parameters result in decreased sensitivity, but the VarScan-Cons variant list could still recover 84-88{\%} of the high-quality GATK SNPs in the exome datasets. This study also provides limited evidence that VarScan-Cons has a decreased false positive rate among novel variants (relative to high-quality GATK SNPs) and that the GATK HaplotypeCaller has an increased false positive rate for indels (relative to VarScan-Cons and high-quality GATK UnifiedGenotyper indels). More broadly, we believe the metrics used for comparison in this study can be useful in assessing the quality of variant calls in the context of a specific experimental design. As an example, a limited number of variant calling comparisons are also performed on two additional variant callers.},
author = {Warden, Charles D. and Adamson, Aaron W. and Neuhausen, Susan L. and Wu, Xiwei},
doi = {10.7717/peerj.600},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Warden et al. - 2014 - Detailed comparison of two popular variant calling packages for exome and targeted exon studies.pdf:pdf},
isbn = {2167-8359 (Electronic)},
issn = {2167-8359},
journal = {PeerJ},
keywords = {exome,gatk,small indel,snp,targeted sequencing,variant calling,varscan},
pages = {e600},
pmid = {25289185},
title = {{Detailed comparison of two popular variant calling packages for exome and targeted exon studies}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4184249{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2014}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@misc{Xuan2013,
abstract = {The advent of next generation sequencing (NGS) technologies has revolutionized the field of genomics, enabling fast and cost-effective generation of genome-scale sequence data with exquisite resolution and accuracy. Over the past years, rapid technological advances led by academic institutions and companies have continued to broaden NGS applications from research to the clinic. A recent crop of discoveries have highlighted the medical impact of NGS technologies on Mendelian and complex diseases, particularly cancer. However, the ever-increasing pace of NGS adoption presents enormous challenges in terms of data processing, storage, management and interpretation as well as sequencing quality control, which hinder the translation from sequence data into clinical practice. In this review, we first summarize the technical characteristics and performance of current NGS platforms. We further highlight advances in the applications of NGS technologies towards the development of clinical diagnostics and therapeutics. Common issues in NGS workflows are also discussed to guide the selection of NGS platforms and pipelines for specific research purposes. ?? 2012.},
author = {Xuan, Jiekun and Yu, Ying and Qing, Tao and Guo, Lei and Shi, Leming},
booktitle = {Cancer Letters},
doi = {10.1016/j.canlet.2012.11.025},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xuan et al. - 2013 - Next-generation sequencing in the clinic Promises and challenges.pdf:pdf},
isbn = {1872-7980 (Electronic)$\backslash$r0304-3835 (Linking)},
issn = {03043835},
keywords = {Bioinformatics,Clinical applications,Exome sequencing,FFPE,RNA-Seq,Tumor heterogeneity,Whole-genome sequencing},
number = {2},
pages = {284--295},
pmid = {23174106},
title = {{Next-generation sequencing in the clinic: Promises and challenges}},
volume = {340},
year = {2013}
}
@article{Dudley2010,
abstract = {With the continued exponential expansion of publicly available genomic data and access to low-cost, high-throughput molecular technologies for profiling patient populations, computational technologies and informatics are becoming vital considerations in genomic medicine. Although cloud computing technology is being heralded as a key enabling technology for the future of genomic research, available case studies are limited to applications in the domain of high-throughput sequence data analysis. The goal of this study was to evaluate the computational and economic characteristics of cloud computing in performing a large-scale data integration and analysis representative of research problems in genomic medicine. We find that the cloud-based analysis compares favorably in both performance and cost in comparison to a local computational cluster, suggesting that cloud computing technologies might be a viable resource for facilitating large-scale translational research in genomic medicine.},
author = {Dudley, Joel T and Pouliot, Yannick and Chen, Rong and Morgan, Alexander A and Butte, Atul J},
doi = {10.1186/gm172},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudley et al. - 2010 - Translational bioinformatics in the cloud an affordable alternative.pdf:pdf},
isbn = {1756-994X (Electronic)},
issn = {1756-994X},
journal = {Genome medicine},
number = {8},
pages = {51},
pmid = {20691073},
title = {{Translational bioinformatics in the cloud: an affordable alternative.}},
url = {http://genomemedicine.com/content/2/8/51},
volume = {2},
year = {2010}
}
@article{Zhou2013,
abstract = {Next-generation sequencing (NGS) technologies have been widely used in life sciences. However, several kinds of sequencing artifacts, including low-quality reads and contaminating reads, were found to be quite common in raw sequencing data, which compromise downstream analysis. Therefore, quality control (QC) is essential for raw NGS data. However, although a few NGS data quality control tools are publicly available, there are two limitations: First, the processing speed could not cope with the rapid increase of large data volume. Second, with respect to removing the contaminating reads, none of them could identify contaminating sources de novo, and they rely heavily on prior information of the contaminating species, which is usually not available in advance. Here we report QC-Chain, a fast, accurate and holistic NGS data quality-control method. The tool synergeticly comprised of user-friendly tools for (1) quality assessment and trimming of raw reads using Parallel-QC, a fast read processing tool; (2) identification, quantification and filtration of unknown contamination to get high-quality clean reads. It was optimized based on parallel computation, so the processing speed is significantly higher than other QC methods. Experiments on simulated and real NGS data have shown that reads with low sequencing quality could be identified and filtered. Possible contaminating sources could be identified and quantified de novo, accurately and quickly. Comparison between raw reads and processed reads also showed that subsequent analyses (genome assembly, gene prediction, gene annotation, etc.) results based on processed reads improved significantly in completeness and accuracy. As regard to processing speed, QC-Chain achieves 7-8 time speed-up based on parallel computation as compared to traditional methods. Therefore, QC-Chain is a fast and useful quality control tool for read quality process and de novo contamination filtration of NGS reads, which could significantly facilitate downstream analysis. QC-Chain is publicly available at: http://www.computationalbioenergy.org/qc-chain.html.},
author = {Zhou, Qian and Su, Xiaoquan and Wang, Anhui and Xu, Jian and Ning, Kang},
doi = {10.1371/journal.pone.0060234},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2013 - QC-Chain Fast and Holistic Quality Control Method for Next-Generation Sequencing Data.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {23565205},
title = {{QC-Chain: Fast and Holistic Quality Control Method for Next-Generation Sequencing Data}},
volume = {8},
year = {2013}
}
@article{Wu2014,
abstract = {Exome sequencing has been widely used in detecting pathogenic nonsynonymous single nucleotide variants (SNVs) for human inherited diseases. However, traditional statistical genetics methods are ineffective in analyzing exome sequencing data, due to such facts as the large number of sequenced variants, the presence of non-negligible fraction of pathogenic rare variants or de novo mutations, and the limited size of affected and normal populations. Indeed, prevalent applications of exome sequencing have been appealing for an effective computational method for identifying causative nonsynonymous SNVs from a large number of sequenced variants. Here, we propose a bioinformatics approach called SPRING (Snv PRioritization via the INtegration of Genomic data) for identifying pathogenic nonsynonymous SNVs for a given query disease. Based on six functional effect scores calculated by existing methods (SIFT, PolyPhen2, LRT, MutationTaster, GERP and PhyloP) and five association scores derived from a variety of genomic data sources (gene ontology, protein-protein interactions, protein sequences, protein domain annotations and gene pathway annotations), SPRING calculates the statistical significance that an SNV is causative for a query disease and hence provides a means of prioritizing candidate SNVs. With a series of comprehensive validation experiments, we demonstrate that SPRING is valid for diseases whose genetic bases are either partly known or completely unknown and effective for diseases with a variety of inheritance styles. In applications of our method to real exome sequencing data sets, we show the capability of SPRING in detecting causative de novo mutations for autism, epileptic encephalopathies and intellectual disability. We further provide an online service, the standalone software and genome-wide predictions of causative SNVs for 5,080 diseases at http://bioinfo.au.tsinghua.edu.cn/spring.},
author = {Wu, Jiaxin and Li, Yanda and Jiang, Rui},
doi = {10.1371/journal.pgen.1004237},
file = {:home/jennifer/Descargas/journal.pgen.1004237.PDF:PDF},
isbn = {1553-7404 (Electronic)$\backslash$r1553-7390 (Linking)},
issn = {15537404},
journal = {PLoS Genetics},
number = {3},
pmid = {24651380},
title = {{Integrating Multiple Genomic Data to Predict Disease-Causing Nonsynonymous Single Nucleotide Variants in Exome Sequencing Studies}},
volume = {10},
year = {2014}
}
@article{Pandey2016a,
abstract = {Traditional Sanger sequencing as well as Next-Generation Sequencing have been used for the identification of disease causing mutations in human molecular research. The majority of currently available tools are developed for research and explorative purposes and often do not provide a complete, efficient, one-stop solution. As the focus of currently developed tools is mainly on NGS data analysis, no integrative solution for the analysis of Sanger data is provided and consequently a one-stop solution to analyze reads from both sequencing platforms is not available. We have therefore developed a new pipeline called MutAid to analyze and interpret raw sequencing data produced by Sanger or several NGS sequencing platforms. It performs format conversion, base calling, quality trimming, filtering, read mapping, variant calling, variant annotation and analysis of Sanger and NGS data under a single platform. It is capable of analyzing reads from multiple patients in a single run to create a list of potential disease causing base substitutions as well as insertions and deletions. MutAid has been developed for expert and non-expert users and supports four sequencing platforms including Sanger, Illumina, 454 and Ion Torrent. Furthermore, for NGS data analysis, five read mappers including BWA, TMAP, Bowtie, Bowtie2 and GSNAP and four variant callers including GATK-HaplotypeCaller, SAMTOOLS, Freebayes and VarScan2 pipelines are supported. MutAid is freely available at https://sourceforge.net/projects/mutaid.},
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1371/journal.pone.0147697},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - MutAid Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human m.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--22},
pmid = {26840129},
title = {{MutAid: Sanger and NGS based integrated pipeline for mutation identification, validation and annotation in human molecular genetics}},
volume = {11},
year = {2016}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Pabinger2014,
abstract = {Recent advances in genome sequencing technologies provide unprecedented opportunities to characterize individual genomic landscapes and identify mutations relevant for diagnosis and therapy. Specifically, whole-exome sequencing using next-generation sequencing (NGS) technologies is gaining popularity in the human genetics community due to the moderate costs, manageable data amounts and straightforward interpretation of analysis results. While whole-exome and, in the near future, whole-genome sequencing are becoming commodities, data analysis still poses significant challenges and led to the development of a plethora of tools supporting specific parts of the analysis workflow or providing a complete solution. Here, we surveyed 205 tools for whole-genome/whole-exome sequencing data analysis supporting five distinct analytical steps: quality assessment, alignment, variant identification, variant annotation and visualization. We report an overview of the functionality, features and specific requirements of the individual tools. We then selected 32 programs for variant identification, variant annotation and visualization, which were subjected to hands-on evaluation using four data sets: one set of exome data from two patients with a rare disease for testing identification of germline mutations, two cancer data sets for testing variant callers for somatic mutations, copy number variations and structural variations, and one semi-synthetic data set for testing identification of copy number variations. Our comprehensive survey and evaluation of NGS tools provides a valuable guideline for human geneticists working on Mendelian disorders, complex diseases and cancers.},
archivePrefix = {arXiv},
arxivId = {209},
author = {Pabinger, Stephan and Dander, Andreas and Fischer, Maria and Snajder, Rene and Sperk, Michael and Efremova, Mirjana and Krabichler, Birgit and Speicher, Michael R. and Zschocke, Johannes and Trajanoski, Zlatko},
doi = {10.1093/bib/bbs086},
eprint = {209},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pabinger et al. - 2014 - A survey of tools for variant analysis of next-generation genome sequencing data.pdf:pdf},
isbn = {4351290037310},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics tools,Cancer,Mendelian disorders,Next-generation sequencing,Variants},
number = {2},
pages = {256--278},
pmid = {23341494},
title = {{A survey of tools for variant analysis of next-generation genome sequencing data}},
volume = {15},
year = {2014}
}
@article{Lopez2017,
author = {Lopez, Javier and Coll, Jacobo and Haimel, Matthias and Kandasamy, Swaathi and Tarraga, Joaquin and Furio-tari, Pedro and Bari, Wasim and Bleda, Marta and Rueda, Antonio and Rendon, Augusto and Dopazo, Joaquin and Medina, Ignacio},
doi = {10.1093/nar/gkx445},
file = {:home/jennifer/Descargas/lopez2017.pdf:pdf},
pages = {1--6},
title = {{HGVA: the Human Genome Variation Archive}},
year = {2017}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells,Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,RNA,Sequence Analysis,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192{\%}7B{\&}{\%}7Dtool=pmcentrez{\%}7B{\&}{\%}7Drendertype=abstract},
volume = {9},
year = {2014}
}
@article{Bao2014,
abstract = {The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigentic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing vaiants and discover gene targets. In this review, we outline the general framework of whole exome processing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis and summarization in the field of bioinformatics.},
author = {Bao, Riyue and Huang, Lei and Andrade, Jorge and Tan, Wei and Kibbe, Warren a and Jiang, Hongmei and Feng, Gang},
doi = {10.4137/CIN.S13779.Received},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bao et al. - 2014 - Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequenci.pdf:pdf},
isbn = {1176-9351 (Electronic)$\backslash$r1176-9351 (Linking)},
issn = {1176-9351},
journal = {Libertas Academica},
keywords = {10,13,4137,67,82 doi,a,and data management for,and statistical analysis of,applications,bao et al,big data,cancer data,cancer informatics 2014,cin,citation,classification,indel,next generation sequencing,predictive modelling,review of current methods,s13779,s2,sequence alignment,snv,supplement,the bioinformatics analysis of,variant analysis,whole exome sequencing},
pages = {67--82},
pmid = {25288881},
title = {{Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing}},
volume = {13},
year = {2014}
}
@article{Lescai2014,
abstract = {The choice of an appropriate variant calling pipeline for exome sequencing data is becoming increasingly more important in translational medicine projects and clinical contexts. Within GOSgene, which facilitates genetic analysis as part of a joint effort of the University College London and the Great Ormond Street Hospital, we aimed to optimize a variant calling pipeline suitable for our clinical context. We implemented the GATK/Queue framework and evaluated the performance of its two callers: the classical UnifiedGenotyper and the new variant discovery tool HaplotypeCaller. We performed an experimental validation of the loss-of-function (LoF) variants called by the two methods using Sequenom technology. UnifiedGenotyper showed a total validation rate of 97.6{\%} for LoF single-nucleotide polymorphisms (SNPs) and 92.0{\%} for insertions or deletions (INDELs), whereas HaplotypeCaller was 91.7{\%} for SNPs and 55.9{\%} for INDELs. We confirm that GATK/Queue is a reliable pipeline in translational medicine and clinical context. We conclude that in our working environment, UnifiedGenotyper is the caller of choice, being an accurate method, with a high validation rate of error-prone calls like LoF variants. We finally highlight the importance of experimental validation, especially for INDELs, as part of a standard pipeline in clinical environments.},
author = {Lescai, Francesco and Marasco, Elena and Bacchelli, Chiara and Stanier, Philip and Mantovani, Vilma and Beales, Philip},
doi = {10.1002/mgg3.42},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lescai et al. - 2014 - Identification and validation of loss of function variants in clinical contexts.pdf:pdf},
issn = {2324-9269},
journal = {Molecular genetics {\&} genomic medicine},
keywords = {gatk,pipelines,sequencing,variant calling},
number = {1},
pages = {58--63},
pmid = {24498629},
title = {{Identification and validation of loss of function variants in clinical contexts.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3907911{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2},
year = {2014}
}
@article{McKenna2009,
abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS—the 1000 Genome pilot alone includes nearly five terabases—make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management in- frastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide poly- morphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas},
author = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and Depristo, Mark A},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKenna et al. - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Cold Spring Harbor Laboratory Press Resource},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data}},
volume = {20},
year = {2009}
}
@article{Li2009,
abstract = {MOTIVATION: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.$\backslash$n$\backslash$nRESULTS: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is approximately 10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.$\backslash$n$\backslash$nAVAILABILITY: http://maq.sourceforge.net.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Williams2011,
abstract = {Editorial Biological data mining is playing an increasingly important role throughout the spectrum of biological and biomedical research with broad implications for the understanding of life science questions such as the tree of life and practical applica-tions of such knowledge to improving human health. Perhaps nowhere is data mining needed more than the emerging discipline of precision medicine. The ability to predict individual risk of presenting with a disease or response to treatment is at the core of the concept of precision medicine, which is gaining ever-increasing levels of traction in the era of technology-driven measurement of biological systems. This has become especially important with the new Presidential initiative on precision medi-cine in the United States [1]. It is obvious to the readers of BioData Mining that this will require careful analyses of large and often complex data sets to best translate information into increasingly individualized risk. Here we ask why improved and appropriate data mining is not only positive but a vast improvement on most current analyses of genomic data. The answer lies to some extent in elucidating the present practice of -omic analyses and how we will need to expand it. Many current -omic approaches rely on univariate and linear analyses that can often miss the underlying architecture of complex traits. For example, univariate analyses of single genetic markers for association with disease risk, prognosis, or drug response that are the analytical standards for genetic analyses of human disease, and have been promoted as a means to develop personalized or more recently precision medicine, make many assumptions about architecture. Given the interest in precision medicine, it is important to ask explicitly what is being assayed in these types of studies that have been argued, incorrectly we believe, as the precursors to precision medicine. Most human geneticists study the association of genetic variants, be they common or rare, assessed across moderate to large samples of cases and controls. The effect of each allelic substitution is then measured as it associates with a particular phenotype. These estimates can provide useful population level risks; however, they are simply the average effect of an allelic substitution across the population, not necessarily predictive of results in an individual or a subgroup. The concept of average allelic effect is one that is well developed in quantitative genetics, but by its very name is suggestive not of precision medicine but of average medicine. Hence, it is possible in a large outbreeding},
author = {Williams, Scott M and Moore, Jason H},
doi = {10.1186/s13040-015-0049-1},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Moore - 2011 - Lumping versus splitting the need for biological data mining in precision medicine.pdf:pdf},
title = {{Lumping versus splitting: the need for biological data mining in precision medicine}},
volume = {8},
year = {2011}
}
@article{Liu2014,
abstract = {Discriminative pattern mining is one of the most important techniques in data mining. This challenging task is concerned with finding a set of patterns that occur with disproportionate frequency in data sets with various class labels. Such patterns are of great value for group difference detection and classifier construction. Research on finding interesting discriminative patterns in class-labeled data evolves rapidly and lots of algorithms have been proposed to specifically address this problem. Discriminative pattern mining techniques have proven their considerable value in biological data analysis. The archetypical applications in bioinformatics include phosphorylation motif discovery, differentially expressed gene identification, discriminative genotype pattern detection, etc. In this article, we present an overview of discriminative pattern mining and the corresponding effective methods, and subsequently we illustrate their applications to tackling the bioinformatics problems. In the end, we give a general discussion of potential challenges and future work for this task.},
author = {Liu, Xiaoqing and Wu, Jun and Gu, Feiyang and Wang, Jie and He, Zengyou},
doi = {10.1093/bib/bbu042},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Discriminative pattern mining and its applications in bioinformatics.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {contrast sets,discriminative pattern mining,emerging patterns,subgroup discovery},
number = {October},
pages = {1--17},
title = {{Discriminative pattern mining and its applications in bioinformatics}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu042},
year = {2014}
}
@article{Mckenna2010,
abstract = {The concept of absorptive capacity was introduced by Cohen and Levinthal in 1989. Since then it has been enhanced through reconceptualizations and extended by various empirical studies. Despite the growing interest in absorptive capacity it is unclear what this large stream of papers has collectively accomplished. The used definitions, antecedents, components and outcomes of the construct are extremely heterogeneous. Due to this heterogeneity, the empirical study of the construct remains difficult. There is no standard measure and no standard method of measurement, which can be used in empirical research. To bring more clarity into this research area, this paper provides a critical review of previous empirical treatments of absorptive capacity. For this purpose, different methods of measurement are classified in the following way: within quantitative methods proxy indicators and perceptive instruments are differentiated. Proxy indicators use single firm-level data for measuring absorptive capacity and can be input-oriented (R{\&}D efforts, R{\&}D human capital) or output-oriented (R{\&}D patents, R{\&}D publications). Perceptive instruments imply that researchers develop single questions or a set of questions, which reflect absorptive capacity or parts of it at the operational level. The main weakness of both proxy indicators and perceptive instruments is that they don't meet the complexity and emergence of the construct. Only few qualitative studies have started to adopt a new perspective, recognizing the process and practice-based character of absorptive capacity. In summary, the critical review prints out the necessity of advancing research in this area. For this reason, we set out to develop an alternative approach to capture absorptive capacity. It is a practice-oriented approach that allows studying actual absorptive practices in real world situations and enables researchers to capture the complex, embedded, and context-dependent patterns of acting.},
author = {Schmidt, Stephanie},
doi = {10.1101/gr.107524.110.20},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt - 2009 - The Genome Analysis Toolkit A MapReduce framework for analyzing next-generation DNA sequencing data.pdf:pdf},
journal = {Proceedings of the International Conference on Intellectual Capital, Knowledge Management {\&} Organizational Learning},
keywords = {Absorptive capacity,capabilities,measuring,practices,routines},
pages = {254--260},
title = {{The Genome Analysis Toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data.}},
volume = {20},
year = {2009}
}
@article{Mohammed2014,
abstract = {The emergence of massive datasets in a clinical setting presents both challenges and opportunities in data storage and analysis. This so called “big data” challenges traditional analytic tools and will increasingly require novel solutions adapted from other fields. Advances in information and communication technology present the most viable solutions to big data analysis in terms of efficiency and scalability. It is vital those big data solutions are multithreaded and that data access approaches be precisely tailored to large volumes of semi-structured/unstructured data. The MapReduce programming framework uses two tasks common in functional programming: Map and Reduce. MapReduce is a new parallel processing framework and Hadoop is its open-source implementation on a single computing node or on clusters. Compared with existing parallel processing paradigms (e.g. grid computing and graphical processing unit (GPU)), MapReduce and Hadoop have two advantages: 1) fault-tolerant storage resulting in reliable data processing by replicating the computing tasks, and cloning the data chunks on different computing nodes across the computing cluster; 2) high-throughput data processing via a batch processing framework and the Hadoop distributed file system (HDFS). Data are stored in the HDFS and made available to the slave nodes for computation. In this paper, we review the existing applications of the MapReduce programming framework and its implementation platform Hadoop in clinical big data and related medical health informatics fields. The usage of MapReduce and Hadoop on a distributed system represents a significant advance in clinical big data processing and utilization, and opens up new opportunities in the emerging era of big data analytics. The objective of this paper is to summarize the state-of-the-art efforts in clinical big data analytics and highlight what might be needed to enhance the outcomes of clinical big data analytics tools. This paper is concluded by summarizing the potential usage of the MapReduce programming framework and Hadoop platform to process huge volumes of clinical data in medical health informatics related fields.},
author = {Mohammed, Emad a. and Far, Behrouz H. and Naugler, Christopher},
doi = {10.1186/1756-0381-7-22},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammed, Far, Naugler - 2014 - Applications of the MapReduce programming framework to clinical big data analysis current landscape and.pdf:pdf},
isbn = {1756-0381 (Linking)},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {Big data,Bioinformatics,Clinical big data analysis,Clinical data analysis,Distributed programming,Hadoop,MapReduce},
number = {1},
pages = {1--23},
pmid = {25383096},
title = {{Applications of the MapReduce programming framework to clinical big data analysis: current landscape and future trends}},
url = {http://link.springer.com/article/10.1186/1756-0381-7-22{\%}5Cnhttp://link.springer.com/content/pdf/10.1186/1756-0381-7-22.pdf},
volume = {7},
year = {2014}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@misc{FBI,
author = {FBI},
pages = {https://www.fbi.gov/services/laboratory/biometric--},
title = {{Combined DNA Index System (CODIS)}}
}
@article{Field2015,
abstract = {A diversity of tools is available for identification of variants from genome sequence data. Given the current complexity of incorporating external software into a genome analysis infrastructure, a tendency exists to rely on the results from a single tool alone. The quality of the output variant calls is highly variable however, depending on factors such as sequence library quality as well as the choice of short-read aligner, variant caller, and variant caller filtering strategy. Here we present a two-part study first using the high quality 'genome in a bottle' reference set to demonstrate the significant impact the choice of aligner, variant caller, and variant caller filtering strategy has on overall variant call quality and further how certain variant callers outperform others with increased sample contamination, an important consideration when analyzing sequenced cancer samples. This analysis confirms previous work showing that combining variant calls of multiple tools results in the best quality resultant variant set, for either specificity or sensitivity, depending on whether the intersection or union, of all variant calls is used respectively. Second, we analyze a melanoma cell line derived from a control lymphocyte sample to determine whether software choices affect the detection of clinically important melanoma risk-factor variants finding that only one of the three such variants is unanimously detected under all conditions. Finally, we describe a cogent strategy for implementing a clinical variant detection pipeline; a strategy that requires careful software selection, variant caller filtering optimizing, and combined variant calls in order to effectively minimize false negative variants. While implementing such features represents an increase in complexity and computation the results offer indisputable improvements in data quality.},
author = {Field, Matthew A. and Cho, Vicky and Andrews, T. Daniel and Goodnow, Chris C.},
doi = {10.1371/journal.pone.0143199},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Field et al. - 2015 - Reliably detecting clinically important variants requires both combined variant calls and optimized filtering stra.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--19},
title = {{Reliably detecting clinically important variants requires both combined variant calls and optimized filtering strategies}},
volume = {10},
year = {2015}
}
@article{DanecekPAutonA2011,
abstract = {SUMMARY: The Variant Call Format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing, and also provides a general Perl API. AVAILABILITY: http://vcftools.sourceforge.net CONTACT: rd@sanger.ac.uk.},
author = {Danecek, P and Auton, A and Abecasis, G and Albers, C and Banks, E and Depristo, M and Handsaker, R and Lunter, G and Marth, G and Sherry, S and McVean, G and Durbin, R},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Danecek et al. - 2011 - The Variant Call Format and VCFtools.pdf:pdf},
journal = {Bioinformatics},
pages = {3--5},
title = {{The Variant Call Format and VCFtools}},
url = {http://bioinformatics.oxfordjournals.org/content/early/2011/06/07/bioinformatics.btr330.long},
year = {2011}
}
@article{Fisch2015a,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics{\_}pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Harold2016,
abstract = {Motivation: Integrating heterogeneous datasets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access, data filtering, format conversions, identifier mapping, among further diverse operations. Data integration is especially important when annotating next generation sequencing data, where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic locations, such a single nucleotide variants or genes. Each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose. However, software that always produces all available data is difficult to maintain and quickly leads to an excess of data, creating an information overload rather than the desired goal-oriented and integrated result. Results: We present SoFIA, a framework for workflow-driven data integration with a focus on genomic annotation. SoFIA conceptualizes workflow templates as comprehensive workflows that cover as many data integration operations as possible in a given domain. However, these templates are not intended to be executed as a whole; instead, when given an integration task consisting of a set of input data and a set of desired output data, SoFIA derives a minimal workflow that completes the task. These workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work. Using a comprehensive genome annotation template, we highlight the flexibility, extensibility and power of the framework using real-life case studies},
author = {Harold, Liam and Mamlouk, Soulafa and Brandt, J{\"{o}}rgen and Sers, Christine and Lesser, Ulf},
file = {:home/jennifer/Descargas/childs2016.pdf:pdf},
journal = {Bioinformatics (Oxford, England)},
number = {17},
pages = {2590--2597},
title = {{SoFIA a data integration framework for annotating high-throughput datasets}},
volume = {32},
year = {2016}
}
@article{Staccini2014,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and {Vanden Berghe}, Wim and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - 2013 - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
isbn = {1477-4054 (Electronic)},
issn = {1477-4054},
journal = {Briefings in bioinformatics},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
number = {2},
pages = {3066--3076},
pmid = {24162173},
title = {{A primer to frequent itemset mining for bioinformatics.}},
url = {http://link.springer.com/10.1007/978-2-8178-0478-1{\_}13{\%}5Cnhttp://link.springer.com/chapter/10.1007/978-2-8178-0478-1{\_}13 http://linkinghub.elsevier.com/retrieve/pii/S0957417408000195{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/24162173},
volume = {36},
year = {2013}
}
@book{Auwera2014,
author = {Auwera, Geraldine A Van Der and Carneiro, Mauricio O and Hartl, Chris and Poplin, Ryan and Levy-moonshine, Ami and Jordan, Tadeusz and Shakir, Khalid and Roazen, David and Thibault, Joel and Banks, Eric and Garimella, Kiran V and Altshuler, David and Gabriel, Stacey and Depristo, Mark A},
booktitle = {Curr Protoc Bioinformatics},
doi = {10.1002/0471250953.bi1110s43.From},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Auwera et al. - 2014 - From FastQ data to high confidence varant calls the Genonme Analysis Toolkit best practices pipeline.pdf:pdf},
isbn = {0471250953},
keywords = {exome,genotyping,ngs,variant detection,wgs},
number = {1110},
title = {{From FastQ data to high confidence varant calls: the Genonme Analysis Toolkit best practices pipeline}},
volume = {11},
year = {2014}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.{\$}\backslash{\$}n{\$}\backslash{\$}nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{\{}{\%}{\}} (87.0{\{}{\%}{\}}) and average genotype concordance in correctly identified SNVs was 99.2{\{}{\%}{\}} (99.2{\{}{\%}{\}}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.{\$}\backslash{\$}n{\$}\backslash{\$}nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,DNA,DNA: methods,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma.png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma}}
}
@article{Wang2010,
abstract = {High-throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes, but it remains a challenge to pinpoint a small subset of functionally important variants. To fill these unmet needs, we developed the ANNOVAR tool to annotate single nucleotide variants (SNVs) and insertions/deletions, such as examining their functional consequence on genes, inferring cytogenetic bands, reporting functional importance scores, finding variants in conserved regions, or identifying variants reported in the 1000 Genomes Project and dbSNP. ANNOVAR can utilize annotation databases from the UCSC Genome Browser or any annotation data set conforming to Generic Feature Format version 3 (GFF3). We also illustrate a 'variants reduction' protocol on 4.7 million SNVs and indels from a human genome, including two causal mutations for Miller syndrome, a rare recessive disease. Through a stepwise procedure, we excluded variants that are unlikely to be causal, and identified 20 candidate genes including the causal gene. Using a desktop computer, ANNOVAR requires ∼4 min to perform gene-based annotation and ∼15 min to perform variants reduction on 4.7 million variants, making it practical to handle hundreds of human genomes in a day. ANNOVAR is freely available at http://www.openbioinformatics.org/annovar/.},
author = {Wang, Kai and Li, Mingyao and Hakonarson, Hakon},
doi = {10.1093/nar/gkq603},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Li, Hakonarson - 2010 - ANNOVAR functional annotation of genetic variants from high-throughput sequencing data.pdf:pdf},
isbn = {0305-1048},
issn = {13624962},
journal = {Nucleic acids research},
number = {16},
pages = {e164},
pmid = {20601685},
title = {{ANNOVAR: functional annotation of genetic variants from high-throughput sequencing data}},
volume = {38},
year = {2010}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{La2012,
abstract = {Sirve como ejercicio para que no se te olvide como hacer esto},
author = {La, P},
doi = {10.5867/medwave.2003.11.2757},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/La - 2012 - Ejercicio 1.pdf:pdf},
isbn = {8707955499},
issn = {07176384},
pages = {10--12},
title = {{Ejercicio 1}},
year = {2012}
}
@article{Tsai2016,
abstract = {Effective implementation of precision medicine will be enhanced by a thorough understanding of each patient's genetic composition to better treat his or her presenting symptoms or mitigate the onset of disease. This ideally includes the sequence information of a complete genome for each individual. At Partners HealthCare Personalized Medicine, we have developed a clinical process for whole genome sequencing (WGS) with application in both healthy individuals and those with disease. In this manuscript, we will describe our bioinformatics strategy to efficiently process and deliver genomic data to geneticists for clinical interpretation. We describe the handling of data from FASTQ to the final variant list for clinical review for the final report. We will also discuss our methodology for validating this workflow and the cost implications of running WGS.},
author = {Tsai, Ellen and Shakbatyan, Rimma and Evans, Jason and Rossetti, Peter and Graham, Chet and Sharma, Himanshu and Lin, Chiao-Feng and Lebo, Matthew},
doi = {10.3390/jpm6010012},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai et al. - 2016 - Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine.pdf:pdf},
issn = {2075-4426},
journal = {Journal of Personalized Medicine},
keywords = {NGS,WGS,bioinformatics,clinical sequencing,next generation sequencing,precision medicine,validation},
number = {1},
pages = {12},
pmid = {26927186},
title = {{Bioinformatics Workflow for Clinical Whole Genome Sequencing at Partners HealthCare Personalized Medicine}},
url = {http://www.mdpi.com/2075-4426/6/1/12/htm http://www.mdpi.com/2075-4426/6/1/12},
volume = {6},
year = {2016}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.{\$}\backslash{\$}n{\$}\backslash{\$}nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.{\$}\backslash{\$}n{\$}\backslash{\$}nDATA SOURCES: Literature review and authors' experience.{\$}\backslash{\$}n{\$}\backslash{\$}nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M and Durtschi, Jacob D and Margraf, Rebecca L and Voelkerding, Karl V},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic){\$}\backslash{\$}r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@article{Naulaerts,
abstract = {Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solu-tions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address vari-ations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinfor-matics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future poten-tial and open challenges for frequent itemset mining in the life sciences.},
author = {Naulaerts, Stefan and Meysman, Pieter and Bittremieux, Wout and Vu, Trung Nghia and Berghe, Wim Vanden and Goethals, Bart and Laukens, Kris},
doi = {10.1093/bib/bbt074},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Naulaerts et al. - Unknown - A primer to frequent itemset mining for bioinformatics.pdf:pdf},
keywords = {association rule,biclustering,frequent item set,market basket analysis,pattern mining},
title = {{A primer to frequent itemset mining for bioinformatics}}
}
@article{Kashyap2015,
abstract = {Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.},
archivePrefix = {arXiv},
arxivId = {1506.05101},
author = {Kashyap, Hirak and Ahmed, Hasin Afzal and Hoque, Nazrul and Roy, Swarup and Bhattacharyya, Dhruba Kumar},
eprint = {1506.05101},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kashyap et al. - 2014 - Big Data Analytics in Bioinformatics A Machine Learning Perspective.pdf:pdf},
journal = {Journal of Latex Class Files},
number = {9},
pages = {1--20},
title = {{Big Data Analytics in Bioinformatics: A Machine Learning Perspective}},
url = {http://arxiv.org/abs/1506.05101},
volume = {13},
year = {2014}
}
@article{Seren2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4788},
author = {Seren, {\"{U}}mit and Grimm, Dominik and Fitz, Joffrey and Weigel, Detlef and Nordborg, Magnus and Borgwardt, Karsten and Korte, Arthur},
doi = {10.1093/nar/gkw986},
eprint = {arXiv:1212.4788},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Seren et al. - 2016 - AraPheno a public database for Arabidopsis thaliana phenotypes.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {October 2016},
pages = {gkw986},
pmid = {27924043},
title = {{AraPheno: a public database for Arabidopsis thaliana phenotypes}},
url = {http://nar.oxfordjournals.org/lookup/doi/10.1093/nar/gkw986},
volume = {45},
year = {2016}
}
@misc{Paez2012,
abstract = {El contexto din{\'{a}}mico y competitivo de la organizaci{\'{o}}n actual exige permanentes soluciones inform{\'{a}}ticas que apoyen efectivamente sus estrategias y objetivos. Las bodegas de datos- Datawarehouse, han incursionado en el mercado como una soluci{\'{o}}n innovadora al problema del manejo de datos enmarcando dicha soluci{\'{o}}n mayormente, desde el punto de vista tecnol{\'{o}}gico, sin considerar los aspectos organizacionales y metodol{\'{o}}gicos involucrados.},
author = {de P{\'{a}}ez, Raquel Anaya},
booktitle = {Revista Universidad EAFIT},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{a}}ez - 2012 - Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio.pdf:pdf},
issn = {0120-341X},
keywords = {Sistemas de informaci{\'{o}}n en administraci{\'{o}}n,Trabajo intelectual. Universidad EAFIT},
number = {104},
pages = {93--101},
title = {{Las bodegas de datos como apoyo a los sistemas de informaci{\'{o}}n acerca del negocio}},
url = {http://publicaciones.eafit.edu.co/index.php/revista-universidad-eafit/article/view/1176},
volume = {32},
year = {2012}
}
@article{Brueffer2015,
abstract = {Summary: TopHat is a popular spliced junction mapper for RNA sequencing data, and writes files in the BAM format - the binary version of the Sequence Alignment/Map (SAM) format. BAM is the standard exchange format for aligned sequencing reads, thus correct format implementation is paramount for software interoperability and correct analysis. However, TopHat writes its unmapped reads in a way that is not compatible with other software that implements the SAM/BAM format. We have developed TopHat-Recondition, a post-processor for TopHat unmapped reads that restores read information in the proper format. TopHat-Recondition thus enables downstream software to process the plethora of BAM files written by TopHat. Availability and implementation: TopHat-Recondition is implemented in Python using the Pysam library and is freely available under a 2-clause BSD license on GitHub: https://github.com/cbrueffer/tophat-recondition. Contact: christian.brueffer@med.lu.se, lao.saal@med.lu.se},
author = {Brueffer, Christian and Saal, Lao H},
doi = {10.1101/033530},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brueffer, Saal - 2015 - TopHat-Recondition A post-processor for TopHat unmapped reads.pdf:pdf},
issn = {1471-2105},
journal = {bioRxiv},
keywords = {deep sequencing,rna-seq,sequence alignment,sequence analysis},
pages = {1--6},
pmid = {27142976},
publisher = {BMC Bioinformatics},
title = {{TopHat-Recondition: A post-processor for TopHat unmapped reads}},
url = {http://dx.doi.org/10.1186/s12859-016-1058-x},
volume = {2},
year = {2015}
}
@misc{Babraham2016,
author = {{Babraham Bioinformatics}},
booktitle = {www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules},
title = {{FASTQC manual}},
url = {http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/Help/3 Analysis Modules/},
urldate = {2016-06-25},
year = {2016}
}
@article{Cornish2015,
abstract = {High-throughput sequencing, especially of exomes, is a popular diagnostic tool, but it is difficult to determine which tools are the best at analyzing this data. In this study, we use the NIST Genome in a Bottle results as a novel resource for validation of our exome analysis pipeline. We use six different aligners and five different variant callers to determine which pipeline, of the 30 total, performs the best on a human exome that was used to help generate the list of variants detected by the Genome in a Bottle Consortium. Of these 30 pipelines, we found that Novoalign in conjunction with GATK UnifiedGenotyper exhibited the highest sensitivity while maintaining a low number of false positives for SNVs. However, it is apparent that indels are still difficult for any pipeline to handle with none of the tools achieving an average sensitivity higher than 33{\%} or a Positive Predictive Value (PPV) higher than 53{\%}. Lastly, as expected, it was found that aligners can play as vital a role in variant detection as variant callers themselves},
author = {Cornish, Adam and Guda, Chittibabu},
doi = {10.1155/2015/456479},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cornish, Guda - 2015 - A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference.pdf:pdf},
issn = {23146141},
journal = {BioMed Research International},
number = {BioMed Research International},
pages = {11},
pmid = {26539496},
title = {{A Comparison of Variant Calling Pipelines Using Genome in a Bottle as a Reference}},
volume = {2015},
year = {2015}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (2).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (2)}}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics{\{}{\_}{\}}pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M and Loguercio, Salvatore and Su, Andrew I},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis(2).pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
pages = {51--68},
title = {{Salud P{\{}{\'{u}}{\}}blica • Epidemiolog{\{}{\'{i}}{\}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{Li2009,
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Durbin - 2009 - Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
journal = {Bioinformatics},
number = {14},
pages = {1754--1760},
title = {{Fast and accurate short read alignment with Burrows–Wheeler transform}},
volume = {25},
year = {2009}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
isbn = {1367-4803$\backslash$r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Ghoneim2014,
abstract = {BACKGROUND: Insertions/deletions (indels) are the second most common type of genomic variant and the most common type of structural variant. Identification of indels in next generation sequencing data is a challenge, and algorithms commonly used for indel detection have not been compared on a research cohort of human subject genomic data. Guidelines for the optimal detection of biologically significant indels are limited. We analyzed three sets of human next generation sequencing data (48 samples of a 200 gene target exon sequencing, 45 samples of whole exome sequencing, and 2 samples of whole genome sequencing) using three algorithms for indel detection (Pindel, Genome Analysis Tool Kit's UnifiedGenotyper and HaplotypeCaller). RESULTS: We observed variation in indel calls across the three algorithms. The intersection of the three tools comprised only 5.70{\%} of targeted exon, 19.52{\%} of whole exome, and 14.25{\%} of whole genome indel calls. The majority of the discordant indels were of lower read depth and likely to be false positives. When software parameters were kept consistent across the three targets, HaplotypeCaller produced the most reliable results. Pindel results did not validate well without adjustments to parameters to account for varied read depth and number of samples per run. Adjustments to Pindel's M (minimum support for event) parameter improved both concordance and validation rates. Pindel was able to identify large deletions that surpassed the length capabilities of the GATK algorithms. CONCLUSIONS: Despite the observed variability in indel identification, we discerned strengths among the individual algorithms on specific data sets. This allowed us to suggest best practices for indel calling. Pindel's low validation rate of indel calls made in targeted exon sequencing suggests that HaplotypeCaller is better suited for short indels and multi-sample runs in targets with very high read depth. Pindel allows for optimization of minimum support for events and is best used for detection of larger indels at lower read depths.},
author = {Ghoneim, Dalia H and Myers, Jason R and Tuttle, Emily and Paciorkowski, Alex R},
doi = {10.1186/1756-0500-7-864},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghoneim et al. - 2014 - Comparison of insertiondeletion calling algorithms on human next-generation sequencing data.pdf:pdf},
isbn = {1756-0500 (Electronic)$\backslash$r1756-0500 (Linking)},
issn = {1756-0500},
journal = {BMC research notes},
keywords = {concordance,gatk,indels,next generation sequencing,pindel,validation},
number = {1},
pages = {864},
pmid = {25435282},
title = {{Comparison of insertion/deletion calling algorithms on human next-generation sequencing data.}},
url = {http://www.biomedcentral.com/1756-0500/7/864},
volume = {7},
year = {2014}
}
@article{Cock2009,
abstract = {FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.},
author = {Cock, Peter J A and Fields, Christopher J. and Goto, Naohisa and Heuer, Michael L. and Rice, Peter M.},
doi = {10.1093/nar/gkp1137},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cock et al. - 2009 - The Sanger FASTQ file format for sequences with quality scores, and the SolexaIllumina FASTQ variants.pdf:pdf},
isbn = {1362-4962 (Electronic)$\backslash$r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
number = {6},
pages = {1767--1771},
pmid = {20015970},
title = {{The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants}},
volume = {38},
year = {2009}
}
@article{Lauzon2016,
author = {Lauzon, David and Kanzki, Beatriz and Dupuy, Victor and April, Alain and Phillips, Michael S. and Tremblay, Johanne and Hamet, Pavel},
doi = {10.1109/CHASE.2016.79},
file = {:home/jennifer/Descargas/lauzon2016.pdf:pdf},
isbn = {9781509009435},
journal = {Proceedings - 2016 IEEE 1st International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2016},
keywords = {Big Data,GWAS health systems provenance,GWAS visualization,dbSNP discrepancies,open-source},
pages = {382--387},
title = {{Addressing Provenance Issues in Big Data Genome Wide Association Studies (GWAS)}},
year = {2016}
}
@misc{Watson1953,
abstract = {A structure for nucleic acid has already been proposed by Pauling and Corey 1 . They kindly made their manuscript available to us in advance of publication. Their model consists of three intertwined chains, with the phosphates near},
author = {Watson, James D and Crick, Francis H C},
booktitle = {Nature},
doi = {10.1097/BLO.0b013e3181468780},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Crick - 1953 - Molecular structure of nucleic acids.pdf:pdf},
isbn = {0226284158},
issn = {0028-0836},
keywords = {nucleic acids},
number = {4356},
pages = {737--738},
pmid = {1943},
title = {{Molecular structure of nucleic acids}},
url = {http://www.nature.com/physics/looking-back/crick/{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/13054692},
volume = {171},
year = {1953}
}
@article{Handl2005,
author = {Handl, J. and Knowles, J. and Kell, D. B.},
doi = {10.1093/bioinformatics/bti517},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Handl, Knowles, Kell - 2005 - Computational cluster validation in post-genomic data analysis.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {3201--3212},
title = {{Computational cluster validation in post-genomic data analysis}},
url = {http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bti517},
volume = {21},
year = {2005}
}
@article{Wang2014,
abstract = {Motivation: The transition/transversion (Ti/Tv) ratio and heterozygous/nonreference-homozygous (het/nonref-hom) ratio have been commonly computed in genetic studies as a quality control (QC) measurement. Additionally, these two ratios are helpful in our understanding of the patterns of DNA sequence evolution.Results: To thoroughly understand these two genomic measures, we performed a study using 1000 Genomes Project (1000G) released genotype data (N = 1092). An additional two datasets (N = 581 and N = 6) were used to validate our findings from the 1000G dataset. We compared the two ratios among continental ancestry, genome regions and gene functionality. We found that the Ti/Tv ratio can be used as a quality indicator for single nucleotide polymorphisms inferred from high-throughput sequencing data. The Ti/Tv ratio varies greatly by genome region and functionality, but not by ancestry. The het/nonref-hom ratio varies greatly by ancestry, but not by genome regions and functionality. Furthermore, extreme guanine + cytosine content (either high or low) is negatively associated with the Ti/Tv ratio magnitude. Thus, when performing QC assessment using these two measures, care must be taken to apply the correct thresholds based on ancestry and genome region. Failure to take these considerations into account at the QC stage will bias any following analysis.Contact: yan.guo@vanderbilt.eduSupplementary information: Supplementary data are available at Bioinformatics online. },
author = {Wang, Jing and Raskin, Leon and Samuels, David C. and Shyr, Yu and Guo, Yan},
doi = {10.1093/bioinformatics/btu668},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Genome measures used for quality control are dependent on gene function and ancestry.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {3},
pages = {318--323},
pmid = {25297068},
title = {{Genome measures used for quality control are dependent on gene function and ancestry}},
volume = {31},
year = {2014}
}
@article{Medina2016,
abstract = {As sequencing technologies progress, the amount of data produced grows exponentially, shifting the bottleneck of discovery towards the data analysis phase. In particular, currently available mapping solutions for RNA-seq leave room for improvement in terms of sensitivity and performance, hindering an efficient analysis of transcriptomes by massive sequencing. Here, we present an innovative approach that combines re-engineering, optimization and parallelization. This solution results in a significant increase of mapping sensitivity over a wide range of read lengths and substantial shorter runtimes when compared with current RNA-seq mapping methods available.},
author = {Medina, I and T{\'{a}}rraga, J and Mart{\'{i}}nez, H and Barrachina, S and Castillo, M I and Paschall, J and Salavert-Torres, J and Blanquer-Espert, I and Hern{\'{a}}ndez-Garc{\'{i}}a, V and Quintana-Ort{\'{i}}, E S and Dopazo, J},
doi = {10.1093/dnares/dsv039},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Medina et al. - 2016 - Highly sensitive and ultrafast read mapping for RNA-seq analysis.pdf:pdf},
issn = {1756-1663},
journal = {DNA research : an international journal for rapid publication of reports on genes and genomes},
keywords = {burrows-wheeler transform,high-performance computing,mapping,rna-seq},
number = {January},
pages = {dsv039},
pmid = {26740642},
title = {{Highly sensitive and ultrafast read mapping for RNA-seq analysis.}},
url = {http://dnaresearch.oxfordjournals.org/content/early/2016/01/05/dnares.dsv039.full},
volume = {23},
year = {2016}
}
@misc{,
annote = {NULL},
booktitle = {La Patria},
pages = {http://www.lapatria.com/nacional/adn--clave--para--id},
title = {{ADN, clave para identificar 311 cuerpos}},
url = {http://www.lapatria.com/nacional/adn-clave-para-id},
urldate = {2016-11-20}
}
@article{Bamshad2011,
abstract = {Nature Reviews Genetics 12, 745 (2011). doi:10.1038/nrg3031},
archivePrefix = {arXiv},
arxivId = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
author = {Bamshad, Michael J and Ng, Sarah B and Bigham, Abigail W and Tabor, Holly K and Emond, Mary J and Nickerson, Deborah A and Shendure, Jay},
doi = {10.1038/nrg3031},
eprint = {/www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3006164{\&}tool=pmcentrez{\&}rendertype=abstract.},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bamshad et al. - 2011 - Exome sequencing as a tool for Mendelian disease gene discovery.pdf:pdf},
isbn = {1471-0064 (Electronic)$\backslash$r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature Publishing Group},
keywords = {Alleles,Base Sequence,Exome,Exome: genetics,Genetic Predisposition to Disease,Genome, Human,Genome-Wide Association Study,Humans,Molecular Sequence Data,Pedigree,Phenotype,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
number = {11},
pages = {745--755},
pmid = {21946919},
primaryClass = {Figures, S., 2010. Supplementary information. Nature, 1(c), pp.1–7. Available at: http:},
publisher = {Nature Publishing Group},
title = {{Exome sequencing as a tool for Mendelian disease gene discovery}},
url = {http://dx.doi.org/10.1038/nrg3031{\%}5Cnpapers2://publication/doi/10.1038/nrg3031},
volume = {12},
year = {2011}
}
@article{Cartwright2012,
abstract = {Recent advances in high-throughput DNA sequencing technologies and associated statistical analyses have enabled in-depth analysis of whole-genome sequences. As this technology is applied to a growing number of individual human genomes, entire families are now being sequenced. Information contained within the pedigree of a sequenced family can be leveraged when inferring the donors' genotypes. The presence of a de novo mutation within the pedigree is indicated by a violation of Mendelian inheritance laws. Here, we present a method for probabilistically inferring genotypes across a pedigree using high-throughput sequencing data and producing the posterior probability of de novo mutation at each genomic site examined. This framework can be used to disentangle the effects of germline and somatic mutational processes and to simultaneously estimate the effect of sequencing error and the initial genetic variation in the population from which the founders of the pedigree arise. This approach is examined in detail through simulations and areas for method improvement are noted. By applying this method to data from members of a well-defined nuclear family with accurate pedigree information, the stage is set to make the most direct estimates of the human mutation rate to date.},
author = {Cartwright, Reed A and Keebler, Jonathan E M and Carolina, North and Stone, Eric A and Hussin, Julie},
doi = {10.2202/1544-6115.1713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright et al. - 2012 - S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput S.pdf:pdf},
isbn = {1544-6115 (Electronic)$\backslash$r1544-6115 (Linking)},
issn = {1544-6115},
journal = {Statistical applications in genetics and molecular biology},
keywords = {Algorithms,Alleles,Computer Simulation,DNA Mutational Analysis,Family,Genetic,Genome,Genotype,High-Throughput Nucleotide Sequencing,Human,Humans,Models,Mutation,Pedigree,Probability,ROC Curve},
number = {2},
pmid = {22499693},
title = {{S YSTEMS B IOLOGY A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data A Family-Based Probabilistic Method for Capturing De Novo Mutations from High- Throughput Short-Read Sequencing Data}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3728889{\&}tool=pmcentrez{\&}rendertype=abstract{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/22499693{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3728889},
volume = {11},
year = {2012}
}
@article{Rehm2013,
abstract = {Next-generation sequencing technologies have been and continue to be deployed in clinical laboratories, enabling rapid transformations in genomic medicine. These technologies have reduced the cost of large-scale sequencing by several orders of magnitude, and continuous advances are being made. It is now feasible to analyze an individual's near-complete exome or genome to assist in the diagnosis of a wide array of clinical scenarios. Next-generation sequencing technologies are also facilitating further advances in therapeutic decision making and disease prediction for at-risk patients. However, with rapid advances come additional challenges involving the clinical validation and use of these constantly evolving technologies and platforms in clinical laboratories. To assist clinical laboratories with the validation of next-generation sequencing methods and platforms, the ongoing monitoring of next-generation sequencing testing to ensure quality results, and the interpretation and reporting of variants found using these technologies, the American College of Medical Genetics and Genomics has developed the following professional standards and guidelines.},
author = {Rehm, Heidi L and Bale, Sherri J and Bayrak-Toydemir, Pinar and Berg, Jonathan S and Brown, Kerry K and Deignan, Joshua L and Friez, Michael J and Funke, Birgit H and Hegde, Madhuri R and Lyon, Elaine},
doi = {10.1038/gim.2013.92},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rehm et al. - 2013 - ACMG clinical laboratory standards for next-generation sequencing.pdf:pdf},
isbn = {1098-3600},
issn = {1530-0366},
journal = {Genetics in medicine : official journal of the American College of Medical Genetics},
keywords = {DNA,DNA: instrumentation,DNA: methods,DNA: standards,Exome,Genetic Testing,Genetic Testing: standards,Genome,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: standards,Human,Humans,Laboratories,Laboratories: standards,Reproducibility of Results,Sequence Analysis,Translational Medical Research,United States},
number = {9},
pages = {733--47},
pmid = {23887774},
title = {{ACMG clinical laboratory standards for next-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4098820{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {15},
year = {2013}
}
@book{Soames2006,
author = {Soames, Scott and Misak, Cheryl},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Soames, Misak - 2006 - by Edited by.pdf:pdf},
isbn = {063119083X},
title = {{by Edited by}},
volume = {132},
year = {2006}
}
@article{VanderSpoel2015,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{\{}{\%}{\}} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {van der Spoel, Evie and Rozing, Maarten P and Houwing-Duistermaat, Jeanine J and {Eline Slagboom}, P and Beekman, Marian and de Craen, Anton J M and Westendorp, Rudi G J and van Heemst, Diana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Spoel et al. - 2015 - Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status i.pdf:pdf},
isbn = {9788578110796},
issn = {19454589},
journal = {Aging},
keywords = {Familial longevity,Functional status,Human,IGF-1 axis,Survival},
number = {11},
pages = {956--963},
pmid = {25246403},
title = {{Association analysis of insulin-like growth factor-1 axis parameters with survival and functional status in nonagenarians of the Leiden Longevity Study}},
volume = {7},
year = {2015}
}
@article{Liu2013,
abstract = {Next generation sequencing (NGS) has been leading the genetic study of human disease into an era of unprecedented productivity. Many bioinformatics pipelines have been developed to call variants from NGS data. The performance of these pipelines depends crucially on the variant caller used and on the calling strategies implemented. We studied the performance of four prevailing callers, SAMtools, GATK, glftools and Atlas2, using single-sample and multiple-sample variant-calling strategies. Using the same aligner, BWA, we built four single-sample and three multiple-sample calling pipelines and applied the pipelines to whole exome sequencing data taken from 20 individuals. We obtained genotypes generated by Illumina Infinium HumanExome v1.1 Beadchip for validation analysis and then used Sanger sequencing as a "gold-standard" method to resolve discrepancies for selected regions of high discordance. Finally, we compared the sensitivity of three of the single-sample calling pipelines using known simulated whole genome sequence data as a gold standard. Overall, for single-sample calling, the called variants were highly consistent across callers and the pairwise overlapping rate was about 0.9. Compared with other callers, GATK had the highest rediscovery rate (0.9969) and specificity (0.99996), and the Ti/Tv ratio out of GATK was closest to the expected value of 3.02. Multiple-sample calling increased the sensitivity. Results from the simulated data suggested that GATK outperformed SAMtools and glfSingle in sensitivity, especially for low coverage data. Further, for the selected discrepant regions evaluated by Sanger sequencing, variant genotypes called by exome sequencing versus the exome array were more accurate, although the average variant sensitivity and overall genotype consistency rate were as high as 95.87{\%} and 99.82{\%}, respectively. In conclusion, GATK showed several advantages over other variant callers for general purpose NGS analyses. The GATK pipelines we developed perform very well.},
author = {Liu, Xiangtao and Han, Shizhong and Wang, Zuoheng and Gelernter, Joel and Yang, Bao Zhu},
doi = {10.1371/journal.pone.0075619},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Variant Callers for Next-Generation Sequencing Data A Comparison Study.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--11},
pmid = {24086590},
title = {{Variant Callers for Next-Generation Sequencing Data: A Comparison Study}},
volume = {8},
year = {2013}
}
@book{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - FastQC Manual.pdf:pdf},
title = {{FastQC Manual}},
url = {https://biof-edu.colorado.edu/videos/dowell-short-read-class/day-4/fastqc-manual}
}
@article{Eduardoff2015,
abstract = {Abstract Next generation sequencing (NGS) offers the opportunity to analyse forensic DNA samples and obtain massively parallel coverage of targeted short sequences with the variants they carry. We evaluated the levels of sequence coverage, genotyping precision, sensitivity and mixed DNA patterns of a prototype version of the first commercial forensic NGS kit: the HID-Ion AmpliSeq™ Identity Panel with 169-markers designed for the Ion PGM™ system. Evaluations were made between three laboratories following closely matched Ion PGM™ protocols and a simple validation framework of shared DNA controls. The sequence coverage obtained was extensive for the bulk of SNPs targeted by the HID-Ion AmpliSeq™ Identity Panel. Sensitivity studies showed 90-95{\%} of SNP genotypes could be obtained from 25 to 100 pg of input DNA. Genotyping concordance tests included Coriell cell-line control DNA analyses checked against whole-genome sequencing data from 1000 Genomes and Complete Genomics, indicating a very high concordance rate of 99.8{\%}. Discordant genotypes detected in rs1979255, rs1004357, rs938283, rs2032597 and rs2399332 indicate these loci should be excluded from the panel. Therefore, the HID-Ion AmpliSeq™ Identity Panel and Ion PGM™ system provide a sensitive and accurate forensic SNP genotyping assay. However, low-level DNA produced much more varied sequence coverage and in forensic use the Ion PGM™ system will require careful calibration of the total samples loaded per chip to preserve the genotyping reliability seen in routine forensic DNA. Furthermore, assessments of mixed DNA indicate the user's control of sequence analysis parameter settings is necessary to ensure mixtures are detected robustly. Given the sensitivity of Ion PGM™, this aspect of forensic genotyping requires further optimisation before massively parallel sequencing is applied to routine casework.},
author = {Eduardoff, M. and Santos, C. and {De La Puente}, M. and Gross, T. E. and Fondevila, M. and Strobl, C. and Sobrino, B. and Ballard, D. and Schneider, P. M. and Carracedo and Lareu, M. V. and Parson, W. and Phillips, C.},
doi = {10.1016/j.fsigen.2015.04.007},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eduardoff et al. - 2015 - Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the.pdf:pdf},
issn = {18780326},
journal = {Forensic Science International: Genetics},
keywords = {Identification SNPs,Ion PGM™,Ion Torrent,Massively parallel sequencing,Next generation sequencing},
pages = {110--121},
pmid = {25955683},
publisher = {Elsevier Ireland Ltd},
title = {{Inter-laboratory evaluation of SNP-based forensic identification by massively parallel sequencing using the Ion PGM™}},
url = {http://dx.doi.org/10.1016/j.fsigen.2015.04.007},
volume = {17},
year = {2015}
}
@article{Morimoto2016,
abstract = {We developed a new approach for pairwise kinship analysis in forensic genetics based on chromosomal sharing between two individuals. Here, we defined "index of chromosome sharing" (ICS) calculated using 174,254 single nucleotide polymorphism (SNP) loci typed by SNP microarray and genetic length of the shared segments from the genotypes of two individuals. To investigate the expected ICS distributions from first- to fifth-degree relatives and unrelated pairs, we used computationally generated genotypes to consider the effect of linkage disequilibrium and recombination. The distributions were used for probabilistic evaluation of the pairwise kinship analysis, such as likelihood ratio (LR) or posterior probability, without allele frequencies and haplotype frequencies. Using our method, all actual sample pairs from volunteers showed significantly high LR values (i.e., ≥ 108); therefore, we can distinguish distant relationships (up to the fifth-degree) from unrelated pairs based on LR. Moreover, we can determine accurate degrees of kinship in up to third-degree relationships with a probability of {\textgreater} 80{\%} using the criterion of posterior probability ≥ 0.90, even if the kinship of the pair is totally unpredictable. This approach greatly improves pairwise kinship analysis of distant relationships, specifically in cases involving identification of disaster victims or missing persons.},
author = {Morimoto, Chie and Manabe, Sho and Kawaguchi, Takahisa and Kawai, Chihiro and Fujimoto, Shuntaro and Hamano, Yuya and Yamada, Ryo and Matsuda, Fumihiko and Tamaki, Keiji},
doi = {10.1371/journal.pone.0160287},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morimoto et al. - 2016 - Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
number = {7},
pages = {e0160287},
pmid = {27472558},
title = {{Pairwise Kinship Analysis by the Index of Chromosome Sharing Using High-Density Single Nucleotide Polymorphisms.}},
url = {http://dx.plos.org/10.1371/journal.pone.0160287{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/27472558{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4966930},
volume = {11},
year = {2016}
}
@article{Cheng2015,
author = {Cheng, Phil and Levesque, Mitch and Cheng, Phil F and Dummer, Reinhard and Levesque, Mitch P},
doi = {10.4414/smw.2015.14183},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2015 - Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in.pdf:pdf},
keywords = {cancer are somatic mutation,copy number,data mining,data mining, Genomics, transcriptomics, Cancer Gen,gene expres-,genomics,on the tcga data,portal for each,the cancer genome atlas,the data types listed,transcriptomics},
number = {October},
pages = {1--5},
title = {{Data mining The Cancer Genome Atlas in the era of precision cancer medicine Data mining The Cancer Genome Atlas in the era of precision cancer medicine}},
year = {2015}
}
@article{Urbanczyk2016,
author = {Urbanczyk, Tomas and Peter, Lukas},
doi = {10.1016/j.ifacol.2016.12.047},
file = {:home/jennifer/Descargas/1-s2.0-S2405896316326830-main.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Database,MySQL,RFID,The Urgent department,environment analysis},
number = {25},
pages = {278--283},
publisher = {Elsevier B.V.},
title = {{Database Development for the Urgent Department of Hospital based on Tagged Entity Storage Following the IoT Concept}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2405896316326830},
volume = {49},
year = {2016}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Reviews3.csv:csv},
title = {{Reviews3}}
}
@article{Santani2017,
abstract = {Context.—The number of targeted next-generation se-quencing (NGS) panels for genetic diseases offered by clinical laboratories is rapidly increasing. Before an NGS-based test is implemented in a clinical laboratory, appropriate validation studies are needed to determine the performance characteristics of the test. Objective.—To provide examples of assay design and validation of targeted NGS gene panels for the detection of germline variants associated with inherited disorders. Data Sources.—The approaches used by 2 clinical laboratories for the development and validation of targeted NGS gene panels are described. Important design and validation considerations are examined. Conclusions.—Clinical laboratories must validate per-formance specifications of each test prior to implementa-tion. Test design specifications and validation data are provided, outlining important steps in validation of targeted NGS panels by clinical diagnostic laboratories.},
author = {Santani, Avni and Murrell, Jill and Funke, Birgit and Yu, Zhenming and Hegde, Madhuri and Mao, Rong and Ferreira-Gonzalez, Andrea and Voelkerding, Karl V. and Weck, Karen E.},
doi = {10.5858/arpa.2016-0517-RA},
file = {:home/jennifer/Descargas/arpa.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {6},
pages = {787--797},
title = {{Development and validation of targeted next-generation sequencing panels for detection of germline variants in inherited diseases}},
volume = {141},
year = {2017}
}
@article{Canuel2015,
author = {Canuel, V. and Rance, B. and Avillach, P. and Degoulet, P. and Burgun, A.},
doi = {10.1093/bib/bbu006},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canuel et al. - 2015 - Translational research platforms integrating clinical and omics data a review of publicly available solutions.pdf:pdf},
issn = {1467-5463},
journal = {Briefings in Bioinformatics},
keywords = {biomedical research,clinical data,high-throughput technologies,information storage,translational medical research},
number = {2},
pages = {280--290},
title = {{Translational research platforms integrating clinical and omics data: a review of publicly available solutions}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbu006},
volume = {16},
year = {2015}
}
@book{Quinlan2014,
abstract = {Technological advances have enabled the use of DNA sequencing as a flexible tool to characterize genetic variation and to measure the activity of diverse cellular phenomena such as gene isoform expression and transcription factor binding. Extracting biological insight from the experiments enabled by these advances demands the analysis of large, multi-dimensional datasets. This unit describes the use of the BEDTools toolkit for the exploration of high-throughput genomics datasets. Several protocols are presented for common genomic analyses, demonstrating how simple BEDTools operations may be combined to create bespoke pipelines addressing complex questions. Curr. Protoc. Bioinform. 47:11.12.1-11.12.34. {\textcopyright} 2014 by John Wiley {\&} Sons, Inc.},
author = {Quinlan, Aaron R.},
booktitle = {Current Protocols in Bioinformatics},
doi = {10.1002/0471250953.bi1112s47},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan - 2014 - BEDTools The Swiss-Army tool for genome feature analysis.pdf:pdf},
isbn = {0471250953},
issn = {1934340X},
keywords = {Bioinformatics,Genome analysis,Genome features,Genome intervals,Genomics},
pages = {11.12.1--11.12.34},
pmid = {25199790},
title = {{BEDTools: The Swiss-Army tool for genome feature analysis}},
volume = {2014},
year = {2014}
}
@article{Conesa2016,
abstract = {RNA-sequencing (RNA-seq) has a wide variety of applications, but no single analysis pipeline can be used in all cases. We review all of the major steps in RNA-seq data analysis, including experimental design, quality control, read alignment, quantification of gene and transcript levels, visualization, differential gene expression, alternative splicing, functional analysis, gene fusion detection and eQTL mapping. We highlight the challenges associated with each step. We discuss the analysis of small RNAs and the integration of RNA-seq with other functional genomics techniques. Finally, we discuss the outlook for novel technologies that are changing the state of the art in transcriptomics.},
author = {Conesa, Ana and Madrigal, Pedro and Tarazona, Sonia and Gomez-Cabrero, David and Cervera, Alejandra and McPherson, Andrew and Szcze{\'{s}}niak, Micha{\l} Wojciech and Gaffney, Daniel J. and Elo, Laura L. and Zhang, Xuegong and Mortazavi, Ali},
doi = {10.1186/s13059-016-0881-8},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conesa et al. - 2016 - A survey of best practices for RNA-seq data analysis.pdf:pdf},
isbn = {1474-760X (Electronic)$\backslash$r1474-7596 (Linking)},
issn = {1474-760X},
journal = {Genome Biology},
keywords = {Animal Genetics and Genomics,Bioinformatics,Evolutionary Biology,Human Genetics,Microbial Genetics and Genomics,Plant Genetics {\&} Genomics},
number = {1},
pages = {13},
pmid = {26813401},
title = {{A survey of best practices for RNA-seq data analysis}},
url = {http://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0881-8},
volume = {17},
year = {2016}
}
@misc{,
annote = {NULL},
pages = {http://www.infobae.com/america/america--latina/2016},
title = {{Colombia se propuso identificar a 16.000 v{\'{i}}ctimas del conflicto armado con las FARC}},
url = {http://www.infobae.com/america/america-latina/2016},
urldate = {2016-11-20}
}
@article{ORawe2013,
abstract = {BACKGROUND: To facilitate the clinical implementation of genomic medicine by next-generation sequencing, it will be critically important to obtain accurate and consistent variant calls on personal genomes. Multiple software tools for variant calling are available, but it is unclear how comparable these tools are or what their relative merits in real-world scenarios might be.$\backslash$n$\backslash$nMETHODS: We sequenced 15 exomes from four families using commercial kits (Illumina HiSeq 2000 platform and Agilent SureSelect version 2 capture kit), with approximately 120X mean coverage. We analyzed the raw data using near-default parameters with five different alignment and variant-calling pipelines (SOAP, BWA-GATK, BWA-SNVer, GNUMAP, and BWA-SAMtools). We additionally sequenced a single whole genome using the sequencing and analysis pipeline from Complete Genomics (CG), with 95{\%} of the exome region being covered by 20 or more reads per base. Finally, we validated 919 single-nucleotide variations (SNVs) and 841 insertions and deletions (indels), including similar fractions of GATK-only, SOAP-only, and shared calls, on the MiSeq platform by amplicon sequencing with approximately 5000X mean coverage.$\backslash$n$\backslash$nRESULTS: SNV concordance between five Illumina pipelines across all 15 exomes was 57.4{\%}, while 0.5 to 5.1{\%} of variants were called as unique to each pipeline. Indel concordance was only 26.8{\%} between three indel-calling pipelines, even after left-normalizing and intervalizing genomic coordinates by 20 base pairs. There were 11{\%} of CG variants falling within targeted regions in exome sequencing that were not called by any of the Illumina-based exome analysis pipelines. Based on targeted amplicon sequencing on the MiSeq platform, 97.1{\%}, 60.2{\%}, and 99.1{\%} of the GATK-only, SOAP-only and shared SNVs could be validated, but only 54.0{\%}, 44.6{\%}, and 78.1{\%} of the GATK-only, SOAP-only and shared indels could be validated. Additionally, our analysis of two families (one with four individuals and the other with seven), demonstrated additional accuracy gained in variant discovery by having access to genetic data from a multi-generational family.$\backslash$n$\backslash$nCONCLUSIONS: Our results suggest that more caution should be exercised in genomic medicine settings when analyzing individual genomes, including interpreting positive and negative findings with scrutiny, especially for indels. We advocate for renewed collection and sequencing of multi-generational families to increase the overall accuracy of whole genomes.},
author = {O'Rawe, Jason and Jiang, Tao and Sun, Guangqing and Wu, Yiyang and Wang, Wei and Hu, Jingchu and Bodily, Paul and Tian, Lifeng and Hakonarson, Hakon and Johnson, W Evan and Wei, Zhi and Wang, Kai and Lyon, Gholson J},
doi = {10.1186/gm432},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Rawe et al. - 2013 - Low concordance of multiple variant-calling pipelines practical implications for exome and genome sequencing.pdf:pdf},
isbn = {1756-994X (Print)},
issn = {1756-994X},
journal = {Genome medicine},
number = {3},
pages = {28},
pmid = {23537139},
title = {{Low concordance of multiple variant-calling pipelines: practical implications for exome and genome sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3706896{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2013}
}
@article{Lazaridis2016,
abstract = {OBJECTIVE: To describe the experience and outcome of performing whole-exome sequencing (WES) for resolution of patients on a diagnostic odyssey in the first 18 months of an individualized medicine clinic (IMC).

PATIENTS AND METHODS: The IMC offered WES to physicians of Mayo Clinic practice for patients with suspected genetic disease. DNA specimens of the proband and relatives were submitted to WES laboratories. We developed the Genomic Odyssey Board with multidisciplinary expertise to determine the appropriateness for IMC services, review WES reports, and make the final decision about whether the exome findings explain the disease. This study took place from September 30, 2012, to March 30, 2014.

RESULTS: In the first 18 consecutive months, the IMC received 82 consultation requests for patients on a diagnostic odyssey. The Genomic Odyssey Board deferred 7 cases and approved 75 cases to proceed with WES. Seventy-one patients met with an IMC genomic counselor. Fifty-one patients submitted specimens for WES testing, and the results have been received for all. There were 15 cases in which a diagnosis was made on the basis of WES findings; thus, the positive diagnostic yield of this practice was 29{\%}. The mean cost per patient for this service was approximately {\$}8000. Medicaid supported 27{\%} of the patients, and 38{\%} of patients received complete or partial insurance coverage.

CONCLUSION: The significant diagnostic yield, moderate cost, and notable health marketplace acceptance for WES compared with conventional genetic testing make the former method a rational diagnostic approach for patients on a diagnostic odyssey.},
author = {Lazaridis, Konstantinos N and Schahl, Kimberly A and Cousin, Margot A and Babovic-Vuksanovic, Dusica and Riegert-Johnson, Douglas L and Gavrilova, Ralitza H and McAllister, Tammy M and Lindor, Noralane M and Abraham, Roshini S and Ackerman, Michael J and Pichurin, Pavel N and Deyle, David R and Gavrilov, Dimitar K and Hand, Jennifer L and Klee, Eric W and Stephens, Michael C and Wick, Myra J and Atkinson, Elizabeth J and Linden, David R and Ferber, Matthew J and Wieben, Eric D and Farrugia, Gianrico},
doi = {10.1016/j.mayocp.2015.12.018},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
issn = {1942-5546},
journal = {Mayo Clinic proceedings},
language = {English},
month = {mar},
number = {3},
pages = {297--307},
pmid = {26944241},
publisher = {Elsevier},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
url = {http://www.mayoclinicproceedings.org/article/S0025619616000240/fulltext},
volume = {91},
year = {2016}
}
@inproceedings{Buchanan2012,
author = {Buchanan, Carrie C. and Wallace, John R. and Frase, Alex T. and Torstenson, Eric S. and Pendergrass, Sarah A. and Ritchie, Marylyn D.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-29066-4_18},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchanan et al. - 2012 - A biologically informed method for detecting associations with rare variants.pdf:pdf},
isbn = {9783642290657},
issn = {03029743},
keywords = {Collapsing Tool,Pathway Analysis,Prior Knowledge,Rare Variants},
title = {{A biologically informed method for detecting associations with rare variants}},
year = {2012}
}
@article{Weitschek2014,
author = {Weitschek, Emanuel and Fiscon, Giulia and Felici, Giovanni},
doi = {10.1186/1756-0381-7-4},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Weitschek, Fiscon, Felici - 2014 - Supervised DNA Barcodes species classification analysis, comparisons and results.pdf:pdf},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {DNA Barcoding,Species identification,Supervised classification methods},
number = {1},
pages = {4},
publisher = {BioData Mining},
title = {{Supervised DNA Barcodes species classification: analysis, comparisons and results}},
url = {http://www.biodatamining.org/content/7/1/4},
volume = {7},
year = {2014}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/},
urldate = {2015-11-04}
}
@phdthesis{Acosta2015,
author = {Acosta, Juan Pablo},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acosta - 2015 - Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data.pdf:pdf},
title = {{Strategy for Multivariate Identification of Di ↵ erentially Expressed Genes in Microarray Data}},
year = {2015}
}
@article{Hwang2015,
abstract = {The success of clinical genomics using next generation sequencing (NGS) requires the accurate and consistent identification of personal genome variants. Assorted variant calling methods have been developed, which show low concordance between their calls. Hence, a systematic comparison of the variant callers could give important guidance to NGS-based clinical genomics. Recently, a set of high-confident variant calls for one individual (NA12878) has been published by the Genome in a Bottle (GIAB) consortium, enabling performance benchmarking of different variant calling pipelines. Based on the gold standard reference variant calls from GIAB, we compared the performance of thirteen variant calling pipelines, testing combinations of three read aligners-BWA-MEM, Bowtie2, and Novoalign-and four variant callers-Genome Analysis Tool Kit HaplotypeCaller (GATK-HC), Samtools mpileup, Freebayes and Ion Proton Variant Caller (TVC), for twelve data sets for the NA12878 genome sequenced by different platforms including Illumina2000, Illumina2500, and Ion Proton, with various exome capture systems and exome coverage. We observed different biases toward specific types of SNP genotyping errors by the different variant callers. The results of our study provide useful guidelines for reliable variant identification from deep sequencing of personal genomes.},
author = {Hwang, Sohyun and Kim, Eiru and Lee, Insuk and Marcotte, Edward M.},
doi = {10.1038/srep17875},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang et al. - 2015 - Systematic comparison of variant calling pipelines using gold standard personal exome variants.pdf:pdf},
isbn = {8010628980},
issn = {2045-2322},
journal = {Scientific Reports},
number = {December},
pages = {17875},
pmid = {26639839},
publisher = {Nature Publishing Group},
title = {{Systematic comparison of variant calling pipelines using gold standard personal exome variants}},
url = {http://www.nature.com/articles/srep17875{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26639839{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4671096},
volume = {5},
year = {2015}
}
@book{Yunis2002,
author = {Yunis, E and Yunis, J},
publisher = {Temis S.A.},
title = {{El ADN en la Identificaci{\'{o}}n Humana}},
year = {2002}
}
@article{Pandey2016,
author = {Pandey, Ram Vinay and Pabinger, Stephan and Kriegner, Albert and Weinh{\"{a}}usel, Andreas},
doi = {10.1186/s12859-016-0915-y},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2016 - ClinQC a tool for quality control and cleaning of Sanger and NGS data in clinical research.pdf:pdf},
isbn = {14712105 (Electronic)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Sanger sequencing,Next generation sequencing,Quali,molecular diagnostic testing,next generation sequencing,quality control,sanger sequencing},
number = {1},
pages = {56},
pmid = {26830926},
publisher = {BMC Bioinformatics},
title = {{ClinQC: a tool for quality control and cleaning of Sanger and NGS data in clinical research}},
url = {http://www.biomedcentral.com/1471-2105/17/56},
volume = {17},
year = {2016}
}
@article{Cleary2014,
author = {Cleary, John G. and Braithwaite, Ross and Gaastra, Kurt and Hilbush, Brian S. and Inglis, Stuart and Irvine, Sean A. and Jackson, Alan and Littin, Richard and Nohzadeh-Malakshah, Sahar and Rathod, Mehul and Ware, David and Trigg, Len and {De La Vega}, Francisco M.},
doi = {10.1089/cmb.2014.0029},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cleary et al. - 2014 - Joint Variant and iDe Novoi Mutation Identification on Pedigrees from High-Throughput Sequencing Data.pdf:pdf},
issn = {1066-5277},
journal = {Journal of Computational Biology},
keywords = {bayesian networks,indel,mnp,multiple-nucleotide polymorphism,next-generation,pedigree,sequencing,single-nucleotide variant,snv,trios,variant calling},
number = {6},
pages = {405--419},
title = {{Joint Variant and {\textless}i{\textgreater}De Novo{\textless}/i{\textgreater} Mutation Identification on Pedigrees from High-Throughput Sequencing Data}},
url = {http://online.liebertpub.com/doi/abs/10.1089/cmb.2014.0029},
volume = {21},
year = {2014}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@book{,
title = {{FastQC Manual}},
url = {https://biof-edu.colorado.edu/videos/dowell-short-read-class/day-4/fastqc-manual}
}
@article{Li2012,
abstract = {Family samples, which can be enriched for rare causal variants by focusing on families with multiple extreme individuals and which facilitate detection of de novo mutation events, provide an attractive resource for next-generation sequencing studies. Here, we describe, implement, and evaluate a likelihood-based framework for analysis of next generation sequence data in family samples. Our framework is able to identify variant sites accurately and to assign individual genotypes, and can handle de novo mutation events, increasing the sensitivity and specificity of variant calling and de novo mutation detection. Through simulations we show explicit modeling of family relationships is especially useful for analyses of low-frequency variants and that genotype accuracy increases with the number of individuals sequenced per family. Compared with the standard approach of ignoring relatedness, our methods identify and accurately genotype more variants, and have high specificity for detecting de novo mutation events. The improvement in accuracy using our methods over the standard approach is particularly pronounced for low-frequency variants. Furthermore the family-aware calling framework dramatically reduces Mendelian inconsistencies and is beneficial for family-based analysis. We hope our framework and software will facilitate continuing efforts to identify genetic factors underlying human diseases.},
author = {Li, Bingshan and Chen, Wei and Zhan, Xiaowei and Busonero, Fabio and Sanna, Serena and Sidore, Carlo and Cucca, Francesco and Kang, Hyun M. and Abecasis, Gon??alo R.},
doi = {10.1371/journal.pgen.1002944},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families.pdf:pdf},
isbn = {1553-7404 (Electronic)$\backslash$n1553-7390 (Linking)},
issn = {15537390},
journal = {PLoS Genetics},
number = {10},
pmid = {23055937},
title = {{A Likelihood-Based Framework for Variant Calling and De Novo Mutation Detection in Families}},
volume = {8},
year = {2012}
}
@article{Moore2010,
abstract = {MOTIVATION: The sequencing of the human genome has made it possible to identify an informative set of {\textgreater}1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs). The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing. This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies. However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing. One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology. Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context. There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype-phenotype relationship that is characterized by significant heterogeneity and gene-gene and gene-environment interaction. We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases. The goal of this review is to identify and discuss those GWAS challenges that will require computational methods.},
author = {Moore, Jason H. and Asselbergs, Folkert W. and Williams, Scott M.},
doi = {10.1093/bioinformatics/btp713},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Asselbergs, Williams - 2010 - Bioinformatics challenges for genome-wide association studies.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pages = {445--455},
pmid = {20053841},
title = {{Bioinformatics challenges for genome-wide association studies}},
volume = {26},
year = {2010}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey{\#}0},
volume = {16},
year = {2004}
}
@article{Fisch2015,
abstract = {Omics Pipe (https://bitbucket.org/sulab/omics{\_}pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis.},
author = {Fisch, Kathleen M. and Mei{\ss}ner, Tobias and Gioia, Louis and Ducom, Jean Christophe and Carland, Tristan M. and Loguercio, Salvatore and Su, Andrew I.},
doi = {10.1093/bioinformatics/btv061},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisch et al. - 2015 - Omics Pipe A community-based framework for reproducible multi-omics data analysis.pdf:pdf},
isbn = {13674811 (Electronic)},
issn = {14602059},
journal = {Bioinformatics},
number = {11},
pages = {1724--1728},
pmid = {25637560},
title = {{Omics Pipe: A community-based framework for reproducible multi-omics data analysis}},
volume = {31},
year = {2015}
}
@article{Jiang2004,
author = {Jiang, D and Tang, C and Zhang, a},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Tang, Zhang - 2004 - Cluster analysis for gene expression data A survey.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data},
number = {11},
pages = {1370--1386},
title = {{Cluster analysis for gene expression data: A survey}},
url = {http://scholar.google.com/scholar?hl=en{\%}7B{\&}{\%}7DbtnG=Search{\%}7B{\&}{\%}7Dq=intitle:Cluster+Analysis+for+Gene+Expression+Data+A+Survey{\%}7B{\#}{\%}7D0},
volume = {16},
year = {2004}
}
@book{Herraez2012,
address = {Barcelona},
author = {Herr{\'{a}}ez, Angel},
isbn = {978-84-8086-647-7},
pages = {241},
publisher = {Elsevier Ltd},
title = {{Biolog{\'{i}}a Molecular e Ingenier{\'{i}}a Gen{\'{e}}tica. 2{\textordfeminine} ed.}},
year = {2012}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@article{KonstantinosN.LazaridisMD;KimberlyA.SchahlCGC;MargotA.CousinPhD;DusicaBabovic-VuksanovicMD;DouglasL.Riegert-JohnsonMD;RalitzaH.GavrilovaMD;TammyM.McAllisterMA;NoralaneM.LindorMD;RoshiniS.AbrahamPhD;MichaelJ.Ac2016,
author = {{Konstantinos N. Lazaridis, MD; Kimberly A. Schahl, CGC; Margot A. Cousin, PhD; Dusica Babovic-Vuksanovic, MD; Douglas L. Riegert-Johnson, MD; Ralitza H. Gavrilova, MD; Tammy M. McAllister, MA; Noralane M. Lindor, MD; Roshini S. Abraham, PhD; Michael J. Ac}, MD; and the Individualized Medicine Clinic Members},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantinos N. Lazaridis, MD Kimberly A. Schahl, CGC Margot A. Cousin, PhD Dusica Babovic-Vuksanovic, MD Douglas L. Riegert-Johnson, MD.pdf:pdf},
journal = {Mayo Clinic Proceedings},
number = {3},
pages = {29},
title = {{Outcome of Whole Exome Sequencing for Diagnostic Odyssey Cases of an Individualized Medicine Clinic: The Mayo Clinic Experience.}},
volume = {91},
year = {2016}
}
@book{Smith98,
address = {London},
author = {Smith, J},
edition = {2nd},
publisher = {The publishing company},
title = {{The Book}},
year = {1998}
}
@article{Guo2013,
abstract = {Advances in next-generation sequencing (NGS) technologies have greatly improved our ability to detect genomic variants for biomedical research. In particular, NGS technologies have been recently applied with great success to the discovery of mutations associated with the growth of various tumours and in rare Mendelian diseases. The advance in NGS technologies has also created significant challenges in bioinformatics. One of the major challenges is quality control of the sequencing data. In this review, we discuss the proper quality control procedures and parameters for Illumina technology-based human DNA re-sequencing at three different stages of sequencing: raw data, alignment and variant calling. Monitoring quality control metrics at each of the three stages of NGS data provides unique and independent evaluations of data quality from differing perspectives. Properly conducting quality control protocols at all three stages and correctly interpreting the quality control results are crucial to ensure a successful and meaningful study.},
author = {Guo, Yan and Ye, Fei and Sheng, Quanghu and Clark, Travis and Samuels, David C.},
doi = {10.1093/bib/bbt069},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - 2013 - Three-stage quality control strategies for DNA re-sequencing data.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$r1467-5463 (Linking)},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Alignment,FASTQ,Quality control,Sequencing,Variant calling},
number = {6},
pages = {879--889},
pmid = {24067931},
title = {{Three-stage quality control strategies for DNA re-sequencing data}},
volume = {15},
year = {2013}
}
@article{Maharjan2011,
author = {Maharjan, Merina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maharjan - 2011 - Genome Analysis with MapReduce.pdf:pdf},
pages = {1--23},
title = {{Genome Analysis with MapReduce}},
year = {2011}
}
@article{Niroula2016,
abstract = {Next generation sequencing (NGS) methods have revolutionized the speed of generating variation information. Sequence data have a plethora of applications and will increasingly be used for disease diagnosis. Interpretation of the identified variants is usually not possible with experimental methods. This has caused a bottleneck that many computational methods aim at addressing. Fast and efficient methods for explaining the significance and mechanisms of detected variants are required for efficient precision/personalized medicine. Computational prediction methods have been developed in three areas to address the issue. There are generic tolerance (pathogenicity) predictors for filtering harmful variants. Gene/protein/disease-specific tools are available for some applications. Mechanism and effect-specific computer programs aim at explaining the consequences of variations. Here, we discuss the different types of predictors and their applications. We review available variation databases and prediction methods useful for variation interpretation. We discuss how the performance of methods is assessed and summarize existing assessment studies. A brief introduction is provided to the principles of the methods developed for variation interpretation as well as guidelines for how to choose the optimal tools and where the field is heading in the future. This article is protected by copyright. All rights reserved.},
author = {Niroula, Abhishek and Vihinen, Mauno},
doi = {10.1002/humu.22987},
file = {:home/jennifer/Descargas/niroula2016.pdf:pdf},
issn = {1098-1004},
journal = {Human mutation},
month = {mar},
pmid = {26987456},
title = {{Variation Interpretation Predictors: Principles, Types, Performance and Choice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26987456},
year = {2016}
}
@article{Coonrod2013,
abstract = {CONTEXT: Advances in sequencing technology with the commercialization of next-generation sequencing (NGS) has substantially increased the feasibility of sequencing human genomes and exomes. Next-generation sequencing has been successfully applied to the discovery of disease-causing genes in rare, inherited disorders. By necessity, the advent of NGS has fostered the concurrent development of bioinformatics approaches to expeditiously analyze the large data sets generated. Next-generation sequencing has been used for important discoveries in the research setting and is now being implemented into the clinical diagnostic arena.$\backslash$n$\backslash$nOBJECTIVE: To review the current literature on technical and bioinformatics approaches for exome and genome sequencing and highlight examples of successful disease gene discovery in inherited disorders. To discuss the challenges for implementing NGS in the clinical research and diagnostic arenas.$\backslash$n$\backslash$nDATA SOURCES: Literature review and authors' experience.$\backslash$n$\backslash$nCONCLUSIONS: Next-generation sequencing approaches are powerful and require an investment in infrastructure and personnel expertise for effective use; however, the potential for improvement of patient care through faster and more accurate molecular diagnoses is high.},
author = {Coonrod, Emily M. and Durtschi, Jacob D. and Margraf, Rebecca L. and Voelkerding, Karl V.},
doi = {10.5858/arpa.2012-0107-RA},
isbn = {1543-2165 (Electronic)$\backslash$r0003-9985 (Linking)},
issn = {00039985},
journal = {Archives of Pathology and Laboratory Medicine},
number = {3},
pages = {415--433},
pmid = {22770468},
title = {{Developing genome and exome sequencing for candidate gene identification in inherited disorders: An integrated technical and bioinformatics approach}},
volume = {137},
year = {2013}
}
@techreport{Chu2011,
abstract = {SAM (Significance Analysis of Microarrays) is a statistical technique for finding significant genes in a set of microarray experiments. It was proposed by Tusher, Tibshirani and Chu [9]. The software was written by Balasubramanian Narasimhan and Robert Tibshirani. The input to SAM is gene expression measurements from a set of microarray experiments, as well as a response variable from each experiment. The response variable may be a grouping like untreated, treated [either unpaired or paired], a multiclass grouping (like breast cancer, lymphoma, colon cancer, . . . ), a quantitative variable (like blood pressure) or a possibly censored survival time. SAM computes a statistic di for each gene i, measuring the strength of the relationship between gene expression and the response variable. It uses repeated permutations of the data to determine if the expression of any genes are significantly related to the response. The cutoff for significance is determined by a tuning parameter delta, chosen by the user based on the false positive rate. One can also choose a fold change parameter, to ensure that called genes change at least a pre-specified amount.},
author = {Chu, Gil and Li, Jun and Narasimhan, Balasubramanian and Tibshirani, Robert and Tusher, Virginia},
booktitle = {Policy},
doi = {10.1261/rna.1473809},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chu et al. - 2011 - SAM - Significance Analysis of Microarrays - Users guide and technical document.pdf:pdf},
issn = {1469-9001},
keywords = {gene expression,microarrays,significance analysi},
pages = {1--42},
pmid = {19307293},
title = {{SAM - Significance Analysis of Microarrays - Users guide and technical document}},
year = {2011}
}
@article{Systems2009a,
author = {Systems, Computational and Group, Biology},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Systems, Group - 2009 - Structural variation and Medical Genomics.pdf:pdf},
title = {{Structural variation and Medical Genomics}},
year = {2009}
}
@article{Bajcsy2005,
abstract = {Recent progress in biology, medical science, bioinformatics, and biotechnology has led to the accumulation of tremendous amounts of biodata that demands in-depth analysis. On the other hand, recent progress in data mining research has led to the development of numerous efficient and scalable methods for mining interesting patterns in large databases. The question becomes how to bridge the two fields, data mining and bioinformatics, for successful mining of biological data. In this chapter, we present an overview of the data mining methods that help biodata analysis. Moreover, we outline some research problems that may motivate the further development of data mining tools for the analysis of various kinds of biological data.},
author = {Bajcsy, Peter and Han, Jiawei and Liu, Lei and Yang, Jiong},
doi = {10.1007/1-84628-059-1_2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bajcsy et al. - 2005 - Survey of Biodata Analysis from a Data Mining Perspective.pdf:pdf},
isbn = {1852336714},
journal = {Data Mining in Bioinformatics},
pages = {9--39},
title = {{Survey of Biodata Analysis from a Data Mining Perspective}},
url = {http://dx.doi.org/10.1007/1-84628-059-1{\_}2},
year = {2005}
}
@article{Paila2013,
abstract = {注释你 Mutation$\backslash$r$\backslash$n可以研究家系遗传病},
archivePrefix = {arXiv},
arxivId = {1304.4860},
author = {Paila, Umadevi and Chapman, Brad A. and Kirchner, Rory and Quinlan, Aaron R.},
doi = {10.1371/journal.pcbi.1003153},
eprint = {1304.4860},
file = {:home/jennifer/Descargas/journal.pcbi.1003153(1).PDF:PDF},
isbn = {10.1371/journal.pcbi.1003153},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {7},
pmid = {23874191},
title = {{GEMINI: Integrative Exploration of Genetic Variation and Genome Annotations}},
volume = {9},
year = {2013}
}
@article{Tetreault2015,
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
journal = {Expert Review of Molecular Diagnostics},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities.}},
year = {2015}
}
@article{Bash2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bash - 2015 - VARIATION INTERPRETATION PREDICTORS PRINCIPLES, TYPES, PERFORMANCE AND CHOICE.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {PhD Proposal},
keywords = {icle},
pmid = {25246403},
title = {{VARIATION INTERPRETATION PREDICTORS: PRINCIPLES, TYPES, PERFORMANCE AND CHOICE}},
volume = {1},
year = {2015}
}
@article{Sujansky2001,
abstract = {The rapid expansion of biomedical knowledge, reduction in computing costs, and spread of internet access have created an ocean of electronic data. The decentralized nature of our scientific community and healthcare system, however, has resulted in a patchwork of diverse, or heterogeneous, database implementations, making access to and aggregation of data across databases very difficult. The database heterogeneity problem applies equally to clinical data describing individual patients and biological data characterizing our genome. Specifically, databases are highly heterogeneous with respect to the data models they employ, the data schemas they specify, the query languages they support, and the terminologies they recognize. Heterogeneous database systems attempt to unify disparate databases by providing uniform conceptual schemas that resolve representational heterogeneities, and by providing querying capabilities that aggregate and integrate distributed data. Research in this area has applied a variety of database and knowledge-based techniques, including semantic data modeling, ontology definition, query translation, query optimization, and terminology mapping. Existing systems have addressed heterogeneous database integration in the realms of molecular biology, hospital information systems, and application portability.},
author = {Sujansky, W},
doi = {10.1006/jbin.2001.1024},
file = {:home/jennifer/Descargas/1-s2.0-S153204640191024X-main.pdf:pdf},
isbn = {1532-0464},
issn = {1532-0464},
journal = {Journal of biomedical informatics},
keywords = {ancient mariner,data warehouse,data warehouse.,database,database integration,drink,everywhere,federated database,heterogeneous database,nor any drop to,samuel taylor coleridge,the rime of the,water},
number = {2001},
pages = {285--298},
pmid = {11977810},
title = {{Heterogeneous database integration in biomedicine.}},
volume = {34},
year = {2001}
}
@misc{Information,
author = {Information, National Center for Biotechnology},
title = {{NCBI}},
url = {http://www.ncbi.nlm.nih.gov/probe/docs/techmicroarray/}
}
@article{Huttenhower2010,
abstract = {The aim of this work is to be able to publish the information concerning communication with cancer patients as recommended in England. The observation and the study protocol during the stay abroad have been given the opportunity to stylize specific information on the methodology of communication of important information to terminally ill patients. It seems readily apparent as they characterized by both technical precision and sensivity to emotions and descriptions for the individual patient. How is shared by all chronic pain is predominantly complex emotion, a mix of additions and perceived physical and emotional pain - emotional. Because accurate information is beneficial to the patient and that really is not turned, so to speak, a "bullet" it is necessary that you have created, over time, a concrete "therapeutic alliance" between body physician, patient and possibly family. This arises, for sure, even at first accepted the patient during the clinical visit attentive to detail, is renewed in the definition of the common objective to be achieved, so analgesia and it is expressed in the certainty that the physician provides all the resources realistically available. It is then up to the sensitivity of the operator, doctor and/or nurse, described in the "take charge" find, from time to time, the words and manners, verbal and nonverbal, to respond fully to questions of the patient same.},
author = {Huttenhower, Curtis and Hofmann, Oliver},
doi = {10.1371/journal.pcbi.1000779},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huttenhower, Hofmann - 2010 - A quick guide to large-scale genomic data mining.pdf:pdf},
isbn = {1553-7358},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {5},
pages = {1--6},
pmid = {20523745},
title = {{A quick guide to large-scale genomic data mining}},
volume = {6},
year = {2010}
}
@misc{Tetreault2015a,
abstract = {Whole-exome sequencing (WES) represents a significant breakthrough in the field of human genetics. This technology has largely contributed to the identification of new disease-causing genes and is now entering clinical laboratories. WES represents a powerful tool for diagnosis and could reduce the ‘diagnostic odyssey' for many patients. In this review, we present a technical overview of WES analysis, variants annotation and interpretation in a clinical setting. We evaluate the usefulness of clinical WES in different clinical indications, such as rare diseases, cancer and complex diseases. Finally, we discuss the efficacy of WES as a diagnostic tool and the impact on patient management.},
author = {Tetreault, Martine and Bareke, Eric and Nadaf, Javad and Alirezaie, Najmeh and Majewski, Jacek},
booktitle = {Expert Review of Molecular Diagnostics},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tetreault et al. - 2015 - Whole-exome sequencing as a diagnostic tool current challenges and future opportunities.pdf:pdf},
keywords = {cancer,diagnostic,rare diseases,variants detection,whole-exome sequencing},
language = {en},
month = {may},
publisher = {Informa Healthcare},
title = {{Whole-exome sequencing as a diagnostic tool: current challenges and future opportunities}},
url = {http://www.tandfonline.com/doi/abs/10.1586/14737159.2015.1039516?journalCode=iero20{\#}.VxmiWVg{\_}zcU.mendeley},
year = {2015}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Distribuci{\'{o}}n de variantes por cromosoma (1).png:png},
title = {{Distribuci{\'{o}}n de variantes por cromosoma (1)}}
}
@article{Neto2014,
abstract = {Inherited myopathies are a heterogeneous group of disabling disorders with still barely understood pathological mechanisms. Around 40{\%} of afflicted patients remain without a molecular diagnosis after exclusion of known genes. The advent of high-throughput sequencing has opened avenues to the discovery of new implicated genes, but a working list of prioritized candidate genes is necessary to deal with the complexity of analyzing large-scale sequencing data. Here we used an integrative data mining strategy to analyze the genetic network linked to myopathies, derive specific signatures for inherited myopathy and related disorders, and identify and rank candidate genes for these groups. Training sets of genes were selected after literature review and used in Manteia, a public web-based data mining system, to extract disease group signatures in the form of enriched descriptor terms, which include functional annotation, human and mouse phenotypes, as well as biological pathways and protein interactions. These specific signatures were then used as an input to mine and rank candidate genes, followed by filtration against skeletal muscle expression and association with known diseases. Signatures and identified candidate genes highlight both potential common pathological mechanisms and allelic disease groups. Recent discoveries of gene associations to diseases, like B3GALNT2, GMPPB and B3GNT1 to congenital muscular dystrophies, were prioritized in the ranked lists, suggesting a posteriori validation of our approach and predictions. We show an example of how the ranked lists can be used to help analyze high-throughput sequencing data to identify candidate genes, and highlight the best candidate genes matching genomic regions linked to myopathies without known causative genes. This strategy can be automatized to generate fresh candidate gene lists, which help cope with database annotation updates as new knowledge is incorporated.},
author = {Neto, Osorio Abath and Tassy, Olivier and Biancalana, Val?? Rie and Zanoteli, Edmar and Pourqui??, Olivier and Laporte, Jocelyn},
doi = {10.1371/journal.pone.0110888},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto et al. - 2014 - Integrative data mining highlights candidate genes for monogenic myopathies.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {25353622},
title = {{Integrative data mining highlights candidate genes for monogenic myopathies}},
volume = {9},
year = {2014}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas (1)}
}
@article{Hehir-Kwa2016,
abstract = {Structural variation (SV) represents a major source of differences between individual human genomes and has been linked to disease phenotypes. However, the majority of studies pro-vide neither a global view of the full spectrum of these variants nor integrate them into reference panels of genetic variation. Here, we analyse whole genome sequencing data of 769 individuals from 250 Dutch families, and provide a haplotype-resolved map of 1.9 million genome variants across 9 different variant classes, including novel forms of complex indels, and retrotransposition-mediated insertions of mobile elements and processed RNAs. A large proportion are previously under reported variants sized between 21 and 100 bp. We detect 4 megabases of novel sequence, encoding 11 new transcripts. Finally, we show 191 known, trait-associated SNPs to be in strong linkage disequilibrium with SVs and demonstrate that our panel facilitates accurate imputation of SVs in unrelated individuals.},
author = {Hehir-Kwa, Jayne Y and Marschall, Tobias and Kloosterman, Wigard P and Francioli, Laurent C},
doi = {10.1038/ncomms12989},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hehir-Kwa et al. - 2016 - ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural var.pdf:pdf},
journal = {Nature Communications},
number = {11},
title = {{ARTICLE A high-quality human reference panel reveals the complexity and distribution of genomic structural variants}},
volume = {7},
year = {2016}
}
@article{Matthijs2015,
abstract = {We present, on behalf of EuroGentest and the European Society of Human Genetics, guidelines for the evaluation and validation of next-generation sequencing (NGS) applications for the diagnosis of genetic disorders. The work was performed by a group of laboratory geneticists and bioinformaticians, and discussed with clinical geneticists, industry and patients' representatives, and other stakeholders in the field of human genetics. The statements that were written during the elaboration of the guidelines are presented here. The background document and full guidelines are available as supplementary material. They include many examples to assist the laboratories in the implementation of NGS and accreditation of this service. The work and ideas presented by others in guidelines that have emerged elsewhere in the course of the past few years were also considered and are acknowledged in the full text. Interestingly, a few new insights that have not been cited before have emerged during the preparation of the guidelines. The most important new feature is the presentation of a 'rating system' for NGS-based diagnostic tests. The guidelines and statements have been applauded by the genetic diagnostic community, and thus seem to be valuable for the harmonization and quality assurance of NGS diagnostics in Europe. Next-generation sequencing (NGS) allows for the fast generation of thousands to millions of base pairs of DNA sequence of an individual patient. The relatively fast emergence and the great success of these technologies in research herald a new era in genetic diagnostics. However, the new technologies bring challenges, both at the technical level and in terms of data management, as well as for the interpreta-tion of the results and for counseling. We believe that all these aspects warrant consideration of what the precise role of NGS in diagnostics will be, today and tomorrow. Before even embarking on acquisition of machines and skills for performing NGS in diagnostics, many issues have to be dealt with. It is in this context that we propose the guidelines. These guidelines mostly deal with NGS testing in the context of rare and mostly monogenic diseases. They mainly focus on the targeted analysis of gene panels, either through specific capture assays, or by extracting data from whole-exome sequencing. In principle, whole-genome sequencing may – and shortly will – also be used to extract similar information. In that case, the guidelines would still apply, but because whole-genome sequencing would also allow detecting other molecular features of disease, they would have to be extended accordingly. The different aspects of NGS and diagnostics were discussed during three workshops. The first took place in Leuven, 25–26 February 2013. The preliminary views were presented during the EuroGentest Scientific Meeting in Prague, 7–8 March 2013. The second was an editorial workshop in Leuven, 1–2 October 2013, where the different people involved in writing the document came together to discuss the layout of the document and prepare},
author = {Matthijs, Gert and Souche, Erika and Alders, Mari{\"{e}}lle and Corveleyn, Anniek and Eck, Sebastian and Feenstra, Ilse and Race, Val{\'{e}}rie and Sistermans, Erik and Sturm, Marc and Weiss, Marjan and Yntema, Helger and Bakker, Egbert and Scheffer, Hans and Bauer, Peter},
doi = {10.1038/ejhg.2015.226},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthijs et al. - 2015 - Guidelines for diagnostic next-generation sequencing.pdf:pdf},
journal = {European Journal of Human Genetics},
number = {10},
pages = {2--5},
title = {{Guidelines for diagnostic next-generation sequencing}},
volume = {24},
year = {2015}
}
@article{Baes2014,
abstract = {BACKGROUND: Advances in human genomics have allowed unprecedented productivity in terms of algorithms, software, and literature available for translating raw next-generation sequence data into high-quality information. The challenges of variant identification in organisms with lower quality reference genomes are less well documented. We explored the consequences of commonly recommended preparatory steps and the effects of single and multi sample variant identification methods using four publicly available software applications (Platypus, HaplotypeCaller, Samtools and UnifiedGenotyper) on whole genome sequence data of 65 key ancestors of Swiss dairy cattle populations. Accuracy of calling next-generation sequence variants was assessed by comparison to the same loci from medium and high-density single nucleotide variant (SNV) arrays.$\backslash$n$\backslash$nRESULTS: The total number of SNVs identified varied by software and method, with single (multi) sample results ranging from 17.7 to 22.0 (16.9 to 22.0) million variants. Computing time varied considerably between software. Preparatory realignment of insertions and deletions and subsequent base quality score recalibration had only minor effects on the number and quality of SNVs identified by different software, but increased computing time considerably. Average concordance for single (multi) sample results with high-density chip data was 58.3{\%} (87.0{\%}) and average genotype concordance in correctly identified SNVs was 99.2{\%} (99.2{\%}) across software. The average quality of SNVs identified, measured as the ratio of transitions to transversions, was higher using single sample methods than multi sample methods. A consensus approach using results of different software generally provided the highest variant quality in terms of transition/transversion ratio.$\backslash$n$\backslash$nCONCLUSIONS: Our findings serve as a reference for variant identification pipeline development in non-human organisms and help assess the implication of preparatory steps in next-generation sequencing pipelines for organisms with incomplete reference genomes (pipeline code is included). Benchmarking this information should prove particularly useful in processing next-generation sequencing data for use in genome-wide association studies and genomic selection.},
author = {Baes, Christine F and Dolezal, Marlies A and Koltes, James E and Bapst, Beat and Fritz-Waters, Eric and Jansen, Sandra and Flury, Christine and Signer-Hasler, Heidi and Stricker, Christian and Fernando, Rohan and Fries, Ruedi and Moll, Juerg and Garrick, Dorian J and Reecy, James M and Gredler, Birgit},
doi = {10.1186/1471-2164-15-948},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baes et al. - 2014 - Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Algorithms,Animals,Cattle,Genetic Variation,Genome,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
number = {1},
pages = {948},
pmid = {25361890},
title = {{Evaluation of variant identification methods for whole genome sequencing data in dairy cattle.}},
url = {http://www.biomedcentral.com/1471-2164/15/948},
volume = {15},
year = {2014}
}
@techreport{Henderson2015,
author = {Henderson, Keith and Gallagher, Brian and Eliassi-rad, Tina},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Gallagher, Eliassi-rad - 2015 - EP-MEANS An Efficient Nonparametric Clustering of Empirical Probability Distributions.pdf:pdf},
isbn = {9781450331968},
title = {{EP-MEANS: An Efficient Nonparametric Clustering of Empirical Probability Distributions}},
year = {2015}
}
@article{Pirooznia2014,
abstract = {BACKGROUND: The processing and analysis of the large scale data generated by next-generation sequencing (NGS) experiments is challenging and is a burgeoning area of new methods development. Several new bioinformatics tools have been developed for calling sequence variants from NGS data. Here, we validate the variant calling of these tools and compare their relative accuracy to determine which data processing pipeline is optimal. RESULTS: We developed a unified pipeline for processing NGS data that encompasses four modules: mapping, filtering, realignment and recalibration, and variant calling. We processed 130 subjects from an ongoing whole exome sequencing study through this pipeline. To evaluate the accuracy of each module, we conducted a series of comparisons between the single nucleotide variant (SNV) calls from the NGS data and either gold-standard Sanger sequencing on a total of 700 variants or array genotyping data on a total of 9,935 single-nucleotide polymorphisms. A head to head comparison showed that Genome Analysis Toolkit (GATK) provided more accurate calls than SAMtools (positive predictive value of 92.55{\%} vs. 80.35{\%}, respectively). Realignment of mapped reads and recalibration of base quality scores before SNV calling proved to be crucial to accurate variant calling. GATK HaplotypeCaller algorithm for variant calling outperformed the UnifiedGenotype algorithm. We also showed a relationship between mapping quality, read depth and allele balance, and SNV call accuracy. However, if best practices are used in data processing, then additional filtering based on these metrics provides little gains and accuracies of {\textgreater}99{\%} are achievable. CONCLUSIONS: Our findings will help to determine the best approach for processing NGS data to confidently call variants for downstream analyses. To enable others to implement and replicate our results, all of our codes are freely available at http://metamoodics.org/wes.},
author = {Pirooznia, Mehdi and Kramer, Melissa and Parla, Jennifer and Goes, Fernando S and Potash, James B and McCombie, W Richard and Zandi, Peter P},
doi = {10.1186/1479-7364-8-14},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pirooznia et al. - 2014 - Validation and assessment of variant calling pipelines for next-generation sequencing.pdf:pdf},
isbn = {1479-7364 (Electronic)$\backslash$r1473-9542 (Linking)},
issn = {1479-7364},
journal = {Human genomics},
keywords = {Bipolar Disorder,Bipolar Disorder: genetics,DNA,Data Interpretation,Exome,High-Throughput Nucleotide Sequencing,Humans,Polymorphism,Sequence Analysis,Single Nucleotide,Software,Statistical},
number = {1},
pages = {14},
pmid = {25078893},
title = {{Validation and assessment of variant calling pipelines for next-generation sequencing.}},
url = {http://www.humgenomics.com/content/8/1/14},
volume = {8},
year = {2014}
}
@article{Del2014,
author = {Del, Optimizaci{\'{o}}n and Al, Tiempo Puerta-electrocardiograma and Una, Interior D E and Cr{\'{i}}tica, Ruta and El, E N},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Del et al. - 2014 - Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology.pdf:pdf},
pages = {51--68},
title = {{Salud P{\'{u}}blica • Epidemiolog{\'{i}}a Public Health • Epidemiology}},
volume = {2},
year = {2014}
}
@article{Hannah-Shmouni2015,
author = {Hannah-Shmouni, Fady and Seidelmann, Sara B. and Sirrs, Sandra and Mani, Arya and Jacoby, Daniel},
doi = {10.1016/j.cjca.2015.07.735},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannah-Shmouni et al. - 2015 - The Genetic Challenges and Opportunities in Advanced Heart Failure.pdf:pdf},
issn = {0828282X},
journal = {Canadian Journal of Cardiology},
number = {11},
pages = {1338--1350},
publisher = {Elsevier Ltd},
title = {{The Genetic Challenges and Opportunities in Advanced Heart Failure}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0828282X15013161},
volume = {31},
year = {2015}
}
@article{Li2009b,
abstract = {SUMMARY: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments. AVAILABILITY: http://samtools.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1006.1266v2},
author = {Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
doi = {10.1093/bioinformatics/btp352},
eprint = {1006.1266v2},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2009 - The Sequence AlignmentMap format and SAMtools.pdf:pdf},
isbn = {1367-4803{\$}\backslash{\$}r1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {2078--2079},
pmid = {19505943},
title = {{The Sequence Alignment/Map format and SAMtools}},
volume = {25},
year = {2009}
}
@article{Zhao2014,
abstract = {To demonstrate the benefits of RNA-Seq over microarray in transcriptome profiling, both RNA-Seq and microarray analyses were performed on RNA samples from a human T cell activation experiment. In contrast to other reports, our analyses focused on the difference, rather than similarity, between RNA-Seq and microarray technologies in transcriptome profiling. A comparison of data sets derived from RNA-Seq and Affymetrix platforms using the same set of samples showed a high correlation between gene expression profiles generated by the two platforms. However, it also demonstrated that RNA-Seq was superior in detecting low abundance transcripts, differentiating biologically critical isoforms, and allowing the identification of genetic variants. RNA-Seq also demonstrated a broader dynamic range than microarray, which allowed for the detection of more differentially expressed genes with higher fold-change. Analysis of the two datasets also showed the benefit derived from avoidance of technical issues inherent to microarray probe performance such as cross-hybridization, non-specific hybridization and limited detection range of individual probes. Because RNA-Seq does not rely on a pre-designed complement sequence detection probe, it is devoid of issues associated with probe redundancy and annotation, which simplified interpretation of the data. Despite the superior benefits of RNA-Seq, microarrays are still the more common choice of researchers when conducting transcriptional profiling experiments. This is likely because RNA-Seq sequencing technology is new to most researchers, more expensive than microarray, data storage is more challenging and analysis is more complex. We expect that once these barriers are overcome, the RNA-Seq platform will become the predominant tool for transcriptome analysis.},
author = {Zhao, Shanrong and Fung-Leung, Wai-Ping and Bittner, Anton and Ngo, Karen and Liu, Xuejun},
doi = {10.1371/journal.pone.0078644},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2014 - Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PloS one},
keywords = {Analysis of Variance,Base Sequence,CD4-Positive T-Lymphocytes,CD4-Positive T-Lymphocytes: metabolism,Cells, Cultured,Gene Expression Profiling,Humans,Lymphocyte Activation,Molecular Sequence Annotation,Oligonucleotide Array Sequence Analysis,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,Sequence Analysis, RNA,Transcriptome},
number = {1},
pages = {e78644},
pmid = {24454679},
title = {{Comparison of RNA-Seq and microarray in transcriptome profiling of activated T cells.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3894192{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2014}
}
@inproceedings{Moore99,
author = {Moore, R and Lopes, J},
booktitle = {TEMPLATE'06, 1st International Conference on Template Production},
publisher = {SCITEPRESS},
title = {{Paper templates}},
year = {1999}
}
@article{Quinlan2010,
abstract = {MOTIVATION: Testing for correlations between different sets of genomic features is a fundamental task in genomics research. However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies. Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.$\backslash$n$\backslash$nRESULTS: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format. BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features. The tools are extremely efficient and allow the user to compare large datasets (e.g. next-generation sequencing data) with both public and custom genome annotation tracks. BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.$\backslash$n$\backslash$nAVAILABILITY AND IMPLEMENTATION: BEDTools was written in C++. Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtools$\backslash$n$\backslash$nCONTACT: aaronquinlan@gmail.com; imh4y@virginia.edu$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Quinlan, Aaron R. and Hall, Ira M.},
doi = {10.1093/bioinformatics/btq033},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan, Hall - 2010 - BEDTools A flexible suite of utilities for comparing genomic features.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {6},
pages = {841--842},
pmid = {20110278},
title = {{BEDTools: A flexible suite of utilities for comparing genomic features}},
volume = {26},
year = {2010}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - scatter-mode (1).png:png},
title = {scatter-mode (1)}
}
@misc{,
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - variantescomparadas.png:png},
title = {variantescomparadas}
}
@article{Farid2016,
abstract = {In this paper, we introduce a new adaptive rule-based classifier for multi-class classification of biological data, where several problems of classifying biological data are addressed: overfitting, noisy instances and class-imbalance data. It is well known that rules are interesting way for representing data in a human interpretable way. The proposed rule-based classifier combines the random subspace and boosting approaches with ensemble of decision trees to construct a set of classification rules without involving global optimisation. The classifier considers random subspace approach to avoid overfitting, boosting approach for classifying noisy instances and ensemble of decision trees to deal with class-imbalance problem. The classifier uses two popular classification techniques: decision tree and k-nearest-neighbor algorithms. Decision trees are used for evolving classification rules from the training data, while k-nearest-neighbor is used for analysing the misclassified instances and removing vagueness between the contradictory rules. It considers a series of k iterations to develop a set of classification rules from the training data and pays more attention to the misclassified instances in the next iteration by giving it a boosting flavour. This paper particularly focuses to come up with an optimal ensemble classifier that will help for improving the prediction accuracy of DNA variant identification and classification task. The performance of proposed classifier is tested with compared to well-approved existing machine learning and data mining algorithms on genomic data (148 Exome data sets) of Brugada syndrome and 10 real benchmark life sciences data sets from the UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed classifier has exemplary classification accuracy on different types of biological data. Overall, the proposed classifier offers good prediction accuracy to new DNA variants classification where noisy and misclassified variants are optimised to increase test performance.},
annote = {NULL},
author = {Farid, Dewan Md. and Al-Mamun, Mohammad Abdullah and Manderick, Bernard and Nowe, Ann},
doi = {10.1016/j.eswa.2016.08.008},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farid et al. - 2016 - An adaptive rule-based classifier for mining big biological data.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Brugada syndrome,Classification,Decision tree,Genomic data,Rule-based classifier},
pages = {305--316},
title = {{An adaptive rule-based classifier for mining big biological data}},
volume = {64},
year = {2016}
}
@article{Cook2016,
abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI's vast public data resources.},
author = {Cook, Charles E. and Bergman, Mary Todd and Finn, Robert D. and Cochrane, Guy and Birney, Ewan and Apweiler, Rolf},
doi = {10.1093/nar/gkv1352},
file = {:home/jennifer/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cook et al. - 2016 - The European Bioinformatics Institute in 2016 Data growth and integration.pdf:pdf},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {D1},
pages = {D20--D26},
pmid = {26673705},
title = {{The European Bioinformatics Institute in 2016: Data growth and integration}},
volume = {44},
year = {2016}
}
@article{Rishishwar2015,
abstract = {The human dimension of the Columbian Exchange entailed substantial genetic admixture between ancestral source populations from Africa, the Americas and Europe, which had evolved separately for many thousands of years. We sought to address the implications of the creation of admixed American genomes, containing novel allelic combinations, for human health and fitness via analysis of an admixed Colombian population from Medellin. Colombian genomes from Medellin show a wide range of three-way admixture contributions from ancestral source populations. The primary ancestry component for the population is European (average = 74.6{\%}, range = 45.0{\%}-96.7{\%}), followed by Native American (average = 18.1{\%}, range = 2.1{\%}-33.3{\%}) and African (average = 7.3{\%}, range = 0.2{\%}-38.6{\%}). Locus-specific patterns of ancestry were evaluated to search for genomic regions that are enriched across the population for particular ancestry contributions. Adaptive and innate immune system related genes and pathways are particularly over-represented among ancestry-enriched segments, including genes (HLA-B and MAPK10) that are involved in defense against endemic pathogens such as malaria. Genes that encode functions related to skin pigmentation (SCL4A5) and cutaneous glands (EDAR) are also found in regions with anomalous ancestry patterns. These results suggest the possibility that ancestry-specific loci were differentially retained in the modern admixed Colombian population based on their utility in the New World environment.},
author = {Rishishwar, Lavanya and Conley, Andrew B. and Wigington, Charles H. and Wang, Lu and Valderrama-Aguirre, Augusto and {King Jordan}, I.},
doi = {10.1038/srep12376},
file = {:home/jennifer/Descargas/srep12376.pdf:pdf},
isbn = {2045-2322 (ISSNLinking)},
issn = {2045-2322},
journal = {Scientific Reports},
number = {1},
pages = {12376},
pmid = {26197429},
publisher = {Nature Publishing Group},
title = {{Ancestry, admixture and fitness in Colombian genomes}},
url = {http://www.nature.com/articles/srep12376},
volume = {5},
year = {2015}
}
@article{Arias-blanco2015,
author = {Arias-blanco, Juan Felipe and Fonseca-mendoza, Dora Janeth and Gamboa-garay, Oscar},
file = {:home/jennifer/Descargas/articulo BRCA1 Y BRCA2.pdf:pdf},
journal = {Revista Colombiana de Obstetricia y Ginecolog{\'{i}}a},
number = {4},
pages = {287--296},
title = {{FRECUENCIA DE MUTACI{\'{O}}N Y DE VARIANTES DE SECUENCIA PARA LOS GENES BRCA1 Y BRCA2 EN UNA MUESTRA DE MUJERES COLOMBIANAS CON SOSPECHA DE S{\'{I}}NDROME DE C{\'{A}}NCER DE MAMA HEREDITARIO: SERIE DE CASOS}},
url = {http://www.nature.com/articles/srep12376},
volume = {65},
year = {2015}
}
@misc{CoriellInstitute,
author = {{Coriell Institute}},
title = {1000 genomes project},
url = {https://catalog.coriell.org/0/Sections/Collections/NHGRI/1000Clm.aspx?PgId=675{\&}coll=HG}
}
@article{Li2017,
abstract = {In 2015, the American College of Medical Genetics and Genomics (ACMG) and the Association for Molecular Pathology (AMP) published updated standards and guidelines for the clinical interpretation of sequence variants with respect to human diseases on the basis of 28 criteria. However, variability between individual interpreters can be extensive because of reasons such as the different understandings of these guidelines and the lack of standard algorithms for implementing them, yet computational tools for semi-automated variant interpretation are not available. To address these problems, we propose a suite of methods for implementing these criteria and have developed a tool called InterVar to help human reviewers interpret the clinical significance of variants. InterVar can take a pre-annotated or VCF file as input and generate automated interpretation on 18 criteria. Furthermore, we have developed a companion web server, wInterVar, to enable user-friendly variant interpretation with an automated interpretation step and a manual adjustment step. These tools are especially useful for addressing severe congenital or very early-onset developmental disorders with high penetrance. Using results from a few published sequencing studies, we demonstrate the utility of InterVar in significantly reducing the time to interpret the clinical significance of sequence variants.},
author = {Li, Quan and Wang, Kai},
doi = {10.1016/j.ajhg.2017.01.004},
file = {:home/jennifer/Descargas/ACMGGUIDELINES.pdf:pdf},
isbn = {0002-9297},
issn = {15376605},
journal = {American Journal of Human Genetics},
keywords = {ACMG,ANNOVAR,ClinVar,InterVar,clinical interpretation,genetic diagnosis,variant annotation,variant interpretation},
number = {2},
pages = {267--280},
pmid = {28132688},
publisher = {ElsevierCompany.},
title = {{InterVar: Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines}},
url = {http://dx.doi.org/10.1016/j.ajhg.2017.01.004},
volume = {100},
year = {2017}
}
